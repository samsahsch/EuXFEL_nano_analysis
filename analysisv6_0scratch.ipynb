{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b43ceb6",
   "metadata": {},
   "source": [
    "### Analysis code snippets for preprocessed h5 files\n",
    "\n",
    "Organized as follows:\n",
    "1. Select runs and filter based on number of hits, including choosing ions, electron and/or photon data\n",
    "2. Filter based on pulse number\n",
    "3. Calibrate runs for m/q values\n",
    "4. Heatmap and electron time of flight plots\n",
    "5. Fish plots\n",
    "6. Intensity dependent plots including into time of flight plots, waterfall plots and heatmaps\n",
    "7. Presentation plots including fish plot, heatmaps, electron and ion data\n",
    "8. Covariances between ion data and between ion and electron data\n",
    "9. PNCCD photon data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74798159",
   "metadata": {},
   "source": [
    "Need to install reading methods the first time:\n",
    "\n",
    "pip install --user tables   ### to read dataframe\n",
    "pip install h5netcdf        ### to read xarrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87da59e",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df658959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_BETWEEN_PULSES = 3.54462e-6\n",
    "CHANNELS_PER_PULSE = 14080\n",
    "channel_time = TIME_BETWEEN_PULSES/CHANNELS_PER_PULSE\n",
    "\n",
    "\n",
    "    \n",
    "def read(runid):\n",
    "    'Read the preprocessed data of run with ID runid saved in the h5 file with a corresponding name'\n",
    "    'Outputs dataframes per event, per pulse, and xarrays etof, pnccd in that order'\n",
    "    \n",
    "    filename = '../preprocess/datarun' + str(runid) + '.h5'\n",
    "    \n",
    "    dfevent = pd.read_hdf(filename, 'dfevent')\n",
    "    dfpulse = pd.read_hdf(filename, 'dfpulse')\n",
    "    \n",
    "    etof = xr.open_dataarray(filename, group=\"etof\")\n",
    "    pnccd = xr.open_dataarray(filename, group=\"pnccd\")\n",
    "    \n",
    "    return dfevent, dfpulse, etof, pnccd\n",
    "\n",
    "\n",
    "\n",
    "def read_ions_electrons(runid,tof_limit=None):\n",
    "    'Read the preprocessed data of run with ID runid saved in the h5 file with a corresponding name'\n",
    "    'Outputs dataframes per event, per pulse, and xarrays etof in that order'\n",
    "    'tof_limit limiting electron time of flight loaded in'\n",
    "    \n",
    "    filename = '../preprocess/datarun' + str(runid) + '.h5'\n",
    "    \n",
    "    dfevent = pd.read_hdf(filename, 'dfevent')\n",
    "    dfpulse = pd.read_hdf(filename, 'dfpulse')\n",
    "    \n",
    "    etof = xr.open_dataarray(filename, group=\"etof\")\n",
    "\n",
    "    if type(tof_limit) == int:\n",
    "        max_coord = int(tof_limit/channel_time)\n",
    "        etof = etof[:max_coord]\n",
    "    \n",
    "    return dfevent, dfpulse, etof\n",
    "\n",
    "\n",
    "\n",
    "def read_ions_photons(runid):\n",
    "    'Read the preprocessed data of run with ID runid saved in the h5 file with a corresponding name'\n",
    "    'Outputs dataframes per event, per pulse, and xarrays pnccd in that order'\n",
    "    \n",
    "    filename = '../preprocess/datarun' + str(runid) + '.h5'\n",
    "    \n",
    "    dfevent = pd.read_hdf(filename, 'dfevent')\n",
    "    dfpulse = pd.read_hdf(filename, 'dfpulse')\n",
    "    \n",
    "    pnccd = xr.open_dataarray(filename, group=\"pnccd\")\n",
    "    \n",
    "    return dfevent, dfpulse, pnccd\n",
    "\n",
    "\n",
    "\n",
    "def read_ion(runid):\n",
    "    'Read the preprocessed data of run with ID runid saved in the h5 file with a corresponding name'\n",
    "    'Outputs dataframes per event and per pulse'\n",
    "    \n",
    "    filename = '../preprocess/datarun' + str(runid) + '.h5'\n",
    "    \n",
    "    dfevent = pd.read_hdf(filename, 'dfevent')\n",
    "    dfpulse = pd.read_hdf(filename, 'dfpulse')\n",
    "    \n",
    "    return dfevent, dfpulse\n",
    "\n",
    "\n",
    "\n",
    "def read_pnccd(runid):\n",
    "    'Read the preprocessed data of run with ID runid saved in the h5 file with a corresponding name'\n",
    "    'Outputs dataframes per event, per pulse, and xarrays etof, pnccd in that order'\n",
    "    \n",
    "    filename = '../preprocess/datarun' + str(runid) + '.h5'\n",
    "    pnccd = xr.open_dataarray(filename, group=\"pnccd\")\n",
    "    \n",
    "    return pnccd\n",
    "\n",
    "\n",
    "\n",
    "def events_selection(runs,thresholds,num_pulses=None):\n",
    "    'Reads one or multiple runs from h5 files'\n",
    "    'Makes a pulse selection based on the number of events per pulse between the defined thresholds'\n",
    "    'If multiple runs are passed, will merge the runs, once hit selected'\n",
    "    'Thresholds can be between one and three tuples (lower threshold, upper threshold)'\n",
    "    'Downsamples by num_pulses'\n",
    "    \n",
    "    lower_threshold1, upper_threshold1 = thresholds[0]\n",
    "    selected_dfevents1 = list()\n",
    "    selected_dfpulses1 = list()\n",
    "    selected_etofs1 = list()\n",
    "    selected_pnccds1 = list()\n",
    "    \n",
    "    if len(thresholds) > 1:\n",
    "        lower_threshold2, upper_threshold2 = thresholds[1]\n",
    "        selected_dfevents2 = list()\n",
    "        selected_dfpulses2 = list()\n",
    "        selected_etofs2 = list()\n",
    "        selected_pnccds2 = list()\n",
    "    \n",
    "    if len(thresholds) > 2:\n",
    "        lower_threshold3, upper_threshold3 = thresholds[2]  \n",
    "        selected_dfevents3 = list()\n",
    "        selected_dfpulses3 = list()\n",
    "        selected_etofs3 = list()\n",
    "        selected_pnccds3 = list()\n",
    "    \n",
    "    dataframes = dict()\n",
    "    \n",
    "    if type(num_pulses) == int:\n",
    "        num_pulses_run = int(num_pulses/len(runs))\n",
    "    \n",
    "    for run in runs:\n",
    "        \n",
    "        dfevent, dfpulse, etof, pnccd = read(run)\n",
    "        \n",
    "        selections = list()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.scatter(dfpulse.pulseId,dfpulse.nevents_pulse,c='black',label='All pulses')\n",
    "        \n",
    "        if type(num_pulses) == int:\n",
    "            dfpulse = dfpulse.sample(n=num_pulses_run)\n",
    "            \n",
    "        selected_dfpulse1 = dfpulse[lower_threshold1 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold1]\n",
    "        selected_dfevent1 = dfevent[dfevent.pulseId.isin(selected_dfpulse1.pulseId)]\n",
    "        selected_etof1 = etof.sel(pulseId=etof.coords['pulseId'].isin(selected_dfpulse1.pulseId))\n",
    "        selected_pnccd1 = pnccd.sel(trainId=pnccd.coords['trainId'].isin(selected_dfpulse1.trainId))\n",
    "        selections.append((selected_dfevent1, selected_dfpulse1, selected_etof1, selected_pnccd1))\n",
    "        plt.scatter(selected_dfpulse1.pulseId,selected_dfpulse1.nevents_pulse,c='r',label=f'Between {lower_threshold1} and {upper_threshold1}')\n",
    "        \n",
    "        if len(thresholds) > 1:\n",
    "            \n",
    "            selected_dfpulse2 = dfpulse[lower_threshold2 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold2]\n",
    "            selected_dfevent2 = dfevent[dfevent.pulseId.isin(selected_dfpulse2.pulseId)]\n",
    "            selected_etof2 = etof.sel(pulseId=etof.coords['pulseId'].isin(selected_dfpulse2.pulseId))\n",
    "            selected_pnccd2 = pnccd.sel(trainId=pnccd.coords['trainId'].isin(selected_dfpulse2.trainId))\n",
    "            selections.append((selected_dfevent2, selected_dfpulse2, selected_etof2, selected_pnccd2))\n",
    "            plt.scatter(selected_dfpulse2.pulseId,selected_dfpulse2.nevents_pulse,c='blue',label=f'Between {lower_threshold2} and {upper_threshold2}')\n",
    "            \n",
    "        if len(thresholds) > 2:\n",
    "            \n",
    "            selected_dfpulse3 = dfpulse[lower_threshold3 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold3]\n",
    "            selected_dfevent3 = dfevent[dfevent.pulseId.isin(selected_dfpulse3.pulseId)]\n",
    "            selected_etof3 = etof.sel(pulseId=etof.coords['pulseId'].isin(selected_dfpulse3.pulseId))\n",
    "            selected_pnccd3 = pnccd.sel(trainId=pnccd.coords['trainId'].isin(selected_dfpulse3.trainId))\n",
    "            selections.append((selected_dfevent3, selected_dfpulse3, selected_etof3, selected_pnccd3))\n",
    "            plt.scatter(selected_dfpulse3.pulseId,selected_dfpulse3.nevents_pulse,c='g',label=f'Between {lower_threshold3} and {upper_threshold3}')  \n",
    "        \n",
    "        dataframes[run] = selections\n",
    "          \n",
    "        plt.xlabel('Pulse ID')\n",
    "        plt.ylabel('Number of events per pulse')\n",
    "        plt.legend()\n",
    "        plt.title(f'Events per pulse with respect to pulse ID for run {run}')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    for key, values in dataframes.items():\n",
    "        \n",
    "        selected_dfevents1.append(values[0][0])\n",
    "        selected_dfpulses1.append(values[0][1])\n",
    "        selected_etofs1.append(values[0][2])\n",
    "        selected_pnccds1.append(values[0][3])\n",
    "        \n",
    "        if len(thresholds) > 1:\n",
    "            selected_dfevents2.append(values[1][0])\n",
    "            selected_dfpulses2.append(values[1][1])\n",
    "            selected_etofs2.append(values[1][2])\n",
    "            selected_pnccds2.append(values[1][3])\n",
    "            \n",
    "        if len(thresholds) > 2: \n",
    "            selected_dfevents3.append(values[2][0])\n",
    "            selected_dfpulses3.append(values[2][1])\n",
    "            selected_etofs3.append(values[2][2])\n",
    "            selected_pnccds3.append(values[2][3])\n",
    "        \n",
    "        \n",
    "    merged_selection = list()\n",
    "    \n",
    "    merged_dfevent1 = pd.concat(selected_dfevents1)\n",
    "    merged_dfevent1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    merged_dfpulse1 = pd.concat(selected_dfpulses1)\n",
    "    merged_dfpulse1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    merged_etof1 = xr.concat(selected_etofs1, dim='pulseId')\n",
    "    merged_pnccd1 = xr.concat(selected_pnccds1, dim='pulseId')\n",
    "    \n",
    "    merged_selection.append((merged_dfevent1, merged_dfpulse1, merged_etof1, merged_pnccd1))\n",
    "    \n",
    "    print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold1} and {upper_threshold1} events: {len(merged_dfpulse1)}\")\n",
    "       \n",
    "        \n",
    "    if len(thresholds) > 1:\n",
    "        merged_dfevent2 = pd.concat(selected_dfevents2)\n",
    "        merged_dfevent2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_dfpulse2 = pd.concat(selected_dfpulses2)\n",
    "        merged_dfpulse2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_etof2 = xr.concat(selected_etofs2, dim='pulseId')\n",
    "        merged_pnccd2 = xr.concat(selected_pnccds2, dim='pulseId')\n",
    "        \n",
    "        merged_selection.append((merged_dfevent2, merged_dfpulse2, merged_etof2, merged_pnccd2))\n",
    "        \n",
    "        print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold2} and {upper_threshold2} events: {len(merged_dfpulse2)}\")\n",
    "    \n",
    "    \n",
    "    if len(thresholds) > 2:\n",
    "        merged_dfevent3 = pd.concat(selected_dfevents3)\n",
    "        merged_dfevent3.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_dfpulse3 = pd.concat(selected_dfpulses3)\n",
    "        merged_dfpulse3.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_etof3 = xr.concat(selected_etofs3, dim='pulseId')\n",
    "        merged_pnccd3 = xr.concat(selected_pnccds3, dim='pulseId')\n",
    "        \n",
    "        merged_selection.append((merged_dfevent3, merged_dfpulse3, merged_etof3, merged_pnccd3))\n",
    "        \n",
    "        print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold3} and {upper_threshold3} events: {len(merged_dfpulse3)}\")\n",
    "        \n",
    "    \n",
    "    return merged_selection\n",
    "\n",
    "\n",
    "\n",
    "def events_selection_ions_electrons(runs,thresholds,num_pulses=None,tof_limit=None):\n",
    "    'Reads one or multiple runs from h5 files'\n",
    "    'Makes a pulse selection based on the number of events per pulse between the defined thresholds'\n",
    "    'If multiple runs are passed, will merge the runs, once hit selected'\n",
    "    'Thresholds can be between one and three tuples (lower threshold, upper threshold)'\n",
    "    'Downsamples by num_pulses'\n",
    "    \n",
    "    lower_threshold1, upper_threshold1 = thresholds[0]\n",
    "    selected_dfevents1 = list()\n",
    "    selected_dfpulses1 = list()\n",
    "    selected_etofs1 = list()\n",
    "    \n",
    "    if len(thresholds) > 1:\n",
    "        lower_threshold2, upper_threshold2 = thresholds[1]\n",
    "        selected_dfevents2 = list()\n",
    "        selected_dfpulses2 = list()\n",
    "        selected_etofs2 = list()\n",
    "    \n",
    "    if len(thresholds) > 2:\n",
    "        lower_threshold3, upper_threshold3 = thresholds[2]  \n",
    "        selected_dfevents3 = list()\n",
    "        selected_dfpulses3 = list()\n",
    "        selected_etofs3 = list()\n",
    "    \n",
    "    dataframes = dict()\n",
    "    \n",
    "    if type(num_pulses) == int:\n",
    "        num_pulses_run = int(num_pulses/len(runs))\n",
    "    \n",
    "    for run in runs:\n",
    "        \n",
    "        dfevent, dfpulse, etof = read_ions_electrons(run,tof_limit)\n",
    "        \n",
    "        selections = list()\n",
    "        \n",
    "        if type(num_pulses) == int:\n",
    "            dfpulse = dfpulse.sample(n=num_pulses_run)\n",
    "            \n",
    "        selected_dfpulse1 = dfpulse[lower_threshold1 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold1]\n",
    "        selected_dfevent1 = dfevent[dfevent.pulseId.isin(selected_dfpulse1.pulseId)]\n",
    "        selected_etof1 = etof.sel(pulseId=etof.coords['pulseId'].isin(selected_dfpulse1.pulseId))\n",
    "        selections.append((selected_dfevent1, selected_dfpulse1, selected_etof1))\n",
    "        plt.scatter(selected_dfpulse1.pulseId,selected_dfpulse1.nevents_pulse,c='r',label=f'Between {lower_threshold1} and {upper_threshold1}')\n",
    "        \n",
    "        if len(thresholds) > 1:\n",
    "            \n",
    "            selected_dfpulse2 = dfpulse[lower_threshold2 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold2]\n",
    "            selected_dfevent2 = dfevent[dfevent.pulseId.isin(selected_dfpulse2.pulseId)]\n",
    "            selected_etof2 = etof.sel(pulseId=etof.coords['pulseId'].isin(selected_dfpulse2.pulseId))\n",
    "            selections.append((selected_dfevent2, selected_dfpulse2, selected_etof2))\n",
    "            plt.scatter(selected_dfpulse2.pulseId,selected_dfpulse2.nevents_pulse,c='blue',label=f'Between {lower_threshold2} and {upper_threshold2}')\n",
    "            \n",
    "        if len(thresholds) > 2:\n",
    "            \n",
    "            selected_dfpulse3 = dfpulse[lower_threshold3 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold3]\n",
    "            selected_dfevent3 = dfevent[dfevent.pulseId.isin(selected_dfpulse3.pulseId)]\n",
    "            selected_etof3 = etof.sel(pulseId=etof.coords['pulseId'].isin(selected_dfpulse3.pulseId))\n",
    "            selections.append((selected_dfevent3, selected_dfpulse3, selected_etof3))\n",
    "            plt.scatter(selected_dfpulse3.pulseId,selected_dfpulse3.nevents_pulse,c='g',label=f'Between {lower_threshold3} and {upper_threshold3}')  \n",
    "        \n",
    "        dataframes[run] = selections\n",
    "        \n",
    "    for key, values in dataframes.items():\n",
    "        \n",
    "        selected_dfevents1.append(values[0][0])\n",
    "        selected_dfpulses1.append(values[0][1])\n",
    "        selected_etofs1.append(values[0][2])\n",
    "        \n",
    "        if len(thresholds) > 1:\n",
    "            selected_dfevents2.append(values[1][0])\n",
    "            selected_dfpulses2.append(values[1][1])\n",
    "            selected_etofs2.append(values[1][2])\n",
    "            \n",
    "        if len(thresholds) > 2: \n",
    "            selected_dfevents3.append(values[2][0])\n",
    "            selected_dfpulses3.append(values[2][1])\n",
    "            selected_etofs3.append(values[2][2])\n",
    "        \n",
    "    merged_selection = list()\n",
    "    \n",
    "    merged_dfevent1 = pd.concat(selected_dfevents1)\n",
    "    merged_dfevent1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    merged_dfpulse1 = pd.concat(selected_dfpulses1)\n",
    "    merged_dfpulse1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    merged_etof1 = xr.concat(selected_etofs1, dim='pulseId')\n",
    "    \n",
    "    merged_selection.append((merged_dfevent1, merged_dfpulse1, merged_etof1))\n",
    "    \n",
    "    print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold1} and {upper_threshold1} events: {len(merged_dfpulse1)}\")\n",
    "           \n",
    "    if len(thresholds) > 1:\n",
    "        merged_dfevent2 = pd.concat(selected_dfevents2)\n",
    "        merged_dfevent2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_dfpulse2 = pd.concat(selected_dfpulses2)\n",
    "        merged_dfpulse2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_etof2 = xr.concat(selected_etofs2, dim='pulseId')\n",
    "        \n",
    "        merged_selection.append((merged_dfevent2, merged_dfpulse2, merged_etof2))\n",
    "        \n",
    "        print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold2} and {upper_threshold2} events: {len(merged_dfpulse2)}\") \n",
    "    \n",
    "    if len(thresholds) > 2:\n",
    "        merged_dfevent3 = pd.concat(selected_dfevents3)\n",
    "        merged_dfevent3.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_dfpulse3 = pd.concat(selected_dfpulses3)\n",
    "        merged_dfpulse3.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_etof3 = xr.concat(selected_etofs3, dim='pulseId')\n",
    "        \n",
    "        merged_selection.append((merged_dfevent3, merged_dfpulse3, merged_etof3))\n",
    "        \n",
    "        print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold3} and {upper_threshold3} events: {len(merged_dfpulse3)}\")\n",
    "        \n",
    "    return merged_selection\n",
    "\n",
    "\n",
    "\n",
    "def events_selection_ions_photons(runs,thresholds,num_pulses=None):\n",
    "    'Reads one or multiple runs from h5 files'\n",
    "    'Makes a pulse selection based on the number of events per pulse between the defined thresholds'\n",
    "    'If multiple runs are passed, will merge the runs, once hit selected'\n",
    "    'Thresholds can be between one and three tuples (lower threshold, upper threshold)'\n",
    "    'Downsamples by num_pulses'\n",
    "    \n",
    "    lower_threshold1, upper_threshold1 = thresholds[0]\n",
    "    selected_dfevents1 = list()\n",
    "    selected_dfpulses1 = list()\n",
    "    selected_pnccds1 = list()\n",
    "    \n",
    "    if len(thresholds) > 1:\n",
    "        lower_threshold2, upper_threshold2 = thresholds[1]\n",
    "        selected_dfevents2 = list()\n",
    "        selected_dfpulses2 = list()\n",
    "        selected_pnccds2 = list()\n",
    "    \n",
    "    if len(thresholds) > 2:\n",
    "        lower_threshold3, upper_threshold3 = thresholds[2]  \n",
    "        selected_dfevents3 = list()\n",
    "        selected_dfpulses3 = list()\n",
    "        selected_pnccds3 = list()\n",
    "    \n",
    "    dataframes = dict()\n",
    "    \n",
    "    if type(num_pulses) == int:\n",
    "        num_pulses_run = int(num_pulses/len(runs))\n",
    "    \n",
    "    for run in runs:\n",
    "        \n",
    "        print('Processing run number',run)\n",
    "        \n",
    "        dfevent, dfpulse, pnccd = read_ions_photons(run)\n",
    "        \n",
    "        selections = list()\n",
    "        \n",
    "        if type(num_pulses) == int:\n",
    "            dfpulse = dfpulse.sample(n=num_pulses_run)\n",
    "            \n",
    "        selected_dfpulse1 = dfpulse[lower_threshold1 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold1]\n",
    "        selected_dfevent1 = dfevent[dfevent.pulseId.isin(selected_dfpulse1.pulseId)]\n",
    "        selected_pnccd1 = pnccd.sel(trainId=pnccd.coords['trainId'].isin(selected_dfpulse1.trainId))\n",
    "        selections.append((selected_dfevent1, selected_dfpulse1, selected_pnccd1))\n",
    "        \n",
    "        if len(thresholds) > 1:\n",
    "            \n",
    "            selected_dfpulse2 = dfpulse[lower_threshold2 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold2]\n",
    "            selected_dfevent2 = dfevent[dfevent.pulseId.isin(selected_dfpulse2.pulseId)]\n",
    "            selected_pnccd2 = pnccd.sel(trainId=pnccd.coords['trainId'].isin(selected_dfpulse2.trainId))\n",
    "            selections.append((selected_dfevent2, selected_dfpulse2, selected_pnccd2))\n",
    "            \n",
    "        if len(thresholds) > 2:\n",
    "            \n",
    "            selected_dfpulse3 = dfpulse[lower_threshold3 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold3]\n",
    "            selected_dfevent3 = dfevent[dfevent.pulseId.isin(selected_dfpulse3.pulseId)]\n",
    "            selected_pnccd3 = pnccd.sel(trainId=pnccd.coords['trainId'].isin(selected_dfpulse3.trainId))\n",
    "            selections.append((selected_dfevent3, selected_dfpulse3, selected_pnccd3))\n",
    "            \n",
    "        dataframes[run] = selections\n",
    "        \n",
    "    for key, values in dataframes.items():\n",
    "        \n",
    "        selected_dfevents1.append(values[0][0])\n",
    "        selected_dfpulses1.append(values[0][1])\n",
    "        selected_pnccds1.append(values[0][2])\n",
    "        \n",
    "        if len(thresholds) > 1:\n",
    "            selected_dfevents2.append(values[1][0])\n",
    "            selected_dfpulses2.append(values[1][1])\n",
    "            selected_pnccds2.append(values[1][2])\n",
    "            \n",
    "        if len(thresholds) > 2: \n",
    "            selected_dfevents3.append(values[2][0])\n",
    "            selected_dfpulses3.append(values[2][1])\n",
    "            selected_pnccds3.append(values[2][2])\n",
    "        \n",
    "        \n",
    "    merged_selection = list()\n",
    "    \n",
    "    merged_dfevent1 = pd.concat(selected_dfevents1)\n",
    "    merged_dfevent1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    merged_dfpulse1 = pd.concat(selected_dfpulses1)\n",
    "    merged_dfpulse1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    merged_pnccd1 = xr.concat(selected_pnccds1, dim='trainId')\n",
    "    \n",
    "    merged_selection.append((merged_dfevent1, merged_dfpulse1, merged_pnccd1))\n",
    "    \n",
    "    print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold1} and {upper_threshold1} events: {len(merged_dfpulse1)}\")\n",
    "       \n",
    "        \n",
    "    if len(thresholds) > 1:\n",
    "        merged_dfevent2 = pd.concat(selected_dfevents2)\n",
    "        merged_dfevent2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_dfpulse2 = pd.concat(selected_dfpulses2)\n",
    "        merged_dfpulse2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_pnccd2 = xr.concat(selected_pnccds2, dim='trainId')\n",
    "        \n",
    "        merged_selection.append((merged_dfevent2, merged_dfpulse2, merged_pnccd2))\n",
    "        \n",
    "        print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold2} and {upper_threshold2} events: {len(merged_dfpulse2)}\")\n",
    "    \n",
    "    \n",
    "    if len(thresholds) > 2:\n",
    "        merged_dfevent3 = pd.concat(selected_dfevents3)\n",
    "        merged_dfevent3.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_dfpulse3 = pd.concat(selected_dfpulses3)\n",
    "        merged_dfpulse3.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_pnccd3 = xr.concat(selected_pnccds3, dim='trainId')\n",
    "        \n",
    "        merged_selection.append((merged_dfevent3, merged_dfpulse3, merged_pnccd3))\n",
    "        \n",
    "        print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold3} and {upper_threshold3} events: {len(merged_dfpulse3)}\")\n",
    "\n",
    "    \n",
    "    return merged_selection\n",
    "\n",
    "\n",
    "\n",
    "def ion_selection(runs,thresholds,num_pulses=None):\n",
    "    'Only handles ion data'\n",
    "    'Reads one or multiple runs from h5 files'\n",
    "    'Makes a pulse selection based on the number of events per pulse between the defined thresholds'\n",
    "    'If multiple runs are passed, will merge the runs, once hit selected'\n",
    "    'Thresholds can be between one and three tuples (lower threshold, upper threshold)'\n",
    "    'Downsamples by num_pulses'\n",
    "    \n",
    "    lower_threshold1, upper_threshold1 = thresholds[0]\n",
    "    selected_dfevents1 = list()\n",
    "    selected_dfpulses1 = list()\n",
    "    \n",
    "    if len(thresholds) > 1:\n",
    "        lower_threshold2, upper_threshold2 = thresholds[1]\n",
    "        selected_dfevents2 = list()\n",
    "        selected_dfpulses2 = list()\n",
    "    \n",
    "    if len(thresholds) > 2:\n",
    "        lower_threshold3, upper_threshold3 = thresholds[2]  \n",
    "        selected_dfevents3 = list()\n",
    "        selected_dfpulses3 = list()\n",
    "    \n",
    "    dataframes = dict()\n",
    "    \n",
    "    if type(num_pulses) == int:\n",
    "        num_pulses_run = int(num_pulses/len(runs))\n",
    "    \n",
    "    for run in runs:\n",
    "        \n",
    "        print('Handling run', run)\n",
    "        dfevent, dfpulse = read_ion(run)\n",
    "        \n",
    "        selections = list()\n",
    "        # plt.figure()\n",
    "        # plt.scatter(dfpulse.pulseId,dfpulse.nevents_pulse,c='black',label='All pulses')\n",
    "        \n",
    "        if type(num_pulses) == int:\n",
    "            dfpulse = dfpulse.sample(n=num_pulses_run)\n",
    "            \n",
    "        selected_dfpulse1 = dfpulse[lower_threshold1 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold1]\n",
    "        selected_dfevent1 = dfevent[dfevent.pulseId.isin(selected_dfpulse1.pulseId)]\n",
    "        selections.append((selected_dfevent1, selected_dfpulse1))\n",
    "        \n",
    "        if len(thresholds) > 1:\n",
    "            \n",
    "            selected_dfpulse2 = dfpulse[lower_threshold2 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold2]\n",
    "            selected_dfevent2 = dfevent[dfevent.pulseId.isin(selected_dfpulse2.pulseId)]\n",
    "            selections.append((selected_dfevent2, selected_dfpulse2))\n",
    "            \n",
    "        if len(thresholds) > 2:\n",
    "            \n",
    "            selected_dfpulse3 = dfpulse[lower_threshold3 < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold3]\n",
    "            selected_dfevent3 = dfevent[dfevent.pulseId.isin(selected_dfpulse3.pulseId)]\n",
    "            selections.append((selected_dfevent3, selected_dfpulse3))\n",
    "        \n",
    "        dataframes[run] = selections\n",
    "          \n",
    "        # plt.xlabel('Pulse ID')\n",
    "        # plt.ylabel('Number of events per pulse')\n",
    "        # plt.legend()\n",
    "        # plt.title(f'Events per pulse with respect to pulse ID for run {run}')\n",
    "        # plt.show()\n",
    "\n",
    "        \n",
    "    for key, values in dataframes.items():\n",
    "        \n",
    "        selected_dfevents1.append(values[0][0])\n",
    "        selected_dfpulses1.append(values[0][1])\n",
    "        \n",
    "        if len(thresholds) > 1:\n",
    "            selected_dfevents2.append(values[1][0])\n",
    "            selected_dfpulses2.append(values[1][1])\n",
    "            \n",
    "        if len(thresholds) > 2: \n",
    "            selected_dfevents3.append(values[2][0])\n",
    "            selected_dfpulses3.append(values[2][1])\n",
    "        \n",
    "        \n",
    "    merged_selection = list()\n",
    "    \n",
    "    merged_dfevent1 = pd.concat(selected_dfevents1)\n",
    "    merged_dfevent1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    merged_dfpulse1 = pd.concat(selected_dfpulses1)\n",
    "    merged_dfpulse1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    merged_selection.append((merged_dfevent1, merged_dfpulse1))\n",
    "    \n",
    "    print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold1} and {upper_threshold1} events: {len(merged_dfpulse1)}\")\n",
    "       \n",
    "        \n",
    "    if len(thresholds) > 1:\n",
    "        merged_dfevent2 = pd.concat(selected_dfevents2)\n",
    "        merged_dfevent2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_dfpulse2 = pd.concat(selected_dfpulses2)\n",
    "        merged_dfpulse2.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        merged_selection.append((merged_dfevent2, merged_dfpulse2))\n",
    "        \n",
    "        print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold2} and {upper_threshold2} events: {len(merged_dfpulse2)}\")\n",
    "    \n",
    "    \n",
    "    if len(thresholds) > 2:\n",
    "        merged_dfevent3 = pd.concat(selected_dfevents3)\n",
    "        merged_dfevent3.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        merged_dfpulse3 = pd.concat(selected_dfpulses3)\n",
    "        merged_dfpulse3.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        merged_selection.append((merged_dfevent3, merged_dfpulse3))\n",
    "        \n",
    "        print(f\"Number of pulses selected across {len(runs)} run(s) between {lower_threshold3} and {upper_threshold3} events: {len(merged_dfpulse3)}\")\n",
    "        \n",
    "    \n",
    "    return merged_selection\n",
    "\n",
    "\n",
    "\n",
    "def heatmap(dfevent):\n",
    "    'Creates heatmap of the ions hits, based on a dfevent dataframe'\n",
    "    \n",
    "    counts_df = dfevent.groupby(['x', 'y']).size().reset_index(name='count')\n",
    "    heatmap_data = counts_df.pivot(index='y', columns='x', values='count')\n",
    "    \n",
    "    plt.figure()\n",
    "    ax = sns.heatmap(heatmap_data, cmap='viridis',cbar_kws={'label': 'Number of events'})\n",
    "    plt.title('Ion heatmap')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "def ion_tof(dfevent,xlimits=(0,TIME_BETWEEN_PULSES),nbins=1000):\n",
    "    'Plots ion time of flight data using dfevent dataframe'\n",
    "    \n",
    "    bounded_dfevent = dfevent[dfevent.tof > xlimits[0]][dfevent.tof < xlimits[1]]\n",
    "    hist, bin_edges = np.histogram(bounded_dfevent.tof, bins=nbins)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(bin_edges[:-1], hist)\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Number of hits per bin')\n",
    "    plt.title('Ions time of flight')\n",
    "    # plt.xlim(xlimits[0],xlimits[1])\n",
    "    plt.show()   \n",
    "    \n",
    "    \n",
    "    \n",
    "def e_tof(etof):\n",
    "    'Plots electron time of flight data using etof xarray data'\n",
    "    \n",
    "    channel_time = TIME_BETWEEN_PULSES/CHANNELS_PER_PULSE\n",
    "    \n",
    "    xaxis = np.arange(14080)*channel_time\n",
    "    avg_selected_etof = -np.mean(etof, axis=0)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(xaxis,avg_selected_etof/max(avg_selected_etof))\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Normalized signal')\n",
    "    plt.title('Electrons time of flight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def events_selection_plots(runs,thresholds,downsampling=None):\n",
    "    'Runs functions events_selection, heatmap, e_tof, ion_tof'\n",
    "    'Downsamples by downsampling integer value if one is given'\n",
    "    \n",
    "    selections = events_selection(runs,thresholds,downsampling)\n",
    "    \n",
    "    print(f'\\n Plots for selection between {thresholds[0][0]} and {thresholds[0][1]} events:')\n",
    "    selected_dfevent1, selected_dfpulse1, selected_etof1, selected_pnccd1 = selections[0]\n",
    "    heatmap(selected_dfevent1)\n",
    "    ion_tof(selected_dfevent1)\n",
    "    e_tof(selected_etof1)\n",
    "    \n",
    "    if len(selections) > 1:\n",
    "        print(f'Plots for selection between {thresholds[1][0]} and {thresholds[1][1]} events:')\n",
    "        selected_dfevent2, selected_dfpulse2, selected_etof2, selected_pnccd2 = selections[1]\n",
    "        heatmap(selected_dfevent2)\n",
    "        ion_tof(selected_dfevent2)\n",
    "        e_tof(selected_etof2)\n",
    "    \n",
    "    elif len(selections) > 2:\n",
    "        print(f'Plots for selection between {thresholds[2][0]} and {thresholds[2][1]} events:')\n",
    "        selected_dfevent3, selected_dfpulse3, selected_etof3, selected_pnccd3 = selections[2]\n",
    "        heatmap(selected_dfevent3)\n",
    "        ion_tof(selected_dfevent3)\n",
    "        e_tof(selected_etof3)\n",
    "    \n",
    "    return selections\n",
    "\n",
    "\n",
    "\n",
    "def pulse_filtered_ion_tof(dfevent,pulse_number=None,xlim=TIME_BETWEEN_PULSES):\n",
    "    'Plots ion time of flight data using dfevent dataframe'\n",
    "    \n",
    "    if pulse_number != None:\n",
    "        pulse_filtered_dfevent = dfevent[dfevent.pulseId.astype(str).str.endswith(pulse_number)]\n",
    "    else:\n",
    "        pulse_filtered_dfevent = dfevent\n",
    "    \n",
    "    bounded_dfevent = pulse_filtered_dfevent[pulse_filtered_dfevent.tof < TIME_BETWEEN_PULSES]\n",
    "    hist, bin_edges = np.histogram(bounded_dfevent.tof, bins=1000)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(bin_edges[:-1], hist/max(hist))\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Normalized signal')\n",
    "    plt.title('Ions time of flight')\n",
    "    plt.xlim(0,xlim)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(bin_edges[:-1], hist/max(hist))\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Normalized signal')\n",
    "    plt.title('Ions time of flight')\n",
    "    plt.xlim(0,TIME_BETWEEN_PULSES)\n",
    "    plt.yscale('log')\n",
    "    plt.show()  \n",
    "    \n",
    "    \n",
    "    \n",
    "def heatmap_with_zones(dfevent,zones):\n",
    "    'Creates heatmap of the ions hits, based on a dfevent dataframe'\n",
    "    'Draws a rectangle around zones defined by a list of tuples where each tuple represents a tilted zone (xstart, ystart, width, height, angle in degrees)'\n",
    "    \n",
    "    counts_df = dfevent.groupby(['x', 'y']).size().reset_index(name='count')\n",
    "    heatmap_data = counts_df.pivot(index='y', columns='x', values='count')\n",
    "    \n",
    "    plt.figure()\n",
    "    ax = sns.heatmap(heatmap_data, cmap='viridis', cbar_kws={'label': 'Number of events'})\n",
    "    \n",
    "    xlim = int(ax.get_xticklabels()[0].get_text())\n",
    "    ylim = int(ax.get_yticklabels()[0].get_text())\n",
    "\n",
    "    for zone in zones:\n",
    "        x, y, width, height, angle = zone\n",
    "        x_adjusted = x - xlim\n",
    "        y_adjusted = y - ylim\n",
    "        \n",
    "        rect = plt.Rectangle((x_adjusted, y_adjusted), width, height, fill=False, edgecolor='red', lw=1, angle=angle)\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    plt.title('Ion heatmap')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "        \n",
    "def spatial_selection(dfevent,dfpulse,etof,zones):\n",
    "    'Square selection from the heatmap using spatial coordinates'\n",
    "    'Zone is a tuple representing a zone that is not tilted (xstart, ystart, width, height)'\n",
    "    'Returns spatially selected dfevent,dfpulse,etof'\n",
    "    \n",
    "    heatmap_with_zones(dfevent,zones)\n",
    "    \n",
    "    selected_dfevents = []\n",
    "    \n",
    "    for zone in zones:\n",
    "        \n",
    "        xstart,ystart,width,height,angle = zone\n",
    "    \n",
    "        spatial_selected_dfevent = dfevent[dfevent.x > xstart][dfevent.x < xstart+width][dfevent.y > ystart][dfevent.y < ystart+height]\n",
    "        selected_dfevents.append(spatial_selected_dfevent)\n",
    "        \n",
    "    merged_dfevent = pd.concat(selected_dfevents)\n",
    "    merged_dfevent.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    spatial_selected_dfpulse = dfpulse[dfpulse.pulseId.isin(merged_dfevent.pulseId)]\n",
    "    spatial_selected_etof = etof.sel(pulseId=etof.coords['pulseId'].isin(merged_dfevent.pulseId))\n",
    "    \n",
    "    return merged_dfevent,spatial_selected_dfpulse,spatial_selected_etof\n",
    "\n",
    "\n",
    "\n",
    "def spatial_ion_selection(dfevent,dfpulse,zones):\n",
    "    'Square ion selection from the heatmap using spatial coordinates'\n",
    "    'Zone is a tuple representing a zone that is not tilted (xstart, ystart, width, height)'\n",
    "    'Returns spatially selected dfevent,dfpulse'\n",
    "    \n",
    "    heatmap_with_zones(dfevent,zones)\n",
    "    \n",
    "    selected_dfevents = []\n",
    "    \n",
    "    for zone in zones:\n",
    "        \n",
    "        xstart,ystart,width,height,angle = zone\n",
    "    \n",
    "        spatial_selected_dfevent = dfevent[dfevent.x > xstart][dfevent.x < xstart+width][dfevent.y > ystart][dfevent.y < ystart+height]\n",
    "        selected_dfevents.append(spatial_selected_dfevent)\n",
    "        \n",
    "    merged_dfevent = pd.concat(selected_dfevents)\n",
    "    merged_dfevent.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    spatial_selected_dfpulse = dfpulse[dfpulse.pulseId.isin(merged_dfevent.pulseId)]\n",
    "    \n",
    "    return merged_dfevent,spatial_selected_dfpulse\n",
    "\n",
    "\n",
    "\n",
    "def big_ion_tof(dfevent):\n",
    "    'Plots widget of big ion time of flight data using dfevent dataframe'\n",
    "    \n",
    "    bounded_dfevent = dfevent[dfevent.tof < TIME_BETWEEN_PULSES]\n",
    "    hist, bin_edges = np.histogram(bounded_dfevent.tof, bins=1000)\n",
    "    \n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.plot(bin_edges[:-1], hist, c='g')\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Number of hits per bin')\n",
    "    plt.title('Ions time of flight')\n",
    "    plt.show()\n",
    "    \n",
    " \n",
    "    \n",
    "def autoscale_y(ax,margin=0.1):\n",
    "    \"\"\"This function rescales the y-axis based on the data that is visible given the current xlim of the axis.\n",
    "    ax -- a matplotlib axes object\n",
    "    margin -- the fraction of the total height of the y-data to pad the upper and lower ylims\"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    def get_bottom_top(line):\n",
    "        xd = line.get_xdata()\n",
    "        yd = line.get_ydata()\n",
    "        lo,hi = ax.get_xlim()\n",
    "        y_displayed = yd[((xd>lo) & (xd<hi))]\n",
    "        h = np.max(y_displayed) - np.min(y_displayed)\n",
    "        bot = np.min(y_displayed)-margin*h\n",
    "        top = np.max(y_displayed)+margin*h\n",
    "        return bot,top\n",
    "\n",
    "    lines = ax.get_lines()\n",
    "    bot,top = np.inf, -np.inf\n",
    "\n",
    "    for line in lines:\n",
    "        new_bot, new_top = get_bottom_top(line)\n",
    "        if new_bot < bot: bot = new_bot\n",
    "        if new_top > top: top = new_top\n",
    "\n",
    "    ax.set_ylim(bot,top)\n",
    "    \n",
    "    \n",
    "    \n",
    "def zoomed_ion_tof(dfevent,anchor):\n",
    "    'Plots zoom around anchor point of ion time of flight data using dfevent dataframe'\n",
    "    \n",
    "    bounded_dfevent = dfevent[dfevent.tof < TIME_BETWEEN_PULSES]\n",
    "    hist, bin_edges = np.histogram(bounded_dfevent.tof, bins=1000)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(bin_edges[:-1], hist, c='g')\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Number of hits per bin')\n",
    "    plt.title('Ions time of flight')\n",
    "    plt.xlim(anchor-1e-7,anchor+1e-7)\n",
    "    autoscale_y(ax)\n",
    "    ax.axvline(x=anchor, color='black', linestyle='--')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def power_law(x, a, b):\n",
    "    'Calibration fit power law'\n",
    "    return a * x**b\n",
    "\n",
    "\n",
    "\n",
    "def compute_calibration(calibration_lines):\n",
    "\n",
    "    # Corresponding m/q argon values\n",
    "    mq_lines = [40,20,40/3,40/4,40/5]\n",
    "    \n",
    "    # Initial guesses for parameters a and b\n",
    "    initial_guess = [1.6e13, 2]#[1.9e20, 3]\n",
    "    \n",
    "    # Perform the curve fitting\n",
    "    params, covariance = curve_fit(power_law, calibration_lines, mq_lines, p0=initial_guess, maxfev=10000)\n",
    "\n",
    "    # Extract the fitted values for a and b\n",
    "    a_fit, b_fit = params\n",
    "\n",
    "    print(f\"The fit looks as follows: m/q = {a_fit:.2e} * tof^{b_fit:.2f}\")\n",
    "    \n",
    "    return a_fit, b_fit\n",
    "\n",
    "\n",
    "\n",
    "def calibrate(backgrd_dfevent):\n",
    "    'Computes calibration by least mean squares using backgrd_dfevent'\n",
    "    'Uses user input to compute fit based on displayed plots'\n",
    "    \n",
    "    # Show a large widget ion tof\n",
    "    big_ion_tof(backgrd_dfevent)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # Ask for five numbers input\n",
    "        anchors = []\n",
    "        for i in range(5):\n",
    "            value = input(f\"Enter value Ar{i + 1}: \")\n",
    "            try:\n",
    "                anchors.append(float(value))\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "        # Show five additional plots based on inputs\n",
    "        %matplotlib inline\n",
    "        for anchor in anchors:\n",
    "            zoomed_ion_tof(backgrd_dfevent,anchor)\n",
    "\n",
    "        # Ask if the user is done\n",
    "        done_response = input(\"Are you done? (y/n): \").strip().lower()\n",
    "        if done_response == 'y':\n",
    "            done = True\n",
    "    \n",
    "    # Compute calibration fit\n",
    "    a_fit, b_fit = compute_calibration(anchors)\n",
    "    \n",
    "    return a_fit, b_fit\n",
    "\n",
    "\n",
    "\n",
    "def apply_calibration(dfevents,a_fit,b_fit):\n",
    "    'Applies calibration to each dfevent of the list of dfevents and outputs calibrated_dfevents list of dataframes with m/q column'\n",
    "        \n",
    "    calibrated_dfevents = list()\n",
    "        \n",
    "    for dfevent in dfevents:\n",
    "        dfevent['mq'] = a_fit * dfevent.tof ** b_fit\n",
    "        calibrated_dfevents.append(dfevent)\n",
    "        \n",
    "    return calibrated_dfevents\n",
    "\n",
    "\n",
    "\n",
    "def mq_selection(calibrated_dfevent,dfpulse,etof,lower_mq,upper_mq):\n",
    "    'Selects based on m/q values. Need to input calibrated_dfevent! Returns m/q selected dfevent,dfpulse,etof.'\n",
    "    \n",
    "    mqselected_dfevent = calibrated_dfevent[lower_mq < calibrated_dfevent.mq][calibrated_dfevent.mq < upper_mq]\n",
    "    mqselected_dfpulse = dfpulse[dfpulse.pulseId.isin(mqselected_dfevent.pulseId)]\n",
    "    mqselected_etof = etof.sel(pulseId=etof.coords['pulseId'].isin(mqselected_dfevent.pulseId))\n",
    "    \n",
    "    return mqselected_dfevent,mqselected_dfpulse,mqselected_etof\n",
    "\n",
    "\n",
    "\n",
    "def find_rectangle_corners(zone):\n",
    "    \n",
    "    x, y, width, height, angle_degrees = zone\n",
    "    \n",
    "    angle_radians = np.deg2rad(angle_degrees)\n",
    "    c, s = np.cos(angle_radians), np.sin(angle_radians)\n",
    "    \n",
    "    # Calculate the coordinates of the other three corners\n",
    "    corners = np.array([\n",
    "        [x, y],\n",
    "        [x + width * c, y + width * s],\n",
    "        [x + width * c - height * s, y + width * s + height * c],\n",
    "        [x - height * s, y + height * c]\n",
    "    ])\n",
    "    \n",
    "    return corners\n",
    "\n",
    "\n",
    "\n",
    "def find_integer_coordinates(corners, zone):\n",
    "    \n",
    "    min_x, min_y = np.floor(np.min(corners, axis=0))\n",
    "    max_x, max_y = np.ceil(np.max(corners, axis=0))\n",
    "\n",
    "    integer_coordinates = []\n",
    "    for x in range(int(min_x), int(max_x) + 1):\n",
    "        for y in range(int(min_y), int(max_y) + 1):\n",
    "            if is_inside(x, y, corners, zone):\n",
    "                integer_coordinates.append((x, y))\n",
    "    \n",
    "    return integer_coordinates\n",
    "\n",
    "\n",
    "\n",
    "def is_inside(x, y, corners, zone):\n",
    "    \n",
    "    ox, oy, width, height, angle_degrees = zone\n",
    "    \n",
    "    angle_radians = np.deg2rad(angle_degrees)\n",
    "    c, s = np.cos(angle_radians), np.sin(angle_radians)\n",
    "    \n",
    "    rotated_x = (x-ox)*c + (y-oy)*s + ox\n",
    "    rotated_y = -(x-ox)*s + (y-oy)*c + oy\n",
    "    \n",
    "    return ox <= rotated_x <= ox + width and oy <= rotated_y <= oy + height\n",
    "\n",
    "\n",
    "\n",
    "def tilted_spatial_ion_selection(dfevent,dfpulse,etof,zones):\n",
    "    'Selection from the heatmap using spatial coordinates'\n",
    "    'Zones is a list of tuple representing zones - tilted or not - (xstart, ystart, width, height, angle in degrees)'\n",
    "    'Returns spatially selected dfevent,dfpulse,etof'\n",
    "    \n",
    "    heatmap_with_zones(dfevent,zones)\n",
    "    \n",
    "    integer_coordinates = []\n",
    "    for zone in zones:\n",
    "        corners = find_rectangle_corners(zone)\n",
    "        integer_coordinates.extend(find_integer_coordinates(corners, zone))\n",
    "    \n",
    "    x_coords, y_coords = zip(*integer_coordinates)\n",
    "    \n",
    "    spatial_selected_dfevent = dfevent[dfevent.x.isin(x_coords)][dfevent.y.isin(y_coords)]\n",
    "    spatial_selected_dfpulse = dfpulse[dfpulse.pulseId.isin(spatial_selected_dfevent.pulseId)]\n",
    "    spatial_selected_etof = etof.sel(pulseId=etof.coords['pulseId'].isin(spatial_selected_dfevent.pulseId))\n",
    "    \n",
    "    return spatial_selected_dfevent,spatial_selected_dfpulse,spatial_selected_etof\n",
    "\n",
    "\n",
    "\n",
    "def fish_plot_ion_selection(dfevent,zone):\n",
    "    'Selection from the heatmap using spatial coordinates'\n",
    "    'Zone is a tuple representing a zone - tilted or not - (xstart, ystart, width, height, angle in degrees)'\n",
    "    'Returns spatially selected dfevent'\n",
    "    \n",
    "    heatmap_with_zones(dfevent,[zone])\n",
    "    \n",
    "    corners = find_rectangle_corners(zone)\n",
    "    integer_coordinates = find_integer_coordinates(corners, zone)\n",
    "    \n",
    "    x_coords, y_coords = zip(*integer_coordinates)\n",
    "    \n",
    "    spatial_selected_dfevent = dfevent[dfevent.x.isin(x_coords)][dfevent.y.isin(y_coords)]\n",
    "    \n",
    "    return spatial_selected_dfevent\n",
    "    \n",
    "\n",
    "\n",
    "def fish_plot_x(dfevent,zone,tof_bins=1000,mq_bins=500):\n",
    "    'Produces fish plots along x with respect to time of flight and m/q from dfevent dataframe and a zone defined as (startx, starty, width, height, angle in degrees)'\n",
    "    \n",
    "    fish_dfevent = fish_plot_ion_selection(dfevent,zone)\n",
    "    \n",
    "    tof_bin_edges = np.linspace(0,TIME_BETWEEN_PULSES,tof_bins+1)\n",
    "    fish_dfevent['tof_binned'] = pd.cut(fish_dfevent['tof'], bins=tof_bin_edges, labels=tof_bin_edges[:-1].astype('str'))\n",
    "    tof_pivot_table = fish_dfevent.pivot_table(index='x', columns='tof_binned', aggfunc='size', fill_value=0)\n",
    "    max_tof_value = tof_pivot_table.values.max()\n",
    "    normalized_tof = tof_pivot_table.values / max_tof_value\n",
    "    \n",
    "    mq_bin_edges = np.linspace(0,200,mq_bins+1)\n",
    "    fish_dfevent['mq_binned'] = pd.cut(fish_dfevent['mq'], bins=mq_bin_edges, labels=mq_bin_edges[:-1].astype('str'))\n",
    "    mq_pivot_table = fish_dfevent.pivot_table(index='x', columns='mq_binned', aggfunc='size', fill_value=0)\n",
    "    max_mq_value = mq_pivot_table.values.max()\n",
    "    normalized_mq = mq_pivot_table.values / max_mq_value\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(30, 12))\n",
    "    \n",
    "    cax_tof = axes[0].imshow(normalized_tof, cmap='viridis', aspect='auto', norm=LogNorm(), extent=[tof_bin_edges.min(), tof_bin_edges.max(), 0, 1])\n",
    "    axes[0].set_xlabel('Time of flight (s)')\n",
    "    axes[0].set_xlim(0,5e-7)\n",
    "    axes[0].set_ylabel('x')\n",
    "    axes[0].set_yticklabels(np.linspace(256,0,6,dtype=int))\n",
    "    axes[0].set_title('Fish plot along x with respect to time of flight')\n",
    "    cbar = plt.colorbar(cax_tof, ax=axes[0], label='Normalized number of events', norm=LogNorm())\n",
    "    \n",
    "    cax_mq = axes[1].imshow(normalized_mq, cmap='viridis', aspect='auto', norm=LogNorm(), extent=[mq_bin_edges.min(), mq_bin_edges.max(), 0, 1])\n",
    "    axes[1].set_xlabel('m/q')\n",
    "    axes[1].set_ylabel('x')\n",
    "    axes[1].set_yticklabels(np.linspace(256,0,6,dtype=int))\n",
    "    axes[1].set_title('Fish plot along x with respect to m/q')\n",
    "    cbar = plt.colorbar(cax_mq, ax=axes[1], label='Normalized number of events', norm=LogNorm())\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "def fish_plot_y(dfevent,zone,tof_bins=1000,mq_bins=500):\n",
    "    'Produces a fish plot along y from dfevent dataframe and a zone defined as (startx, starty, width, height, angle in degrees)'\n",
    "    \n",
    "    fish_dfevent = fish_plot_ion_selection(dfevent,zone)\n",
    "    \n",
    "    tof_bin_edges = np.linspace(0,TIME_BETWEEN_PULSES,tof_bins+1)\n",
    "    fish_dfevent['tof_binned'] = pd.cut(fish_dfevent['tof'], bins=tof_bin_edges, labels=tof_bin_edges[:-1].astype('str'))\n",
    "    tof_pivot_table = fish_dfevent.pivot_table(index='y', columns='tof_binned', aggfunc='size', fill_value=0)\n",
    "    max_tof_value = tof_pivot_table.values.max()\n",
    "    normalized_tof = tof_pivot_table.values / max_tof_value\n",
    "    \n",
    "    mq_bin_edges = np.linspace(0,200,mq_bins+1)\n",
    "    fish_dfevent['mq_binned'] = pd.cut(fish_dfevent['mq'], bins=mq_bin_edges, labels=mq_bin_edges[:-1].astype('str'))\n",
    "    mq_pivot_table = fish_dfevent.pivot_table(index='y', columns='mq_binned', aggfunc='size', fill_value=0)\n",
    "    max_mq_value = mq_pivot_table.values.max()\n",
    "    normalized_mq = mq_pivot_table.values / max_mq_value\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(30, 12))\n",
    "    \n",
    "    cax_tof = axes[0].imshow(normalized_tof, cmap='viridis', aspect='auto', norm=LogNorm(), extent=[tof_bin_edges.min(), tof_bin_edges.max(), 0, 1])\n",
    "    axes[0].set_xlabel('Time of flight (s)')\n",
    "    axes[0].set_ylabel('y')\n",
    "    axes[0].set_yticklabels(np.linspace(256,0,6,dtype=int))\n",
    "    axes[0].set_title('Fish plot along y with respect to time of flight')\n",
    "    cbar = plt.colorbar(cax_tof, ax=axes[0], label='Normalized number of events', norm=LogNorm())\n",
    "    \n",
    "    cax_mq = axes[1].imshow(normalized_mq, cmap='viridis', aspect='auto', norm=LogNorm(), extent=[mq_bin_edges.min(), mq_bin_edges.max(), 0, 1])\n",
    "    axes[1].set_xlabel('m/q')\n",
    "    axes[1].set_ylabel('y')\n",
    "    axes[1].set_yticklabels(np.linspace(256,0,6,dtype=int))\n",
    "    axes[1].set_title('Fish plot along y with respect to m/q')\n",
    "    cbar = plt.colorbar(cax_mq, ax=axes[1], label='Normalized number of events', norm=LogNorm())\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def big_mq_plot(dfevent,nbins_mq=1500,xlimits=(0,200)):\n",
    "    'Plots big intensity vs m/q plot using dfevent'\n",
    "    \n",
    "    x_lower, x_upper = xlimits\n",
    "    \n",
    "    plt.figure(figsize=(18,8))\n",
    "        \n",
    "    hist, bin_edges = np.histogram(dfevent.mq, bins=np.linspace(0,200,nbins_mq+1))\n",
    "    plt.plot(bin_edges[:-1], hist/max(hist))\n",
    "    \n",
    "    plt.xlabel('m/q')\n",
    "    plt.ylabel('Relatively normalized number of hits per bin')\n",
    "    plt.title('Normalized ions time of flight')\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nevents_binning(dfevent,dfpulse,etof,nbins_events,nbins_mq):\n",
    "    'Binning dfevent dataframe into a number of bins nbins using number of events per pulse dfpulse.nevents_pulse'\n",
    "    'Outputs nbins sized list of tuples (filtered_dfevent, filtered_dfpulse) and nbins sized array of histograms'\n",
    "    'hists is divided by number of pulses in a certain number of events bin'\n",
    "    \n",
    "    nevents_min = min(dfpulse.nevents_pulse)\n",
    "    nevents_max = max(dfpulse.nevents_pulse)\n",
    "    \n",
    "    bins = np.linspace(nevents_min,nevents_max,nbins_events+1).astype(int)\n",
    "    \n",
    "    filtered_dfevents = []\n",
    "    filtered_dfpulses = []\n",
    "    filtered_etofs = []\n",
    "    hists = []\n",
    "    \n",
    "    for i in range(len(bins) - 1):\n",
    "\n",
    "        start_edge = bins[i]\n",
    "        end_edge = bins[i + 1]\n",
    "\n",
    "        filtered_dfpulse = dfpulse[(dfpulse.nevents_pulse >= start_edge) & (dfpulse.nevents_pulse < end_edge)]\n",
    "        filtered_dfevent = dfevent[dfevent.pulseId.isin(filtered_dfpulse.pulseId)]\n",
    "        filtered_etof = etof.sel(pulseId=etof.coords['pulseId'].isin(filtered_dfpulse.pulseId))\n",
    "\n",
    "        filtered_dfevents.append(filtered_dfevent)\n",
    "        filtered_dfpulses.append(filtered_dfpulse)\n",
    "        filtered_etofs.append(filtered_etof)\n",
    "        \n",
    "        hist, bin_edges = np.histogram(filtered_dfevent.mq, bins=np.linspace(0,200,nbins_mq+1),range=(0,200))\n",
    "        hists.append(hist/len(filtered_dfpulse))\n",
    "        \n",
    "    return filtered_dfevents, filtered_dfpulses, filtered_etofs, np.array(hists), bins\n",
    "\n",
    "\n",
    "\n",
    "def nevents_binning_tof(dfevent,dfpulse,etof,nbins_events,nbins_tof):\n",
    "    'Bins into time of flight bins'\n",
    "    'Binning dfevent dataframe into a number of bins nbins using number of events per pulse dfpulse.nevents_pulse'\n",
    "    'Outputs nbins sized list of tuples (filtered_dfevent, filtered_dfpulse) and nbins sized array of histograms'\n",
    "    'hists is divided by number of pulses in a certain number of events bin'\n",
    "    \n",
    "    nevents_min = min(dfpulse.nevents_pulse)\n",
    "    nevents_max = max(dfpulse.nevents_pulse)\n",
    "    \n",
    "    bins = np.linspace(nevents_min,nevents_max,nbins_events+1).astype(int)\n",
    "    \n",
    "    filtered_dfevents = []\n",
    "    filtered_dfpulses = []\n",
    "    filtered_etofs = []\n",
    "    hists = []\n",
    "    \n",
    "    for i in range(len(bins) - 1):\n",
    "\n",
    "        start_edge = bins[i]\n",
    "        end_edge = bins[i + 1]\n",
    "\n",
    "        filtered_dfpulse = dfpulse[(dfpulse.nevents_pulse >= start_edge) & (dfpulse.nevents_pulse < end_edge)]\n",
    "        filtered_dfevent = dfevent[dfevent.pulseId.isin(filtered_dfpulse.pulseId)]\n",
    "        filtered_etof = etof.sel(pulseId=etof.coords['pulseId'].isin(filtered_dfpulse.pulseId))\n",
    "\n",
    "        filtered_dfevents.append(filtered_dfevent)\n",
    "        filtered_dfpulses.append(filtered_dfpulse)\n",
    "        filtered_etofs.append(filtered_etof)\n",
    "        \n",
    "        hist, bin_edges = np.histogram(filtered_dfevent.tof, bins=np.linspace(0,TIME_BETWEEN_PULSES,nbins_tof+1),range=(0,TIME_BETWEEN_PULSES))\n",
    "        hists.append(hist/len(filtered_dfpulse))\n",
    "        \n",
    "    return filtered_dfevents, filtered_dfpulses, filtered_etofs, np.array(hists), bins\n",
    "\n",
    "\n",
    "\n",
    "def nions_binning(dfevent,dfpulse,nbins_events,nbins_mq):\n",
    "    'Only handles ions'\n",
    "    'Binning dfevent dataframe into a number of bins nbins using number of events per pulse dfpulse.nevents_pulse'\n",
    "    'Outputs nbins sized list of tuples (filtered_dfevent, filtered_dfpulse) and nbins sized array of histograms'\n",
    "    'hists is divided by number of pulses in a certain number of events bin'\n",
    "    \n",
    "    nevents_min = min(dfpulse.nevents_pulse)\n",
    "    nevents_max = max(dfpulse.nevents_pulse)\n",
    "    \n",
    "    bins = np.linspace(nevents_min,nevents_max,nbins_events+1).astype(int)\n",
    "    \n",
    "    filtered_dfevents = []\n",
    "    filtered_dfpulses = []\n",
    "    hists = []\n",
    "    \n",
    "    for i in range(len(bins) - 1):\n",
    "\n",
    "        start_edge = bins[i]\n",
    "        end_edge = bins[i + 1]\n",
    "\n",
    "        filtered_dfpulse = dfpulse[(dfpulse.nevents_pulse >= start_edge) & (dfpulse.nevents_pulse < end_edge)]\n",
    "        filtered_dfevent = dfevent[dfevent.pulseId.isin(filtered_dfpulse.pulseId)]\n",
    "\n",
    "        filtered_dfevents.append(filtered_dfevent)\n",
    "        filtered_dfpulses.append(filtered_dfpulse)\n",
    "        \n",
    "        hist, bin_edges = np.histogram(filtered_dfevent.mq, bins=np.linspace(0,200,nbins_mq+1),range=(0,200))\n",
    "        hists.append(hist/len(filtered_dfpulse))\n",
    "        \n",
    "    return filtered_dfevents, filtered_dfpulses, np.array(hists), bins\n",
    "\n",
    "\n",
    "\n",
    "def nions_binning_tof(dfevent,dfpulse,nbins_events,nbins_tof):\n",
    "    'Bins into time of flight bins'\n",
    "    'Only handles ions'\n",
    "    'Binning dfevent dataframe into a number of bins nbins using number of events per pulse dfpulse.nevents_pulse'\n",
    "    'Outputs nbins sized list of tuples (filtered_dfevent, filtered_dfpulse) and nbins sized array of histograms'\n",
    "    'hists is divided by number of pulses in a certain number of events bin'\n",
    "    \n",
    "    nevents_min = min(dfpulse.nevents_pulse)\n",
    "    nevents_max = max(dfpulse.nevents_pulse)\n",
    "    \n",
    "    bins = np.linspace(nevents_min,nevents_max,nbins_events+1).astype(int)\n",
    "    \n",
    "    filtered_dfevents = []\n",
    "    filtered_dfpulses = []\n",
    "    hists = []\n",
    "    \n",
    "    for i in range(len(bins) - 1):\n",
    "\n",
    "        start_edge = bins[i]\n",
    "        end_edge = bins[i + 1]\n",
    "\n",
    "        filtered_dfpulse = dfpulse[(dfpulse.nevents_pulse >= start_edge) & (dfpulse.nevents_pulse < end_edge)]\n",
    "        filtered_dfevent = dfevent[dfevent.pulseId.isin(filtered_dfpulse.pulseId)]\n",
    "\n",
    "        filtered_dfevents.append(filtered_dfevent)\n",
    "        filtered_dfpulses.append(filtered_dfpulse)\n",
    "        \n",
    "        hist, bin_edges = np.histogram(filtered_dfevent.tof, bins=np.linspace(0,TIME_BETWEEN_PULSES,nbins_tof+1),range=(0,TIME_BETWEEN_PULSES))\n",
    "        hists.append(hist/len(filtered_dfpulse))\n",
    "        \n",
    "    return filtered_dfevents, filtered_dfpulses, np.array(hists), bins\n",
    "\n",
    "\n",
    "\n",
    "def nevents_binning_plot(dfevent,dfpulse,nbins_events,nbins_mq,xlimits=(0,200)):\n",
    "    'Bins by m/q'\n",
    "    'Binning dfevent dataframe into a number of bins nbins using number of events per pulse dfpulse.nevents_pulse'\n",
    "    'Outputs nbins sized list of tuples (filtered_dfevent, filtered_dfpulse)'\n",
    "    \n",
    "    x_lower, x_upper = xlimits\n",
    "    \n",
    "    nevents_min = min(dfpulse.nevents_pulse)\n",
    "    nevents_max = max(dfpulse.nevents_pulse)\n",
    "    bins = np.linspace(nevents_min,nevents_max,nbins_events+1).astype(int)\n",
    "    \n",
    "    filtered_dfs = []\n",
    "    plt.figure(figsize=(20,8))\n",
    "    \n",
    "    for i in range(len(bins) - 1):\n",
    "\n",
    "        start_edge = bins[i]\n",
    "        end_edge = bins[i + 1]\n",
    "\n",
    "        filtered_dfpulse = dfpulse[(dfpulse.nevents_pulse >= start_edge) & (dfpulse.nevents_pulse < end_edge)]\n",
    "        filtered_dfevent = dfevent[dfevent.pulseId.isin(filtered_dfpulse.pulseId)]\n",
    "\n",
    "        filtered_dfs.append((filtered_dfevent,filtered_dfpulse))\n",
    "        \n",
    "        hist, bin_edges = np.histogram(filtered_dfevent.mq, bins=np.linspace(0,200,nbins_mq),range=(0,200))\n",
    "        \n",
    "        plt.plot(bin_edges[:-1], hist/max(hist), label=f'{start_edge}-{end_edge}')\n",
    "        \n",
    "    plt.xlabel('m/q')\n",
    "    plt.ylabel('Relatively normalized number of hits per bin')\n",
    "    plt.title('Normalized ions time of flight')\n",
    "    plt.legend()\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    plt.show()   \n",
    "        \n",
    "    return filtered_dfs\n",
    "\n",
    "    \n",
    "\n",
    "def nevents_binning_plot_tof(dfevent,dfpulse,nbins_events,nbins_tof,xlimits=(0,TIME_BETWEEN_PULSES)):\n",
    "    'Bins by time of flight'\n",
    "    'Binning dfevent dataframe into a number of bins nbins using number of events per pulse dfpulse.nevents_pulse'\n",
    "    'Outputs nbins sized list of tuples (filtered_dfevent, filtered_dfpulse)'\n",
    "    \n",
    "    x_lower, x_upper = xlimits\n",
    "    \n",
    "    nevents_min = min(dfpulse.nevents_pulse)\n",
    "    nevents_max = max(dfpulse.nevents_pulse)\n",
    "    bins = np.linspace(nevents_min,nevents_max,nbins_events+1).astype(int)\n",
    "    \n",
    "    filtered_dfs = []\n",
    "    plt.figure(figsize=(20,8))\n",
    "    \n",
    "    for i in range(len(bins) - 1):\n",
    "\n",
    "        start_edge = bins[i]\n",
    "        end_edge = bins[i + 1]\n",
    "\n",
    "        filtered_dfpulse = dfpulse[(dfpulse.nevents_pulse >= start_edge) & (dfpulse.nevents_pulse < end_edge)]\n",
    "        filtered_dfevent = dfevent[dfevent.pulseId.isin(filtered_dfpulse.pulseId)]\n",
    "\n",
    "        filtered_dfs.append((filtered_dfevent,filtered_dfpulse))\n",
    "        \n",
    "        hist, bin_edges = np.histogram(filtered_dfevent.tof, bins=np.linspace(0,TIME_BETWEEN_PULSES,nbins_tof),range=(0,TIME_BETWEEN_PULSES))\n",
    "        \n",
    "        plt.plot(bin_edges[:-1], hist/max(hist), label=f'{start_edge}-{end_edge}')\n",
    "        \n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Relatively normalized number of hits per bin')\n",
    "    plt.title('Normalized ions time of flight')\n",
    "    plt.legend()\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    plt.show()   \n",
    "        \n",
    "    return filtered_dfs\n",
    "\n",
    "\n",
    "\n",
    "def nevents_binning_cov(dfevent,dfpulse,etof,nbins_events,nbins_ion_tof,nbins_e_tof,max_ion_limit=TIME_BETWEEN_PULSES,max_e_limit=TIME_BETWEEN_PULSES):\n",
    "    'Binning dfevent and etof by numbers of events, and number of bins along time of flight'\n",
    "    'Can select maximal time limit for electrons and ions'\n",
    "    \n",
    "    channel_time = TIME_BETWEEN_PULSES/CHANNELS_PER_PULSE\n",
    "    \n",
    "    nevents_min = min(dfpulse.nevents_pulse)\n",
    "    nevents_max = max(dfpulse.nevents_pulse)\n",
    "    \n",
    "    bins = np.linspace(nevents_min,nevents_max,nbins_events+1).astype(int)\n",
    "    \n",
    "    max_int_ion_limit = int(max_ion_limit/channel_time)\n",
    "    new_int_ion_limit = max_int_ion_limit - max_int_ion_limit % nbins_ion_tof\n",
    "    new_ion_limit = new_int_ion_limit*channel_time\n",
    "    \n",
    "    max_int_e_limit = int(max_e_limit/channel_time)\n",
    "    new_int_e_limit = max_int_e_limit - max_int_e_limit % nbins_e_tof\n",
    "    e_group_size = int(new_int_e_limit/nbins_e_tof)\n",
    "    \n",
    "    shortened_dfevent = dfevent[dfevent.tof < new_ion_limit]\n",
    "    shortened_etof = etof[:,:new_int_e_limit]\n",
    "    \n",
    "    hists = []\n",
    "    hists_etof = []\n",
    "    \n",
    "    for i in range(len(bins) - 1):\n",
    "\n",
    "        start_edge = bins[i]\n",
    "        end_edge = bins[i + 1]\n",
    "\n",
    "        filtered_dfpulse = dfpulse[(dfpulse.nevents_pulse >= start_edge) & (dfpulse.nevents_pulse < end_edge)]\n",
    "        filtered_dfevent = shortened_dfevent[shortened_dfevent.pulseId.isin(filtered_dfpulse.pulseId)]\n",
    "        filtered_etof = shortened_etof.sel(pulseId=shortened_etof.coords['pulseId'].isin(filtered_dfpulse.pulseId))\n",
    "        \n",
    "        hist, bin_edges = np.histogram(filtered_dfevent.tof, bins=nbins_ion_tof)\n",
    "        hists.append(hist)\n",
    "        \n",
    "        numpy_etof = filtered_etof.to_numpy()\n",
    "        reshaped_etof = numpy_etof.reshape((numpy_etof.shape[0], -1, e_group_size))\n",
    "        summed_etof = np.sum(reshaped_etof, axis=-1)\n",
    "        avg_etof = -np.mean(summed_etof, axis=0)\n",
    "        hists_etof.append(avg_etof)\n",
    "        \n",
    "    hists = np.array(hists)\n",
    "    hists_etof = np.array(hists_etof)\n",
    "        \n",
    "    return hists, hists_etof, bins\n",
    "\n",
    "\n",
    "\n",
    "def stacked_ion_tof_max(hists,nbins_tof,bins,xlimits=(0,TIME_BETWEEN_PULSES),backgrd_dfevent=None):\n",
    "    \"Plots ion tof for each number of events defined on top of each other\"\n",
    "    \"Can determine number of bins along time of flight and limits along x\"\n",
    "    \n",
    "    precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "    x_lower, x_upper = xlimits\n",
    "    hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "    \n",
    "    x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    \n",
    "    for i in range(len(hists)):\n",
    "        \n",
    "        shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "        plt.plot(x_edges, shortened_hist/max(shortened_hist), label=f'{bins[i]}-{bins[i+1]}')\n",
    "        \n",
    "    if isinstance(backgrd_dfevent, pd.DataFrame):\n",
    "        hist, bin_edges = np.histogram(backgrd_dfevent.tof, bins=np.linspace(0,TIME_BETWEEN_PULSES,nbins_tof+1),range=(0,TIME_BETWEEN_PULSES))\n",
    "        shortened_hist = hist[hist_lower:hist_upper]\n",
    "        plt.plot(x_edges, shortened_hist/max(shortened_hist), label='Background')\n",
    "\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Individually normalized signal')\n",
    "    plt.title('Normalized ions time of flight')\n",
    "    plt.legend()\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def stacked_ion_tof_sum(hists,nbins_tof,bins,xlimits=(0,TIME_BETWEEN_PULSES),backgrd_dfevent=None):\n",
    "    \"Plots ion tof for each number of events defined on top of each other\"\n",
    "    \"Can determine number of bins along time of flight and limits along x\"\n",
    "    \n",
    "    precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "    x_lower, x_upper = xlimits\n",
    "    hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "    \n",
    "    x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    \n",
    "    for i in range(len(hists)):\n",
    "        \n",
    "        shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "        plt.plot(x_edges, shortened_hist/sum(shortened_hist), label=f'{bins[i]}-{bins[i+1]}')\n",
    "        \n",
    "    if isinstance(backgrd_dfevent, pd.DataFrame):\n",
    "        hist, bin_edges = np.histogram(backgrd_dfevent.tof, bins=np.linspace(0,TIME_BETWEEN_PULSES,nbins_tof+1),range=(0,TIME_BETWEEN_PULSES))\n",
    "        shortened_hist = hist[hist_lower:hist_upper]\n",
    "        plt.plot(x_edges, shortened_hist/sum(shortened_hist), label='Background')\n",
    "\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Individually normalized signal')\n",
    "    plt.title('Normalized ions time of flight')\n",
    "    plt.legend()\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    plt.show() \n",
    "    \n",
    "\n",
    "\n",
    "def waterfall_rel(hists,nbins_mq,xlimits=(0,200)):\n",
    "    \"Waterfall plot of relative normalization with respect to m/q using hists, which is a list of histograms\"\n",
    "\n",
    "    x_lower, x_upper = xlimits\n",
    "    nbins_events = len(hists)\n",
    "    bin_edges = np.linspace(0, 200, nbins_mq+1)\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    colormap = plt.cm.inferno\n",
    "    color_index = np.linspace(0.2, 0.8, nbins_events)\n",
    "\n",
    "    for i in range(nbins_events):\n",
    "        \n",
    "        hist_norm = hists[i] / max(hists[i]) + i\n",
    "\n",
    "        line_color = colormap(color_index[i])\n",
    "        plt.plot(bin_edges[:-1], hist_norm, color=line_color)\n",
    "\n",
    "    plt.title('Relative waterfall plot')\n",
    "    plt.xlabel('m/q')\n",
    "    plt.ylabel('Relative normalized counts')\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "def nevents_heatmap_rel(hists,nbins_mq,bins,xlimits=(0,200)):\n",
    "    \"Heatmap of relative normalized counts with number of events slices on the y axis, with respect to m/q on the x axis using hists, which is a list of histograms\"\n",
    "    \n",
    "    precision = 200/nbins_mq\n",
    "    x_lower, x_upper = xlimits\n",
    "    hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "    nbins_events = len(hists)\n",
    "    hists_norm = []\n",
    "\n",
    "    for i in range(nbins_events):\n",
    "\n",
    "        shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "        hist_norm = shortened_hist / max(shortened_hist)\n",
    "        hists_norm.append(hist_norm)\n",
    "\n",
    "    x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "    y_edges = bins[:-1]\n",
    "    X, Y = np.meshgrid(x_edges, y_edges)\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "    plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "    plt.xlabel('m/q')\n",
    "    plt.ylabel('Number of events slice')\n",
    "    plt.title('Relative heatmap for number of events slices with respect to m/q')\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def nevents_heatmap_rel_tof(hists,nbins_tof,bins,xlimits=(0,TIME_BETWEEN_PULSES)):\n",
    "    \"Heatmap of relative normalized counts with number of events slices on the y axis, with respect to tof on the x axis using hists, which is a list of histograms\"\n",
    "    \n",
    "    precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "    x_lower, x_upper = xlimits\n",
    "    hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "    nbins_events = len(hists)\n",
    "    hists_norm = []\n",
    "\n",
    "    for i in range(nbins_events):\n",
    "\n",
    "        shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "        hist_norm = shortened_hist / max(shortened_hist)\n",
    "        hists_norm.append(hist_norm)\n",
    "\n",
    "    x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "    X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "    plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Number of events slice')\n",
    "    plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    # plt.yticks(bins[:-1])\n",
    "    plt.show()\n",
    "   \n",
    "    \n",
    "    \n",
    "def waterfall_abs(hists,nbins_mq,xlimits=(0,200)):\n",
    "    \"Waterfall plot of absolute normalization with respect to m/q using hists, which is a list of histograms\"\n",
    "\n",
    "    x_lower, x_upper = xlimits\n",
    "    nbins_events = len(hists)\n",
    "    bin_edges = np.linspace(0, 200, nbins_mq+1)\n",
    "    hists_norm = hists/np.max(hists)\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    colormap = plt.cm.inferno\n",
    "    color_index = np.linspace(0.2, 0.8, nbins_events)\n",
    "\n",
    "    for i in range(nbins_events):\n",
    "\n",
    "        line_color = colormap(color_index[i])\n",
    "        plt.plot(bin_edges[:-1], hists_norm[i] + i, color=line_color)\n",
    "\n",
    "    plt.title('Absolute waterfall plot')\n",
    "    plt.xlabel('m/q')\n",
    "    plt.ylabel('Relative normalized counts')\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "def nevents_heatmap_abs(hists,nbins_mq,bins,xlimits=(0,200)):\n",
    "    \"Heatmap of absolute normalized counts with number of events slices on the y axis, with respect to m/q on the x axis using hists, which is a list of histograms\"\n",
    "    \n",
    "    precision = 200/nbins_mq\n",
    "    x_lower, x_upper = xlimits\n",
    "    hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "    hists_shortened = hists[:,hist_lower:hist_upper]\n",
    "    nbins_events = len(hists_shortened)\n",
    "    hists_norm = hists_shortened/np.max(hists_shortened)\n",
    "\n",
    "    x_edges = np.linspace(0, 200, nbins_mq)\n",
    "    y_edges = bins[:-1]\n",
    "    X, Y = np.meshgrid(x_edges, y_edges)\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "    plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "    plt.xlabel('m/q')\n",
    "    plt.ylabel('Number of events slice')\n",
    "    plt.title('Absolute heatmap for number of events slices with respect to m/q')\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def nevents_heatmap_abs_tof(hists,nbins_tof,bins,xlimits=(0,TIME_BETWEEN_PULSES)):\n",
    "    \"Heatmap of absolute normalized counts with number of events slices on the y axis, with respect to tof on the x axis using hists, which is a list of histograms\"\n",
    "    \n",
    "    precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "    x_lower, x_upper = xlimits\n",
    "    hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "    hists_shortened = hists[:,hist_lower:hist_upper]\n",
    "    nbins_events = len(hists_shortened)\n",
    "    hists_norm = hists_shortened/np.max(hists_shortened)\n",
    "\n",
    "    x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "    X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "    plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Number of events slice')\n",
    "    plt.title('Absolute heatmap for number of events slices with respect to tof')\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    # plt.yticks(bins[:-1])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def waterfall_etof(filtered_etofs,xlimits=(0,200)):\n",
    "    'Waterfall plot of etof data using list of etofs filtered_etofs'\n",
    "\n",
    "    x_lower, x_upper = xlimits\n",
    "    nbins = len(filtered_etofs)\n",
    "    channel_time = TIME_BETWEEN_PULSES/CHANNELS_PER_PULSE\n",
    "    xaxis = np.arange(14080)*channel_time\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    colormap = plt.cm.inferno\n",
    "    color_index = np.linspace(0.2, 0.8, nbins)\n",
    "\n",
    "    for i in range(nbins):\n",
    "        \n",
    "        summed_etof = -np.sum(filtered_etofs[i],axis=0)\n",
    "        line_color = colormap(color_index[i])\n",
    "        plt.plot(xaxis, summed_etof/np.max(summed_etof) + i, color=line_color)\n",
    "\n",
    "    plt.title('Relative electron waterfall plot')\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Normalized signal')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def mq_np_covariance(dfevent,mq_bins=200,log=True,vmin=None,vmax=None):\n",
    "    'Produces a positive and a negative covariance map of m/q vs m/q employing the numpy cov function'\n",
    "    'Uses dfevent as input, can select number of mq bins, can produce plot as log, standard, or between defined ranges'\n",
    "    \n",
    "    mq_bin_edges = np.linspace(0,200,mq_bins+1)\n",
    "\n",
    "    dfevent['mq_bin'] = pd.cut(dfevent['mq'], bins=mq_bin_edges)\n",
    "\n",
    "    result_matrix = pd.crosstab(dfevent['pulseId'], dfevent['mq_bin'])\n",
    "    result_numpy_matrix = result_matrix.values\n",
    "    \n",
    "    cov_matrix = np.cov(result_numpy_matrix, rowvar=False)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if log == True:\n",
    "        ax = sns.heatmap(cov_matrix, cmap='viridis', fmt='.2f', norm=LogNorm())\n",
    "    elif vmax == None:\n",
    "        ax = sns.heatmap(cov_matrix, cmap='viridis', fmt='.2f')\n",
    "    else:\n",
    "        ax = sns.heatmap(cov_matrix, cmap='viridis', fmt='.2f', vmin=0, vmax=vmax)\n",
    "\n",
    "    tick_positions = np.linspace(0, mq_bins, 11)\n",
    "    tick_labels = np.linspace(0, 200, 11).astype(int)\n",
    "\n",
    "    ax.set_xticks(tick_positions)\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    ax.set_yticks(tick_positions)\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.title('Positive Numpy Covariance Heatmap  m/q vs m/q')\n",
    "    plt.xlabel('m/q')\n",
    "    plt.ylabel('m/q')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if log == True:\n",
    "        ax = sns.heatmap(-cov_matrix, cmap='viridis', fmt='.2f', norm=LogNorm())\n",
    "    elif vmin == None:\n",
    "        ax = sns.heatmap(-cov_matrix, cmap='viridis', fmt='.2f')\n",
    "    else:\n",
    "        ax = sns.heatmap(-cov_matrix, cmap='viridis', fmt='.2f', vmin=0, vmax=-vmin)\n",
    "\n",
    "    tick_positions = np.linspace(0, mq_bins, 11)\n",
    "    tick_labels = np.linspace(0, 200, 11).astype(int)\n",
    "\n",
    "    ax.set_xticks(tick_positions)\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    ax.set_yticks(tick_positions)\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.title('Negative Numpy Covariance Heatmap  m/q vs m/q')\n",
    "    plt.xlabel('m/q')\n",
    "    plt.ylabel('m/q')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def mq_covariance_1d(dfevent,mq_bin_range,mq_bins=200):\n",
    "    'Produces 1d covariance of an m/q bin range vs m/q employing the numpy cov function'\n",
    "    'Uses dfevent as input, can select number of mq bins'\n",
    "    \n",
    "    mq_bin_edges = np.linspace(0,200,mq_bins+1)\n",
    "\n",
    "    dfevent['mq_bin'] = pd.cut(dfevent['mq'], bins=mq_bin_edges)\n",
    "\n",
    "    result_matrix = pd.crosstab(dfevent['pulseId'], dfevent['mq_bin'])\n",
    "    result_numpy_matrix = result_matrix.values\n",
    "\n",
    "    array_hyd = result_matrix.values[:,mq_bin_range[0]:mq_bin_range[1]].sum(axis=1)\n",
    "    shape = result_numpy_matrix.shape[1]\n",
    "\n",
    "    # Calculate the covariance between each row of result_numpy_matrix and array_hyd\n",
    "    covariances = np.array([np.cov(np.column_stack((result_numpy_matrix[:, j], array_hyd)), rowvar=False)[0, 1] for j in range(shape)])\n",
    "\n",
    "    xaxis = np.linspace(0,200,shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xaxis,covariances)\n",
    "    plt.xlabel('m/q')\n",
    "    plt.ylabel('Covariance with m/q')\n",
    "    plt.title(f'Covariance Plot between m/q bin range {mq_bin_range} and m/q')\n",
    "    ax.set_yscale('symlog', linthresh=10)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def fix_missing_row(dfevent,dfpulse,mq_bins=200):\n",
    "    'Fixes the missing row in dfevent dataframe when computing the cross-tabulation of pulseId and mq_bin'\n",
    "    \n",
    "    mq_bin_edges = np.linspace(0,200,mq_bins+1)\n",
    "\n",
    "    dfevent['mq_bin'] = pd.cut(dfevent['mq'], bins=mq_bin_edges)\n",
    "\n",
    "    result_matrix = pd.crosstab(dfevent['pulseId'], dfevent['mq_bin'])\n",
    "    result_numpy_matrix = result_matrix.values\n",
    "    \n",
    "    resultlist = result_matrix.index.to_list()\n",
    "    resultlist.append(0)\n",
    "    selectedlist = dfpulse[dfpulse.pulseId.isin(dfevent.pulseId)].pulseId.to_list()\n",
    "    truefalse = np.equal(resultlist,selectedlist)\n",
    "    first_instance = np.argmax(~truefalse)\n",
    "    missing_pulse = int(dfpulse.iloc[first_instance].pulseId)\n",
    "\n",
    "    new_dfevent = dfevent[dfevent.pulseId != missing_pulse]\n",
    "    \n",
    "    return new_dfevent\n",
    "\n",
    "\n",
    "\n",
    "def calc_corrs(array1, array2, pcovparams, alpha=1):\n",
    "    print('calculating covariance')\n",
    "\n",
    "    assert len(pcovparams)==len(array1)==len(array2)\n",
    "    numshots=len(array1)\n",
    "    \n",
    "    # heavy stuff\n",
    "    syx=np.einsum('ij,ik->jk', array1, array2)\n",
    "    print('calculated syx')\n",
    "    syi=np.einsum('ij,i->j', array1, pcovparams)\n",
    "    print('calculated syi')\n",
    "    six=np.einsum('ij,i->j', array2, pcovparams)\n",
    "    print('calculated six')\n",
    "\n",
    "    # lighter stuff\n",
    "    sy=array1.sum(axis=0)\n",
    "    sx=array2.sum(axis=0)\n",
    "    si=pcovparams.sum(axis=0)\n",
    "    \n",
    "    syy=(array1**2).sum(axis=0)\n",
    "    sxx=(array2**2).sum(axis=0)\n",
    "    sii=(pcovparams**2).sum()\n",
    "\n",
    "    sysx=np.outer(sy, sx)\n",
    "    sisx=si*sx\n",
    "    sysi=sy*si\n",
    "    \n",
    "    # calculate covariances\n",
    "    covyx=(syx-sysx/numshots)/(numshots-1)\n",
    "    covyi=(syi-sysi/numshots)/(numshots-1)\n",
    "    covix=(six-sisx/numshots)/(numshots-1)\n",
    "\n",
    "    covyy=(syy-sy**2/numshots)/(numshots-1)\n",
    "    covxx=(sxx-sx**2/numshots)/(numshots-1)\n",
    "    covii=(sii-si**2/numshots)/(numshots-1) # renamed from varii\n",
    "\n",
    "    # calculate partial covariances\n",
    "    pcovyx=(numshots-1)/(numshots-2) * (covyx - alpha * np.outer(covyi, covix)/covii)\n",
    "    pcovyy=(numshots-1)/(numshots-2) * (covyy - (covyi**2)/covii)\n",
    "    pcovxx=(numshots-1)/(numshots-2) * (covxx - (covix**2)/covii)\n",
    "    \n",
    "    # calculate correlation\n",
    "    corryx = covyx / np.sqrt(np.outer(covyy, covxx))\n",
    "    # calculate partial correlation\n",
    "    pcorryx = pcovyx / np.sqrt(np.outer(pcovyy, pcovxx))\n",
    "    \n",
    "    return covyx, pcovyx, corryx, pcorryx\n",
    "\n",
    "\n",
    "\n",
    "def mq_covariance(dfevent,dfpulse,mq_bins=200,log=True,vmin=None,vmax=None):\n",
    "    'Produces covariance maps of m/q vs m/q employing the calc_corrs function'\n",
    "    'Uses dfevent and dfpulse as inputs, can select number of mq bins, can produce plot as log, standard, or between defined ranges'\n",
    "    \n",
    "    mq_bin_edges = np.linspace(0,200,mq_bins+1)\n",
    "\n",
    "    dfevent['mq_bin'] = pd.cut(dfevent['mq'], bins=mq_bin_edges)\n",
    "\n",
    "    result_matrix = pd.crosstab(dfevent['pulseId'], dfevent['mq_bin'])\n",
    "    result_numpy_matrix = result_matrix.values\n",
    "    \n",
    "    nevents_pulse = dfpulse[dfpulse.pulseId.isin(dfevent.pulseId)].nevents_pulse\n",
    "    \n",
    "    covyx, pcovyx, corryx, pcorryx = calc_corrs(result_numpy_matrix, result_numpy_matrix, nevents_pulse)\n",
    "    \n",
    "    \n",
    "    tick_positions = np.linspace(0, mq_bins, 11)\n",
    "    tick_labels = np.linspace(0, 200, 11).astype(int)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if log == True:\n",
    "        ax = sns.heatmap(covyx, cmap='viridis', fmt='.2f', norm=LogNorm())\n",
    "\n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "        ax.set_yticks(tick_positions)\n",
    "        ax.set_yticklabels(tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        plt.title('Positive Covariance Heatmap m/q vs m/q')\n",
    "        plt.xlabel('m/q')\n",
    "        plt.ylabel('m/q')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = sns.heatmap(-covyx, cmap='viridis', fmt='.2f', norm=LogNorm())\n",
    "\n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "        ax.set_yticks(tick_positions)\n",
    "        ax.set_yticklabels(tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        plt.title('Negative Covariance Heatmap m/q vs m/q')\n",
    "        plt.xlabel('m/q')\n",
    "        plt.ylabel('m/q')\n",
    "        plt.show()\n",
    "    \n",
    "    elif vmax == None:\n",
    "        ax = sns.heatmap(covyx, cmap='seismic', fmt='.2f')\n",
    "        \n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "        ax.set_yticks(tick_positions)\n",
    "        ax.set_yticklabels(tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        plt.title('Covariance Heatmap m/q vs m/q')\n",
    "        plt.xlabel('m/q')\n",
    "        plt.ylabel('m/q')\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        ax = sns.heatmap(covyx, cmap='seismic', fmt='.2f', vmin=vmin, vmax=vmax)\n",
    "        \n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "        ax.set_yticks(tick_positions)\n",
    "        ax.set_yticklabels(tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        plt.title('Covariance Heatmap m/q vs m/q')\n",
    "        plt.xlabel('m/q')\n",
    "        plt.ylabel('m/q')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "def mq_partial_covariance(dfevent,dfpulse,mq_bins=200,alpha=1,log=True,vmin=None,vmax=None):\n",
    "    'Produces partial covariance maps of m/q vs m/q employing the calc_corrs function'\n",
    "    'Uses dfevent and dfpulse as inputs, can select number of mq bins and factor alpha, can produce plot as log, standard, or between defined ranges'\n",
    "    \n",
    "    mq_bin_edges = np.linspace(0,200,mq_bins+1)\n",
    "\n",
    "    dfevent['mq_bin'] = pd.cut(dfevent['mq'], bins=mq_bin_edges)\n",
    "\n",
    "    result_matrix = pd.crosstab(dfevent['pulseId'], dfevent['mq_bin'])\n",
    "    result_numpy_matrix = result_matrix.values\n",
    "    \n",
    "    nevents_pulse = dfpulse[dfpulse.pulseId.isin(dfevent.pulseId)].nevents_pulse\n",
    "    \n",
    "    covyx, pcovyx, corryx, pcorryx = calc_corrs(result_numpy_matrix, -result_numpy_matrix, nevents_pulse, alpha)\n",
    "    \n",
    "    \n",
    "    tick_positions = np.linspace(0, mq_bins, 11)\n",
    "    tick_labels = np.linspace(0, 200, 11).astype(int)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if log == True:\n",
    "        ax = sns.heatmap(pcovyx, cmap='viridis', fmt='.2f', norm=LogNorm())\n",
    "\n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "        ax.set_yticks(tick_positions)\n",
    "        ax.set_yticklabels(tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        plt.title('Positive Partial Covariance Heatmap m/q vs m/q')\n",
    "        plt.xlabel('m/q')\n",
    "        plt.ylabel('m/q')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = sns.heatmap(-pcovyx, cmap='viridis', fmt='.2f', norm=LogNorm())\n",
    "\n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "        ax.set_yticks(tick_positions)\n",
    "        ax.set_yticklabels(tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        plt.title('Negative Partial Covariance Heatmap m/q vs m/q')\n",
    "        plt.xlabel('m/q')\n",
    "        plt.ylabel('m/q')\n",
    "        plt.show()\n",
    "    \n",
    "    elif vmax == None:\n",
    "        ax = sns.heatmap(pcovyx, cmap='seismic', fmt='.2f')\n",
    "        \n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "        ax.set_yticks(tick_positions)\n",
    "        ax.set_yticklabels(tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        plt.title('Partial Covariance Heatmap m/q vs m/q')\n",
    "        plt.xlabel('m/q')\n",
    "        plt.ylabel('m/q')\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        ax = sns.heatmap(pcovyx, cmap='seismic', fmt='.2f', vmin=vmin, vmax=vmax)\n",
    "        \n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "        ax.set_yticks(tick_positions)\n",
    "        ax.set_yticklabels(tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        plt.title('Partial Covariance Heatmap m/q vs m/q')\n",
    "        plt.xlabel('m/q')\n",
    "        plt.ylabel('m/q')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def ion_ion_covariance(dfevent, dfpulse, nbins_ion_tof, max_ion_limit=TIME_BETWEEN_PULSES, alpha=1, log=True, vmin=None, vmax=None):\n",
    "    'Produces numpy covariance maps of ion tof vs ion tof employing the calc_corrs function'\n",
    "    \n",
    "    # Create bins for ion TOF\n",
    "    ion_tof_bin_edges = np.linspace(0, max_ion_limit, nbins_ion_tof+1)\n",
    "    dfevent['tof_bin'] = pd.cut(dfevent['tof'], bins=ion_tof_bin_edges)\n",
    "\n",
    "    # Create ion matrix\n",
    "    ion_matrix = pd.crosstab(dfevent['pulseId'], dfevent['tof_bin'])\n",
    "    \n",
    "    # Get number of events per pulse\n",
    "    nevents_pulse = dfpulse[dfpulse.pulseId.isin(ion_matrix.index.to_numpy())].nevents_pulse\n",
    "\n",
    "    # Calculate correlations\n",
    "    covyx, pcovyx, corryx, pcorryx = calc_corrs(ion_matrix.values, ion_matrix.values, nevents_pulse, alpha)\n",
    "\n",
    "    # Setup tick positions and labels\n",
    "    ion_tick_positions = np.linspace(0, nbins_ion_tof, 5).astype(int)\n",
    "    formatted_ion_tick_labels = np.linspace(0, max_ion_limit, 5)\n",
    "    \n",
    "    # Plotting functions\n",
    "    def plot_heatmap(data, title, is_negative=False):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        if log:\n",
    "            data_to_plot = -data if is_negative else data\n",
    "            ax = sns.heatmap(data_to_plot, cmap='viridis', fmt='.2f', norm=LogNorm())\n",
    "        elif vmax is None:\n",
    "            ax = sns.heatmap(data, cmap='seismic', fmt='.2f')\n",
    "        else:\n",
    "            ax = sns.heatmap(data, cmap='seismic', fmt='.2f', vmin=vmin, vmax=vmax)\n",
    "            \n",
    "        ax.set_yticks(ion_tick_positions)\n",
    "        ax.set_yticklabels(formatted_ion_tick_labels)\n",
    "        ax.set_xticks(ion_tick_positions)\n",
    "        ax.set_xticklabels(formatted_ion_tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "        plt.title(title)\n",
    "        plt.ylabel('Ion tof')\n",
    "        plt.xlabel('Ion tof')\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot covariance maps\n",
    "    if log:\n",
    "        plot_heatmap(covyx, 'Positive Covariance Heatmap ion tof vs ion tof')\n",
    "        plot_heatmap(covyx, 'Negative Covariance Heatmap ion tof vs ion tof', is_negative=True)\n",
    "    else:\n",
    "        plot_heatmap(covyx, 'Covariance Heatmap ion tof vs ion tof')\n",
    "    \n",
    "    # Plot partial covariance maps\n",
    "    if log:\n",
    "        plot_heatmap(pcovyx, 'Positive Partial Covariance Heatmap ion tof vs ion tof')\n",
    "        plot_heatmap(pcovyx, 'Negative Partial Covariance Heatmap ion tof vs ion tof', is_negative=True)\n",
    "    else:\n",
    "        plot_heatmap(pcovyx, 'Partial Covariance Heatmap ion tof vs ion tof')\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "def etof_ion_covariance(dfevent,dfpulse,etof,nbins_ion_tof,nbins_e_tof,max_ion_limit=TIME_BETWEEN_PULSES,max_e_limit=TIME_BETWEEN_PULSES,alpha=1,log=True,vmin=None,vmax=None):\n",
    "    'Produces numpy covariance maps of etof vs ion employing the calc_corrs function'\n",
    "    'Uses dfevent and etof as inputs, can select number of bins along ion tof and electron tof, can produce plot as log, standard, or between defined ranges, and set alpha'\n",
    "    'Can select maximal time limit for electrons and ions'\n",
    "    \n",
    "    ion_tof_bin_edges = np.linspace(0,max_ion_limit,nbins_ion_tof+1)\n",
    "    dfevent['tof_bin'] = pd.cut(dfevent['tof'], bins=ion_tof_bin_edges)\n",
    "\n",
    "    ion_matrix = pd.crosstab(dfevent['pulseId'], dfevent['tof_bin'])\n",
    "\n",
    "\n",
    "    coords_etof = etof.assign_coords(data=np.arange(0,TIME_BETWEEN_PULSES,channel_time))\n",
    "\n",
    "    e_tof_bin_edges = np.linspace(0,max_e_limit,nbins_e_tof+1)\n",
    "    binned_etof = coords_etof.groupby_bins(\"data\", e_tof_bin_edges).sum()\n",
    "    e_matrix = binned_etof.to_pandas()\n",
    "    \n",
    "    ion_list = ion_matrix.index.to_list()\n",
    "    # ion_list.append(0)\n",
    "    e_list = e_matrix.index.to_list()\n",
    "    truefalse = np.equal(ion_list,e_list)\n",
    "    first_instance = np.argmax(~truefalse)\n",
    "\n",
    "    new_e_matrix = e_matrix#.drop(e_list[first_instance])\n",
    "\n",
    "\n",
    "    nevents_pulse = dfpulse[dfpulse.pulseId.isin(ion_matrix.index.to_numpy())].nevents_pulse\n",
    "\n",
    "    covyx, pcovyx, corryx, pcorryx = calc_corrs(ion_matrix.values, -new_e_matrix.values, nevents_pulse, alpha)\n",
    "\n",
    "\n",
    "    ion_tick_positions = np.linspace(0, nbins_ion_tof, 5).astype(int)\n",
    "    formatted_ion_tick_labels = np.linspace(0, max_ion_limit, 5)\n",
    "\n",
    "    e_tick_positions = np.linspace(0, nbins_e_tof, 5).astype(int)\n",
    "    formatted_e_tick_labels = np.linspace(0, max_e_limit, 5)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if log == True:\n",
    "        ax = sns.heatmap(covyx, cmap='viridis', fmt='.2f', norm=LogNorm())\n",
    "        ax.set_yticks(ion_tick_positions)\n",
    "        ax.set_yticklabels(formatted_ion_tick_labels)\n",
    "        ax.set_xticks(e_tick_positions)\n",
    "        ax.set_xticklabels(formatted_e_tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "        plt.title('Positive Covariance Heatmap etof vs ion tof')\n",
    "        plt.ylabel('Ion tof')\n",
    "        plt.xlabel('Electron tof')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = sns.heatmap(-covyx, cmap='viridis', fmt='.2f', norm=LogNorm())\n",
    "        ax.set_yticks(ion_tick_positions)\n",
    "        ax.set_yticklabels(formatted_ion_tick_labels)\n",
    "        ax.set_xticks(e_tick_positions)\n",
    "        ax.set_xticklabels(formatted_e_tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "        plt.title('Negative Covariance Heatmap etof vs ion tof')\n",
    "        plt.ylabel('Ion tof')\n",
    "        plt.xlabel('Electron tof')\n",
    "        plt.show()\n",
    "    \n",
    "    elif vmax == None:\n",
    "        ax = sns.heatmap(covyx, cmap='seismic', fmt='.2f')\n",
    "        ax.set_yticks(ion_tick_positions)\n",
    "        ax.set_yticklabels(formatted_ion_tick_labels)\n",
    "        ax.set_xticks(e_tick_positions)\n",
    "        ax.set_xticklabels(formatted_e_tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "        plt.title('Covariance Heatmap etof vs ion tof')\n",
    "        plt.ylabel('Ion tof')\n",
    "        plt.xlabel('Electron tof')\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        ax = sns.heatmap(covyx, cmap='seismic', fmt='.2f', vmin=vmin, vmax=vmax)\n",
    "        ax.set_yticks(ion_tick_positions)\n",
    "        ax.set_yticklabels(formatted_ion_tick_labels)\n",
    "        ax.set_xticks(e_tick_positions)\n",
    "        ax.set_xticklabels(formatted_e_tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "        plt.title('Covariance Heatmap etof vs ion tof')\n",
    "        plt.ylabel('Ion tof')\n",
    "        plt.xlabel('Electron tof')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if log == True:\n",
    "        ax = sns.heatmap(pcovyx, cmap='viridis', fmt='.2f', norm=LogNorm())\n",
    "        ax.set_yticks(ion_tick_positions)\n",
    "        ax.set_yticklabels(formatted_ion_tick_labels)\n",
    "        ax.set_xticks(e_tick_positions)\n",
    "        ax.set_xticklabels(formatted_e_tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "        plt.title('Positive Partial Covariance Heatmap etof vs ion tof')\n",
    "        plt.ylabel('Ion tof')\n",
    "        plt.xlabel('Electron tof')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = sns.heatmap(-pcovyx, cmap='viridis', fmt='.2f', norm=LogNorm())\n",
    "        ax.set_yticks(ion_tick_positions)\n",
    "        ax.set_yticklabels(formatted_ion_tick_labels)\n",
    "        ax.set_xticks(e_tick_positions)\n",
    "        ax.set_xticklabels(formatted_e_tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "        plt.title('Negative Partial Covariance Heatmap etof vs ion tof')\n",
    "        plt.ylabel('Ion tof')\n",
    "        plt.xlabel('Electron tof')\n",
    "        plt.show()\n",
    "    \n",
    "    elif vmax == None:\n",
    "        ax = sns.heatmap(pcovyx, cmap='seismic', fmt='.2f')\n",
    "        ax.set_yticks(ion_tick_positions)\n",
    "        ax.set_yticklabels(formatted_ion_tick_labels)\n",
    "        ax.set_xticks(e_tick_positions)\n",
    "        ax.set_xticklabels(formatted_e_tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "        plt.title('Partial Covariance Heatmap etof vs ion tof')\n",
    "        plt.ylabel('Ion tof')\n",
    "        plt.xlabel('Electron tof')\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        ax = sns.heatmap(pcovyx, cmap='seismic', fmt='.2f', vmin=vmin, vmax=vmax)\n",
    "        ax.set_yticks(ion_tick_positions)\n",
    "        ax.set_yticklabels(formatted_ion_tick_labels)\n",
    "        ax.set_xticks(e_tick_positions)\n",
    "        ax.set_xticklabels(formatted_e_tick_labels)\n",
    "        ax.invert_yaxis()\n",
    "        plt.title('Partial Covariance Heatmap etof vs ion tof')\n",
    "        plt.ylabel('Ion tof')\n",
    "        plt.xlabel('Electron tof')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "def getBadPixelMask(avg_dark, dark_thr, std_dark, std_thr):\n",
    "    mask = np.ones_like(avg_dark)\n",
    "\n",
    "    mask[avg_dark>dark_thr]=0\n",
    "    mask[std_dark>std_thr]=0\n",
    "    mask[std_dark==0]=0\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "def correct_pnccd(pnccd, dark_pnccd):\n",
    "    'Correct pnccd using dark run pnccd input'\n",
    "    \n",
    "    avg_dark= dark_pnccd.mean(axis=0)\n",
    "    std_dark= dark_pnccd.std(axis=0)\n",
    "    \n",
    "    mask = getBadPixelMask(avg_dark, 60000, std_dark, 600)\n",
    "    \n",
    "    corr_pnccd = (pnccd - avg_dark)*mask\n",
    "    \n",
    "    pretty_pnccd = corr_pnccd.isel(dim_0=slice(412, 512), dim_1=slice(412, 612))\n",
    "    \n",
    "    return pretty_pnccd\n",
    "    \n",
    "    \n",
    "    \n",
    "def pnccd_image(corr_pnccd, trainId):\n",
    "    'Show pnccd image for given train id'\n",
    "    \n",
    "    image_data = corr_pnccd.sel(trainId=trainId).squeeze()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image_data, vmin=10)\n",
    "    plt.title(f'PNCCD image of train {trainId}')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def integrate_pnccd(corr_pnccd, trainId, prnt=True):\n",
    "    'Integrate upper half of pnncd for given train id'\n",
    "    \n",
    "    image_data = corr_pnccd.sel(trainId=trainId).squeeze()\n",
    "    number = np.round(image_data.where(image_data > 10).sum().item())\n",
    "    \n",
    "    if prnt:\n",
    "        print(f'Integrating yields {number}')\n",
    "    else:\n",
    "        return number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22648dc",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e699e32",
   "metadata": {},
   "source": [
    "## Selections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00f5f8",
   "metadata": {},
   "source": [
    "### with pnccd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNID = [232,233]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034220f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND1 = 100\n",
    "UPPER_BOUND1 = 500\n",
    "\n",
    "LOWER_BOUND2 = 500\n",
    "UPPER_BOUND2 = 1500\n",
    "\n",
    "LOWER_BOUND3 = 2000\n",
    "UPPER_BOUND3 = 5000\n",
    "\n",
    "THRESHOLDS = [(LOWER_BOUND1, UPPER_BOUND1), (LOWER_BOUND2, UPPER_BOUND2), (LOWER_BOUND3, UPPER_BOUND3)]\n",
    "\n",
    "selections = events_selection_plots(RUNID,THRESHOLDS)\n",
    "\n",
    "selected_dfevent1, selected_dfpulse1, selected_etof1, selected_pnccd1 = selections[0]\n",
    "selected_dfevent2, selected_dfpulse2, selected_etof2, selected_pnccd2 = selections[1]\n",
    "selected_dfevent3, selected_dfpulse3, selected_etof3, selected_pnccd3 = selections[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f732de",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND1 = 5000\n",
    "UPPER_BOUND1 = 8000\n",
    "\n",
    "LOWER_BOUND2 = 8000\n",
    "UPPER_BOUND2 = 12000\n",
    "\n",
    "LOWER_BOUND3 = 12000\n",
    "UPPER_BOUND3 = 20000\n",
    "\n",
    "THRESHOLDS = [(LOWER_BOUND1, UPPER_BOUND1), (LOWER_BOUND2, UPPER_BOUND2), (LOWER_BOUND3, UPPER_BOUND3)]\n",
    "\n",
    "selections = events_selection_plots(RUNID,THRESHOLDS)\n",
    "\n",
    "selected_dfevent4, selected_dfpulse4, selected_etof4, selected_pnccd4 = selections[0]\n",
    "selected_dfevent5, selected_dfpulse5, selected_etof5, selected_pnccd5 = selections[1]\n",
    "selected_dfevent6, selected_dfpulse6, selected_etof6, selected_pnccd6 = selections[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 5000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "selections = events_selection_plots(RUNID,THRESHOLD)\n",
    "selected_dfevent, selected_dfpulse, selected_etof, selected_pnccd = selections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BACKGRD_BOUND = 20\n",
    "UPPER_BACKGRD_BOUND = 40\n",
    "BKGRD_THRESHOLD = [(LOWER_BACKGRD_BOUND,UPPER_BACKGRD_BOUND)]\n",
    "DOWNSAMPLING = 200000\n",
    "\n",
    "backgrd_dfevent, backgrd_dfpulse, backgrd_etof, backgrd_pnccd = events_selection(RUNID,BKGRD_THRESHOLD,DOWNSAMPLING)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c04bc",
   "metadata": {},
   "source": [
    "### only ions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41374482",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfevent_90, dfpulse_90 = read_ion(405)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfevent_33, dfpulse_33 = read_ion(411)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30808263",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfevent, dfpulse = read_ion(413)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_threshold, upper_threshold = 500, 8000\n",
    "\n",
    "selected_dfpulse = dfpulse[lower_threshold < dfpulse.nevents_pulse][dfpulse.nevents_pulse < upper_threshold]\n",
    "selected_dfevent = dfevent[dfevent.pulseId.isin(selected_dfpulse.pulseId)]\n",
    "\n",
    "selected_dfpulse_90 = dfpulse_90[lower_threshold < dfpulse_90.nevents_pulse][dfpulse_90.nevents_pulse < upper_threshold]\n",
    "selected_dfevent_90 = dfevent_90[dfevent_90.pulseId.isin(selected_dfpulse_90.pulseId)]\n",
    "\n",
    "selected_dfpulse_33 = dfpulse_33[lower_threshold < dfpulse_33.nevents_pulse][dfpulse_33.nevents_pulse < upper_threshold]\n",
    "selected_dfevent_33 = dfevent_33[dfevent_33.pulseId.isin(selected_dfpulse_33.pulseId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounded_dfevent = selected_dfevent[selected_dfevent.tof < TIME_BETWEEN_PULSES]\n",
    "hist, bin_edges = np.histogram(bounded_dfevent.tof, bins=1000)\n",
    "\n",
    "bounded_dfevent_90 = selected_dfevent_90[selected_dfevent_90.tof < TIME_BETWEEN_PULSES]\n",
    "hist_90, bin_edges = np.histogram(bounded_dfevent_90.tof, bins=1000)\n",
    "\n",
    "bounded_dfevent_33 = selected_dfevent_33[selected_dfevent_33.tof < TIME_BETWEEN_PULSES]\n",
    "hist_33, bin_edges = np.histogram(bounded_dfevent_33.tof, bins=1000)   \n",
    " \n",
    "plt.figure()\n",
    "plt.plot(bin_edges[:-1], hist/max(hist), label='EtOH background')\n",
    "plt.plot(bin_edges[:-1], hist_90/max(hist_90), label='90% EtOH')\n",
    "plt.plot(bin_edges[:-1], hist_33/max(hist_33), label='33% EtOH')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of hits per bin')\n",
    "plt.title('Ions time of flight')\n",
    "# plt.xlim(0,TIME_BETWEEN_PULSES)\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89599a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7594V 500nm\n",
    "RUNIDstretch = [376,380,382,383,384,386,387,388,389,390,391,393,398,399,400,402,403,404]\n",
    "# 5400V\n",
    "# 500nm\n",
    "RUNID500 = [232,233,313,315,316,317,318,319,320,321,322,325,326,327,328,329,331,332,333,336,337,338,339,340,341,342,343,345,347,348,349]\n",
    "# 300nm\n",
    "RUNID300 = [195,196,197,198,199,200,201,202,203,205,206,207,208,213,214,215,216,217,218,219,220,221,222,223,224,225,226]\n",
    "# 100nm\n",
    "RUNID100 = [293,294,296,297,298,299,300,301,302,303,304,306,307,308,309]\n",
    "# core shell\n",
    "RUNIDcoreshell = [227,228,229,273,274,275,276,277,278,279,280,282,283,284,285,286,287]\n",
    "# water\n",
    "RUNIDwater = [211]\n",
    "# alcohol\n",
    "RUNIDstrongalcohol = [405,406,407]\n",
    "RUNIDweakalcohol = [409,411,412,413]\n",
    "# photon energy\n",
    "RUNIDphotonenergy = [440,441,441,442,443,444,445,446,447,448,449,450,451]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8851093",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND1 = 100\n",
    "UPPER_BOUND1 = 500\n",
    "\n",
    "LOWER_BOUND2 = 500\n",
    "UPPER_BOUND2 = 1500\n",
    "\n",
    "LOWER_BOUND3 = 2000\n",
    "UPPER_BOUND3 = 5000\n",
    "\n",
    "THRESHOLDS = [(LOWER_BOUND1, UPPER_BOUND1), (LOWER_BOUND2, UPPER_BOUND2), (LOWER_BOUND3, UPPER_BOUND3)]\n",
    "\n",
    "ion_selections = ion_selection(RUNID500,THRESHOLDS)\n",
    "\n",
    "ion_dfevent1, ion_dfpulse1 = ion_selections[0]\n",
    "ion_dfevent2, ion_dfpulse2 = ion_selections[1]\n",
    "ion_dfevent3, ion_dfpulse3 = ion_selections[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND1 = 5000\n",
    "UPPER_BOUND1 = 8000\n",
    "\n",
    "LOWER_BOUND2 = 8000\n",
    "UPPER_BOUND2 = 12000\n",
    "\n",
    "LOWER_BOUND3 = 12000\n",
    "UPPER_BOUND3 = 20000\n",
    "\n",
    "THRESHOLDS = [(LOWER_BOUND1, UPPER_BOUND1), (LOWER_BOUND2, UPPER_BOUND2), (LOWER_BOUND3, UPPER_BOUND3)]\n",
    "\n",
    "ion_selections = ion_selection(RUNID,THRESHOLDS)\n",
    "\n",
    "ion_dfevent4, ion_dfpulse4 = ion_selections[0]\n",
    "ion_dfevent5, ion_dfpulse5 = ion_selections[1]\n",
    "ion_dfevent6, ion_dfpulse6 = ion_selections[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 10000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "ion_dfevent500, ion_dfpulse500 = ion_selection(RUNID500,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 10000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "ion_dfevent500, ion_dfpulse500 = ion_selection(RUNID500,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95680c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 10000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "ion_dfevent300, ion_dfpulse300 = ion_selection(RUNID300,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eedb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 5000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "ion_dfevent100, ion_dfpulse100 = ion_selection(RUNID100,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 5000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "ion_dfevent_strongalcohol, ion_dfpulse_strongalcohol = ion_selection(RUNIDstrongalcohol,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fcd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 5000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "ion_dfevent_weakalcohol, ion_dfpulse_weakalcohol = ion_selection(RUNIDweakalcohol,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 2500\n",
    "UPPER_BOUND = 3000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "newion_dfevent300, newion_dfpulse300 = ion_selection(RUNID300,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce181c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND1 = 1000\n",
    "UPPER_BOUND1 = 1500\n",
    "\n",
    "LOWER_BOUND2 = 1500\n",
    "UPPER_BOUND2 = 2000\n",
    "\n",
    "LOWER_BOUND3 = 2000\n",
    "UPPER_BOUND3 = 2500\n",
    "\n",
    "THRESHOLDS = [(LOWER_BOUND1, UPPER_BOUND1), (LOWER_BOUND2, UPPER_BOUND2), (LOWER_BOUND3, UPPER_BOUND3)]\n",
    "\n",
    "ion_selections = ion_selection(RUNID300,THRESHOLDS)\n",
    "\n",
    "ion_dfevent1, ion_dfpulse1 = ion_selections[0]\n",
    "ion_dfevent2, ion_dfpulse2 = ion_selections[1]\n",
    "ion_dfevent3, ion_dfpulse3 = ion_selections[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 5000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "ion_dfevent, ion_dfpulse = ion_selection(RUNIDphotonenergy,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ccd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 5000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "coreshell_ion_dfevent, coreshell_ion_dfpulse = ion_selection(RUNIDcoreshell,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 50\n",
    "UPPER_BOUND = 17500\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "ion_dfeventcoreshell, ion_dfpulsecoreshell = ion_selection(RUNIDcoreshell,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b612f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 50\n",
    "UPPER_BOUND = 17500\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "\n",
    "ion_dfeventwater, ion_dfpulsewater = ion_selection(RUNIDwater,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BACKGRD_BOUND = 20\n",
    "UPPER_BACKGRD_BOUND = 40\n",
    "BKGRD_THRESHOLD = [(LOWER_BACKGRD_BOUND,UPPER_BACKGRD_BOUND)]\n",
    "DOWNSAMPLING = 200000\n",
    "\n",
    "backgrd_ion_dfevent500, backgrd_ion_dfpulse500 = ion_selection(RUNID500,BKGRD_THRESHOLD,DOWNSAMPLING)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ef45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BACKGRD_BOUND = 20\n",
    "UPPER_BACKGRD_BOUND = 40\n",
    "BKGRD_THRESHOLD = [(LOWER_BACKGRD_BOUND,UPPER_BACKGRD_BOUND)]\n",
    "DOWNSAMPLING = 100000\n",
    "\n",
    "backgrd_ion_dfevent300, backgrd_ion_dfpulse300 = ion_selection(RUNID300,BKGRD_THRESHOLD,DOWNSAMPLING)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BACKGRD_BOUND = 20\n",
    "UPPER_BACKGRD_BOUND = 40\n",
    "BKGRD_THRESHOLD = [(LOWER_BACKGRD_BOUND,UPPER_BACKGRD_BOUND)]\n",
    "DOWNSAMPLING = 100000\n",
    "\n",
    "backgrd_ion_dfevent100, backgrd_ion_dfpulse100 = ion_selection(RUNID100,BKGRD_THRESHOLD,DOWNSAMPLING)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76868bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 100\n",
    "nevents_binning = np.linspace(500, 17000, num_bins)\n",
    "\n",
    "bin_counts = pd.cut(ion_dfpulse500['nevents_pulse'], bins=nevents_binning).value_counts().sort_index()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(nevents_binning[:-1], bin_counts)\n",
    "plt.xlabel('Number of events per pulse')\n",
    "plt.ylabel('Number of pulses')\n",
    "plt.title('Number of pulses per number of events bin')\n",
    "plt.yscale('log')\n",
    "plt.axvline(1300)\n",
    "plt.axvline(3400)\n",
    "plt.axvline(6000)\n",
    "plt.axvline(10000) \n",
    "plt.axvline(14000)\n",
    "plt.axvline(16500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [200, 1300, 3400, 6000, 10000]  # Define the bin ranges\n",
    "labels = ['regime1', 'regime2', 'regime3', 'regime4']  # Assign labels to the bins\n",
    "\n",
    "# Create a new column 'bin' with the bin labels\n",
    "ion_dfpulse500['nevents_pulse_bin'] = pd.cut(ion_dfpulse500['nevents_pulse'], bins=bins, labels=labels)\n",
    "\n",
    "# Group the DataFrame by the 'bin' column and create separate DataFrames\n",
    "dataframes = {group: df for group, df in ion_dfpulse500.groupby('nevents_pulse_bin')}\n",
    "\n",
    "# Access the individual DataFrames\n",
    "regime1_ion_dfpulse = dataframes['regime1']\n",
    "regime2_ion_dfpulse = dataframes['regime2']\n",
    "regime3_ion_dfpulse = dataframes['regime3']\n",
    "regime4_ion_dfpulse = dataframes['regime4']\n",
    "\n",
    "ion_dfevent500 = ion_dfevent500[ion_dfevent500.tof < TIME_BETWEEN_PULSES]\n",
    "\n",
    "regime1_ion_dfevent = ion_dfevent500[ion_dfevent500.pulseId.isin(regime1_ion_dfpulse.pulseId)]\n",
    "regime2_ion_dfevent = ion_dfevent500[ion_dfevent500.pulseId.isin(regime2_ion_dfpulse.pulseId)]\n",
    "regime3_ion_dfevent = ion_dfevent500[ion_dfevent500.pulseId.isin(regime3_ion_dfpulse.pulseId)]\n",
    "regime4_ion_dfevent = ion_dfevent500[ion_dfevent500.pulseId.isin(regime4_ion_dfpulse.pulseId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [50, 1300, 3400, 6000, 10000]  # Define the bin ranges\n",
    "labels = ['regime1', 'regime2', 'regime3', 'regime4']  # Assign labels to the bins\n",
    "\n",
    "# Create a new column 'bin' with the bin labels\n",
    "ion_dfpulse300['nevents_pulse_bin'] = pd.cut(ion_dfpulse300['nevents_pulse'], bins=bins, labels=labels)\n",
    "\n",
    "# Group the DataFrame by the 'bin' column and create separate DataFrames\n",
    "dataframes = {group: df for group, df in ion_dfpulse300.groupby('nevents_pulse_bin')}\n",
    "\n",
    "# Access the individual DataFrames\n",
    "regime1_ion_dfpulse = dataframes['regime1']\n",
    "regime2_ion_dfpulse = dataframes['regime2']\n",
    "regime3_ion_dfpulse = dataframes['regime3']\n",
    "regime4_ion_dfpulse = dataframes['regime4']\n",
    "\n",
    "ion_dfevent300 = ion_dfevent300[ion_dfevent300.tof < TIME_BETWEEN_PULSES]\n",
    "\n",
    "regime1_ion_dfevent = ion_dfevent300[ion_dfevent300.pulseId.isin(regime1_ion_dfpulse.pulseId)]\n",
    "regime2_ion_dfevent = ion_dfevent300[ion_dfevent300.pulseId.isin(regime2_ion_dfpulse.pulseId)]\n",
    "regime3_ion_dfevent = ion_dfevent300[ion_dfevent300.pulseId.isin(regime3_ion_dfpulse.pulseId)]\n",
    "regime4_ion_dfevent = ion_dfevent300[ion_dfevent300.pulseId.isin(regime4_ion_dfpulse.pulseId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF = 2000\n",
    "hist1, bin_edges = np.histogram(regime1_ion_dfevent.tof, bins=NBINS_TOF)\n",
    "hist2, bin_edges = np.histogram(regime2_ion_dfevent.tof, bins=NBINS_TOF)\n",
    "hist3, bin_edges = np.histogram(regime3_ion_dfevent.tof, bins=NBINS_TOF)\n",
    "hist4, bin_edges = np.histogram(regime4_ion_dfevent.tof, bins=NBINS_TOF)\n",
    "regimehists = [hist1,hist2,hist3,hist4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe32b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ion_tof_max(regimehists,NBINS_TOF,bins,(3.5e-7,7e-7),backgrd_ion_dfevent500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ion_tof_max(regimehists,NBINS_TOF,bins,(1e-7,5e-7),backgrd_ion_dfevent500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ion_tof_max(regimehists,NBINS_TOF,bins,(1.05e-6,1.4e-6),backgrd_ion_dfevent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ion_tof_max(regimehists,NBINS_TOF,bins,(1.4e-6,1.8e-6),backgrd_ion_dfevent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e122c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ion_tof_max(regimehists,NBINS_TOF,bins,(1.8e-6,2.6e-6),backgrd_ion_dfevent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54fe9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_dfpulse_max = ion_dfpulse500[ion_dfpulse500.nevents_pulse < 4000]\n",
    "ion_dfevent_max = ion_dfevent500[ion_dfevent500.pulseId.isin(ion_dfpulse_max.pulseId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_dfpulse_lower_max = ion_dfpulse500[ion_dfpulse500.nevents_pulse < 1000]\n",
    "ion_dfevent_lower_max = ion_dfevent500[ion_dfevent500.pulseId.isin(ion_dfpulse_lower_max.pulseId)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae01c33c",
   "metadata": {},
   "source": [
    "### transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdb98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transmission percentages\n",
    "RUNID1 = [376,380,382,383,384,398]\n",
    "RUNID05 = [386,387,388,389,390,391,393,399]\n",
    "RUNID025 = [400,402,403]\n",
    "RUNID012 = [404]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a00e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 50\n",
    "UPPER_BOUND = 30000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "    \n",
    "ion_dfevent10, ion_dfpulse10 = ion_selection(RUNID1,THRESHOLD)[0]\n",
    "ion_dfevent05, ion_dfpulse05 = ion_selection(RUNID05,THRESHOLD)[0]\n",
    "ion_dfevent025, ion_dfpulse025 = ion_selection(RUNID025,THRESHOLD)[0]\n",
    "ion_dfevent012, ion_dfpulse012 = ion_selection(RUNID012,THRESHOLD)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND1 = 100\n",
    "UPPER_BOUND1 = 1000\n",
    "\n",
    "LOWER_BOUND2 = 1000\n",
    "UPPER_BOUND2 = 5000\n",
    "\n",
    "LOWER_BOUND3 = 5000\n",
    "UPPER_BOUND3 = 10000\n",
    "\n",
    "THRESHOLDS = [(LOWER_BOUND1, UPPER_BOUND1), (LOWER_BOUND2, UPPER_BOUND2), (LOWER_BOUND3, UPPER_BOUND3)]\n",
    "\n",
    "ion_selections = ion_selection(RUNID1,THRESHOLDS)\n",
    "\n",
    "ion_dfevent10_1, ion_dfpulse10_1 = ion_selections[0]\n",
    "ion_dfevent10_2, ion_dfpulse10_2 = ion_selections[1]\n",
    "ion_dfevent10_3, ion_dfpulse10_3 = ion_selections[2]\n",
    "\n",
    "ion_selections = ion_selection(RUNID05,THRESHOLDS)\n",
    "\n",
    "ion_dfevent05_1, ion_dfpulse05_1 = ion_selections[0]\n",
    "ion_dfevent05_2, ion_dfpulse05_2 = ion_selections[1]\n",
    "ion_dfevent05_3, ion_dfpulse05_3 = ion_selections[2]\n",
    "\n",
    "ion_selections = ion_selection(RUNID025,THRESHOLDS)\n",
    "\n",
    "ion_dfevent025_1, ion_dfpulse025_1 = ion_selections[0]\n",
    "ion_dfevent025_2, ion_dfpulse025_2 = ion_selections[1]\n",
    "ion_dfevent025_3, ion_dfpulse025_3 = ion_selections[2]\n",
    "\n",
    "ion_selections = ion_selection(RUNID012,THRESHOLDS)\n",
    "\n",
    "ion_dfevent012_1, ion_dfpulse012_1 = ion_selections[0]\n",
    "ion_dfevent012_2, ion_dfpulse012_2 = ion_selections[1]\n",
    "ion_dfevent012_3, ion_dfpulse012_3 = ion_selections[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 100\n",
    "nevents_binning = np.linspace(LOWER_BOUND, UPPER_BOUND, num_bins)\n",
    "\n",
    "bin_counts = pd.cut(ion_dfpulse['nevents_pulse'], bins=nevents_binning).value_counts().sort_index()\n",
    "bin_counts10 = pd.cut(ion_dfpulse10['nevents_pulse'], bins=nevents_binning).value_counts().sort_index()\n",
    "bin_counts05 = pd.cut(ion_dfpulse05['nevents_pulse'], bins=nevents_binning).value_counts().sort_index()\n",
    "bin_counts025 = pd.cut(ion_dfpulse025['nevents_pulse'], bins=nevents_binning).value_counts().sort_index()\n",
    "bin_counts012 = pd.cut(ion_dfpulse012['nevents_pulse'], bins=nevents_binning).value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(nevents_binning[:-1], bin_counts/max(bin_counts), label='total')\n",
    "plt.plot(nevents_binning[:-1], bin_counts10/max(bin_counts10), label='1% transmission')\n",
    "plt.plot(nevents_binning[:-1], bin_counts05/max(bin_counts05), label='.5% transmisison')\n",
    "plt.plot(nevents_binning[:-1], bin_counts025/max(bin_counts025), label='.25% transmisison')\n",
    "plt.plot(nevents_binning[:-1], bin_counts012/max(bin_counts012), label='.12% transmisison')\n",
    "plt.xlabel('Number of events per pulse')\n",
    "plt.ylabel('Number of pulses')\n",
    "plt.title('Number of pulses per number of events bin')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ce86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BACKGRD_BOUND = 20\n",
    "UPPER_BACKGRD_BOUND = 40\n",
    "BKGRD_THRESHOLD = [(LOWER_BACKGRD_BOUND,UPPER_BACKGRD_BOUND)]\n",
    "DOWNSAMPLING = 200000\n",
    "\n",
    "backgrd_ion_dfevent, backgrd_ion_dfpulse = ion_selection(RUNID,BKGRD_THRESHOLD,DOWNSAMPLING)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba60432",
   "metadata": {},
   "source": [
    "### Ions and photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5400V\n",
    "# 500nm\n",
    "# remove no pnccd runs\n",
    "RUNID500 = [232,233,315,316,317,318,319,320,322,325,326,327,328,329,331,332,333,336,337,338,339,340,341,342,343,345,347,348,349]\n",
    "# 300nm\n",
    "RUNID300 = [195,196,197,198,199,200,201,202,203,205,206,207,208,213,214,215,216,217,218,219,220,221,222,223,224,225,226]\n",
    "# 100nm\n",
    "RUNID100 = [293,294,296,297,298,299,300,301,302,303,304,306,307,308,309]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0438e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND1 = 1000\n",
    "UPPER_BOUND1 = 5000\n",
    "\n",
    "LOWER_BOUND2 = 5000\n",
    "UPPER_BOUND2 = 7000\n",
    "\n",
    "LOWER_BOUND3 = 8000\n",
    "UPPER_BOUND3 = 10000\n",
    "\n",
    "THRESHOLDS = [(LOWER_BOUND1, UPPER_BOUND1)]#, (LOWER_BOUND2, UPPER_BOUND2), (LOWER_BOUND3, UPPER_BOUND3)]\n",
    "\n",
    "selections = events_selection_ions_photons(RUNID100,THRESHOLDS)\n",
    "\n",
    "selected_dfevent1, selected_dfpulse1, selected_pnccd1 = selections[0]\n",
    "#selected_dfevent2, selected_dfpulse2, selected_pnccd2 = selections[1]\n",
    "#selected_dfevent3, selected_dfpulse3, selected_pnccd3 = selections[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857765a",
   "metadata": {},
   "source": [
    "### Ions and electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a78271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5400V\n",
    "# 500nm\n",
    "# remove no pnccd runs\n",
    "RUNID500 = [315,316,317,318,319,320,322,325,326,327,328,329,331,332,333]#,336,337,338,339,340,341,342,343,345,347,348,349]#[232,233,\n",
    "RUNID300 = [195,196,197,198,199,200,201,202,203,205,206,207,208,213,214,215,216,217,218,219,220,221,222,223,224,225,226]\n",
    "RUNID100 = [293,294,296,297,298,299,300,301,302,303,304,306,307,308,309]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7198f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 10000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "TOF_LIMIT = 6e-7\n",
    "\n",
    "selected_dfevent, selected_dfpulse, selected_etof = events_selection_ions_electrons(RUNID500,THRESHOLD,tof_limit=TOF_LIMIT)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_etof.to_netcdf(\"selected_etof.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 10000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "TOF_LIMIT = 6e-7\n",
    "\n",
    "selected_dfevent300, selected_dfpulse300, selected_etof300 = events_selection_ions_electrons(RUNID300,THRESHOLD,tof_limit=TOF_LIMIT)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48712bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_etof300.to_netcdf(\"selected_etof300.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aecc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 6000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "TOF_LIMIT = 6e-7\n",
    "\n",
    "selected_dfevent100, selected_dfpulse100, selected_etof100 = events_selection_ions_electrons(RUNID100,THRESHOLD,tof_limit=TOF_LIMIT)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_etof100.to_netcdf(\"selected_etof100.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daccb70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = 500\n",
    "UPPER_BOUND = 6000\n",
    "THRESHOLD = [(LOWER_BOUND,UPPER_BOUND)]\n",
    "TOF_LIMIT = 6e-7\n",
    "\n",
    "selected_dfevent_coreshell, selected_dfpulse_coreshell, selected_etof_coreshell = events_selection_ions_electrons([227,228,229,273,274,275,276,277,278],THRESHOLD,tof_limit=TOF_LIMIT)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db06744",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_etof_coreshell.to_netcdf(\"selected_etof_coreshell.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_EVENTS = 50\n",
    "NBINS_TOF = 1000\n",
    "\n",
    "filtered_dfevents, filtered_dfpulses, filtered_etofs, hists, bins = nevents_binning_tof(selected_dfevent,selected_dfpulse,selected_etof,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_EVENTS = 50\n",
    "NBINS_TOF = 1000\n",
    "\n",
    "filtered_dfevents300, filtered_dfpulses300, filtered_etofs300, hists300, bins300 = nevents_binning_tof(selected_dfevent300,selected_dfpulse300,selected_etof300,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_EVENTS = 30\n",
    "NBINS_TOF = 1000\n",
    "\n",
    "filtered_dfevents100, filtered_dfpulses100, filtered_etofs100, hists100, bins100 = nevents_binning_tof(selected_dfevent100,selected_dfpulse100,selected_etof100,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0805ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_EVENTS = 30\n",
    "NBINS_TOF = 1000\n",
    "\n",
    "filtered_dfevents_coreshell, filtered_dfpulses_coreshell, filtered_etofs_coreshell, hists_coreshell, bins_coreshell = nevents_binning_tof(selected_dfevent_coreshell,selected_dfpulse_coreshell,selected_etof_coreshell,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_etof.to_netcdf(\"selected_etof.nc\")\n",
    "selected_etof300.to_netcdf(\"selected_etof300.nc\")\n",
    "selected_etof100.to_netcdf(\"selected_etof100.nc\")\n",
    "selected_etof_coreshell.to_netcdf(\"selected_etof_coreshell.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_etof = xr.open_dataset(\"selected_etof.nc\")\n",
    "selected_etof300 = xr.open_dataset(\"selected_etof300.nc\")\n",
    "selected_etof100 = xr.open_dataset(\"selected_etof100.nc\")\n",
    "selected_etof_coreshell = xr.open_dataset(\"selected_etof_coreshell.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lower, x_upper = (1e-7,2e-7)\n",
    "tof_lower, tof_upper = int(x_lower/channel_time), int(x_upper/channel_time)\n",
    "\n",
    "\n",
    "avg_etofs_500 = []\n",
    "for etof in filtered_etofs:\n",
    "    short_etof = etof.isel(data=slice(tof_lower, tof_upper))\n",
    "    avg_etof = -np.mean(short_etof,axis=0)\n",
    "    avg_etofs_500.append(sum(avg_etof))     \n",
    "\n",
    "avg_etofs_300 = []\n",
    "for etof in filtered_etofs300:\n",
    "    short_etof = etof.isel(data=slice(tof_lower, tof_upper))\n",
    "    avg_etof = -np.mean(short_etof,axis=0)\n",
    "    avg_etofs_300.append(sum(avg_etof))\n",
    "\n",
    "avg_etofs_100 = []\n",
    "for etof in filtered_etofs100:\n",
    "    short_etof = etof.isel(data=slice(tof_lower, tof_upper))\n",
    "    avg_etof = -np.mean(short_etof,axis=0)\n",
    "    avg_etofs_100.append(sum(avg_etof))\n",
    "\n",
    "avg_etofs_coreshell = []\n",
    "for etof in filtered_etofs_coreshell:\n",
    "    short_etof = etof.isel(data=slice(tof_lower, tof_upper))\n",
    "    avg_etof = -np.mean(short_etof,axis=0)\n",
    "    avg_etofs_coreshell.append(sum(avg_etof)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e2362",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.plot(bins[:-1], avg_etofs_500, label='500nm SiO$_2$', linewidth=2.5)\n",
    "plt.plot(bins300[:-1], avg_etofs_300, label='300nm SiO$_2$', linewidth=2.5)\n",
    "plt.plot(bins100[:-1], avg_etofs_100, label='100nm SiO$_2$', linewidth=2.5)\n",
    "plt.plot(bins_coreshell[:-1], avg_etofs_coreshell, label='140nm SiO$_2$@Au', linewidth=2.5)\n",
    "\n",
    "plt.xlabel('Normalized ion counts (arb. unit)', fontsize=18)\n",
    "plt.ylabel('Number of electrons detected', fontsize=18)\n",
    "\n",
    "# plt.xlim(120,490)\n",
    "# plt.ylim(0,2.9)\n",
    "plt.yscale('log')\n",
    "plt.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aeb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_sum(etofs,nbins_tof,bins,xlimits=(0,TIME_BETWEEN_PULSES)):\n",
    "    \"Etof sum values\"\n",
    "\n",
    "    x_lower, x_upper = xlimits\n",
    "    tof_lower, tof_upper = int(x_lower/channel_time), int(x_upper/channel_time)\n",
    "\n",
    "    avg_etofs = []\n",
    "    for etof in etofs:\n",
    "        short_etof = etof.isel(data=slice(tof_lower, tof_upper))\n",
    "        avg_etof = -np.mean(short_etof,axis=0)\n",
    "        avg_etofs.append(sum(avg_etof))\n",
    "        \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(bins[:-1],avg_etofs)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_heatmap_abs_tof(hists,NBINS_TOF,bins,(0,TOF_LIMIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_heatmap_rel_tof(hists,NBINS_TOF,bins,(0,TOF_LIMIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_heatmap_abs_tof(hists,NBINS_TOF,bins,(1.1e-6,1.4e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a3f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_heatmap_rel_tof(hists,NBINS_TOF,bins,(1.1e-6,1.4e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4079b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "etof_nevents_heatmap_abs_tof(filtered_etofs,NBINS_TOF,bins,(1e-8,TOF_LIMIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e70287",
   "metadata": {},
   "outputs": [],
   "source": [
    "etof_nevents_heatmap_rel_tof(filtered_etofs,NBINS_TOF,bins,(1e-8,TOF_LIMIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "etof_nevents_heatmap_rel_tof(filtered_etofs300,NBINS_TOF,bins300,(1e-8,TOF_LIMIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c33b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "etof_nevents_heatmap_rel_tof(filtered_etofs100,NBINS_TOF,bins100,(1e-8,TOF_LIMIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "some(filtered_etofs,NBINS_TOF,bins,(1e-7,2e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f26742",
   "metadata": {},
   "outputs": [],
   "source": [
    "some(filtered_etofs300,NBINS_TOF,bins300,(1e-7,2e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "some(filtered_etofs100,NBINS_TOF,bins100,(1e-7,2e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_sum(filtered_etofs,NBINS_TOF,bins,(1e-7,2e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_sum(filtered_etofs,NBINS_TOF,bins,(1e-7,2e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_sum(filtered_etofs300,NBINS_TOF,bins300,(1e-7,2e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_sum(filtered_etofs100,NBINS_TOF,bins100,(1e-7,2e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bccc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_selected_dfpulse = selected_dfpulse[selected_dfpulse.nevents_pulse < 5000]\n",
    "new_selected_dfevent = selected_dfevent[selected_dfevent.pulseId.isin(new_selected_dfpulse.pulseId)]\n",
    "new_selected_etof = selected_etof.sel(pulseId=selected_etof.coords['pulseId'].isin(new_selected_dfpulse.pulseId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560082e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_selected_dfpulse300 = selected_dfpulse300[selected_dfpulse300.nevents_pulse < 5000]\n",
    "new_selected_dfevent300 = selected_dfevent300[selected_dfevent300.pulseId.isin(new_selected_dfpulse300.pulseId)]\n",
    "new_selected_etof300 = selected_etof300.sel(pulseId=selected_etof300.coords['pulseId'].isin(new_selected_dfpulse300.pulseId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fec7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_selected_dfpulse100 = selected_dfpulse100[selected_dfpulse100.nevents_pulse < 5000]\n",
    "new_selected_dfevent100 = selected_dfevent100[selected_dfevent100.pulseId.isin(new_selected_dfpulse100.pulseId)]\n",
    "new_selected_etof100 = selected_etof100.sel(pulseId=selected_etof100.coords['pulseId'].isin(new_selected_dfpulse100.pulseId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca387e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_EVENTS = 40\n",
    "NBINS_TOF = 4000\n",
    "\n",
    "new_filtered_dfevents, new_filtered_dfpulses, new_filtered_etofs, new_hists, new_bins = nevents_binning_tof(new_selected_dfevent,new_selected_dfpulse,new_selected_etof,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_EVENTS = 20\n",
    "NBINS_TOF = 1000\n",
    "\n",
    "new_filtered_dfevents300, new_filtered_dfpulses300, new_filtered_etofs300, new_hists300, new_bins300 = nevents_binning_tof(new_selected_dfevent300,new_selected_dfpulse300,new_selected_etof300,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_EVENTS = 20\n",
    "NBINS_TOF = 1000\n",
    "\n",
    "fnew_iltered_dfevents100, new_filtered_dfpulses100, new_filtered_etofs100, new_hists100, new_bins100 = nevents_binning_tof(new_selected_dfevent100,new_selected_dfpulse100,new_selected_etof100,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af650793",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins_tof = NBINS_TOF\n",
    "xlimits = (1e-7,5e-7)\n",
    "\n",
    "hists=new_hists\n",
    "bins = new_bins\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "\n",
    "X_ns = X * 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X_ns, Y, hists_norm, cmap='magma', shading='auto')\n",
    "colorbar = plt.colorbar(c, label='Normalized counts (arb. units)', extend='max')\n",
    "plt.xlabel('Time of flight (ns)', fontsize=20)\n",
    "plt.ylabel('Hit intensity (arb. unit)', fontsize=20)\n",
    "plt.xlim(110, 500)\n",
    "plt.ylim(550,4900)\n",
    "\n",
    "ax1 = plt.gca()\n",
    "ax1.tick_params(axis='x', labelsize=16)\n",
    "ax1.tick_params(axis='y', labelsize=16)\n",
    "colorbar.ax.set_ylabel('Normalized counts (arb. unit)', fontsize=20)\n",
    "colorbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "ax2 = ax1.twiny()\n",
    "top_tick_positions = [210,227]\n",
    "top_tick_labels = ['125', '0']\n",
    "ax2.set_xticks(top_tick_positions)\n",
    "ax2.set_xticklabels(top_tick_labels)\n",
    "\n",
    "ax2.set_xlim(ax1.get_xlim())\n",
    "ax2.set_xlabel('Kinetic Energy (eV)', fontsize=20)\n",
    "\n",
    "ax2.spines['top'].set_color('r')\n",
    "ax2.xaxis.label.set_color('r')\n",
    "ax2.tick_params(axis='x', colors='r', labelsize=16)\n",
    "\n",
    "plt.axvline(227,c='r')\n",
    "plt.axvline(210,c='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad4323",
   "metadata": {},
   "outputs": [],
   "source": [
    "some(new_filtered_etofs,NBINS_TOF,new_bins,(1e-7,2e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0cc266",
   "metadata": {},
   "outputs": [],
   "source": [
    "some(new_filtered_etofs300,NBINS_TOF,new_bins300,(1e-7,2e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    x_lower, x_upper = xlimits\n",
    "    tof_lower, tof_upper = int(x_lower/channel_time), int(x_upper/channel_time)\n",
    "\n",
    "    avg_etofs = []\n",
    "    for etof in etofs:\n",
    "        short_etof = etof.isel(data=slice(tof_lower, tof_upper))\n",
    "        avg_etof = -np.mean(short_etof,axis=0)\n",
    "        avg_etofs.append(max(avg_etof))\n",
    "        \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(bins[:-1],avg_etofs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lower, x_upper = (1e-7,2e-7)\n",
    "tof_lower, tof_upper = int(x_lower/channel_time), int(x_upper/channel_time)\n",
    "\n",
    "short_etof = filtered_etofs[0].isel(data=slice(tof_lower, tof_upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def some(etofs,nbins_tof,bins,xlimits=(0,TIME_BETWEEN_PULSES)):\n",
    "    \"Etof max values\"\n",
    "\n",
    "    x_lower, x_upper = xlimits\n",
    "    tof_lower, tof_upper = int(x_lower/channel_time), int(x_upper/channel_time)\n",
    "\n",
    "    avg_etofs = []\n",
    "    for etof in etofs:\n",
    "        short_etof = etof.isel(data=slice(tof_lower, tof_upper))\n",
    "        avg_etof = -np.mean(short_etof,axis=0)\n",
    "        avg_etofs.append(max(avg_etof))\n",
    "        \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(bins[:-1],avg_etofs)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "def some_sum(etofs,nbins_tof,bins,xlimits=(0,TIME_BETWEEN_PULSES)):\n",
    "    \"Etof sum values\"\n",
    "\n",
    "    x_lower, x_upper = xlimits\n",
    "    tof_lower, tof_upper = int(x_lower/channel_time), int(x_upper/channel_time)\n",
    "\n",
    "    avg_etofs = []\n",
    "    for etof in etofs:\n",
    "        short_etof = etof.isel(data=slice(tof_lower, tof_upper))\n",
    "        avg_etof = -np.mean(short_etof,axis=0)\n",
    "        avg_etofs.append(sum(avg_etof))\n",
    "        \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(bins[:-1],avg_etofs)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "def etof_nevents_heatmap_abs_tof(etofs,nbins_tof,bins,xlimits=(0,TIME_BETWEEN_PULSES)):\n",
    "    \"Etof heatmap of relative normalized counts with number of events slices on the y axis, with respect to tof on the x axis using hists, which is a list of histograms\"\n",
    "\n",
    "    x_lower, x_upper = xlimits\n",
    "    tof_lower, tof_upper = int(x_lower/channel_time), int(x_upper/channel_time)\n",
    "\n",
    "    avg_etofs = []\n",
    "    for etof in etofs:\n",
    "        short_etof = etof.isel(data=slice(tof_lower, tof_upper))\n",
    "        avg_etof = -np.mean(short_etof,axis=0)\n",
    "        avg_etofs.append(avg_etof)\n",
    "\n",
    "    x_edges = np.linspace(x_lower, x_upper, tof_upper-tof_lower)\n",
    "    X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    c = plt.pcolormesh(X, Y, avg_etofs, shading='auto')\n",
    "    plt.colorbar(c, label='Signal', extend='max')\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Number of events slice')\n",
    "    plt.title('Electrons heatmap for number of events slices with respect to tof')\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def etof_nevents_heatmap_rel_tof(etofs,nbins_tof,bins,xlimits=(0,TIME_BETWEEN_PULSES)):\n",
    "    \"Etof heatmap of relative normalized counts with number of events slices on the y axis, with respect to tof on the x axis using hists, which is a list of histograms\"\n",
    "\n",
    "    x_lower, x_upper = xlimits\n",
    "    tof_lower, tof_upper = int(x_lower/channel_time), int(x_upper/channel_time)\n",
    "\n",
    "    norm_avg_etofs = []\n",
    "    for etof in etofs:\n",
    "        short_etof = etof.isel(data=slice(tof_lower, tof_upper))\n",
    "        avg_etof = -np.mean(short_etof,axis=0)\n",
    "        norm_avg_etofs.append(avg_etof/max(avg_etof))\n",
    "\n",
    "    x_edges = np.linspace(x_lower, x_upper, tof_upper-tof_lower)\n",
    "    X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    c = plt.pcolormesh(X, Y, norm_avg_etofs, shading='auto')\n",
    "    plt.colorbar(c, label='Signal normalized per events slice', extend='max')\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Number of events slice')\n",
    "    plt.title('Electrons heatmap for number of events slices with respect to tof')\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1df3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nevents_heatmap_rel_tof(hists,nbins_tof,bins,xlimits=(0,TIME_BETWEEN_PULSES)):\n",
    "    \"Heatmap of relative normalized counts with number of events slices on the y axis, with respect to tof on the x axis using hists, which is a list of histograms\"\n",
    "    \n",
    "    precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "    x_lower, x_upper = xlimits\n",
    "    hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "    nbins_events = len(hists)\n",
    "    hists_norm = []\n",
    "\n",
    "    for i in range(nbins_events):\n",
    "\n",
    "        shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "        hist_norm = shortened_hist / max(shortened_hist)\n",
    "        hists_norm.append(hist_norm)\n",
    "\n",
    "    x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "    X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "    plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Number of events slice')\n",
    "    plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    # plt.yticks(bins[:-1])\n",
    "    plt.show()\n",
    "\n",
    "def nevents_heatmap_abs_tof(hists,nbins_tof,bins,xlimits=(0,TIME_BETWEEN_PULSES)):\n",
    "    \"Heatmap of absolute normalized counts with number of events slices on the y axis, with respect to tof on the x axis using hists, which is a list of histograms\"\n",
    "    \n",
    "    precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "    x_lower, x_upper = xlimits\n",
    "    hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "    hists_shortened = hists[:,hist_lower:hist_upper]\n",
    "    nbins_events = len(hists_shortened)\n",
    "    hists_norm = hists_shortened/np.max(hists_shortened)\n",
    "\n",
    "    x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "    X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "    plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "    plt.xlabel('Time of flight (s)')\n",
    "    plt.ylabel('Number of events slice')\n",
    "    plt.title('Absolute heatmap for number of events slices with respect to tof')\n",
    "    plt.xlim(x_lower, x_upper)\n",
    "    # plt.yticks(bins[:-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30311a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND1 = 1000\n",
    "UPPER_BOUND1 = 3000\n",
    "\n",
    "LOWER_BOUND2 = 5000\n",
    "UPPER_BOUND2 = 8000\n",
    "\n",
    "LOWER_BOUND3 = 10000\n",
    "UPPER_BOUND3 = 15000\n",
    "\n",
    "THRESHOLDS = [(LOWER_BOUND1, UPPER_BOUND1), (LOWER_BOUND2, UPPER_BOUND2), (LOWER_BOUND3, UPPER_BOUND3)]\n",
    "\n",
    "ion_electron_selections = events_selection_ions_electrons(RUNID500,THRESHOLDS)\n",
    "\n",
    "dfevent1, dfpulse1, etof1 = ion_electron_selections[0]\n",
    "dfevent2, dfpulse2, etof2 = ion_electron_selections[1]\n",
    "dfevent3, dfpulse3, etof3 = ion_electron_selections[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BACKGRD_BOUND = 20\n",
    "UPPER_BACKGRD_BOUND = 40\n",
    "BKGRD_THRESHOLD = [(LOWER_BACKGRD_BOUND,UPPER_BACKGRD_BOUND)]\n",
    "DOWNSAMPLING = 20000\n",
    "\n",
    "backgrd_dfevent, backgrd_dfpulse, backgrd_etof = events_selection_ions_electrons(RUNID500,BKGRD_THRESHOLD,DOWNSAMPLING)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = np.arange(14080)*channel_time\n",
    "\n",
    "avg_etof1 = -np.mean(etof1, axis=0)\n",
    "max_etof1 = max(avg_etof1)\n",
    "avg_etof2 = -np.mean(etof2, axis=0)\n",
    "max_etof2 = max(avg_etof2)\n",
    "avg_etof3 = -np.mean(etof3, axis=0)\n",
    "max_etof3 = max(avg_etof3)\n",
    "avg_backgrd_etof = -np.mean(backgrd_etof, axis=0)\n",
    "max_backgrd_etof = max(avg_backgrd_etof)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xaxis, avg_etof1/max_etof1, label='1000-3000 events')\n",
    "plt.plot(xaxis, avg_etof2/max_etof2, label='5000-8000 events')\n",
    "plt.plot(xaxis, avg_etof3/max_etof3, label='10000-15000 events')\n",
    "plt.plot(xaxis, avg_backgrd_etof/max_backgrd_etof, label='Background')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.title('Electrons time of flight')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf62847",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgrd_subs1 = np.abs(avg_etof1-avg_backgrd_etof)/np.abs(avg_backgrd_etof)\n",
    "bgrd_subs2 = np.abs(avg_etof2-avg_backgrd_etof)/np.abs(avg_backgrd_etof)\n",
    "bgrd_subs3 = np.abs(avg_etof3-avg_backgrd_etof)/np.abs(avg_backgrd_etof)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xaxis, bgrd_subs1/max(bgrd_subs1), label='1000-3000 events')\n",
    "plt.plot(xaxis, bgrd_subs2/max(bgrd_subs2), label='5000-8000 events')\n",
    "plt.plot(xaxis, bgrd_subs3/max(bgrd_subs3), label='10000-15000 events')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Signal')\n",
    "plt.title('Background substracted electrons time of flight')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 6e-7\n",
    "max_coord = int(max_time/channel_time)\n",
    "\n",
    "avg_etof1 = avg_etof1[:max_coord]\n",
    "max_etof1 = max(avg_etof1)\n",
    "avg_etof2 = avg_etof2[:max_coord]\n",
    "max_etof2 = max(avg_etof2)\n",
    "avg_etof3 = avg_etof3[:max_coord]\n",
    "max_etof3 = max(avg_etof3)\n",
    "avg_backgrd_etof = avg_backgrd_etof[:max_coord]\n",
    "max_backgrd_etof = max(avg_backgrd_etof)\n",
    "\n",
    "xaxis = np.arange(max_coord)*channel_time\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xaxis, avg_etof1/max_etof1, label='1000-3000 events')\n",
    "plt.plot(xaxis, avg_etof2/max_etof2, label='5000-8000 events')\n",
    "plt.plot(xaxis, avg_etof3/max_etof3, label='10000-15000 events')\n",
    "plt.plot(xaxis, avg_backgrd_etof/max_backgrd_etof, label='Background')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.title('Electrons time of flight')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ba4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = np.arange(14080)*channel_time\n",
    "\n",
    "avg_etof1 = -np.mean(etof1, axis=0)\n",
    "max_etof1 = max(avg_etof1)\n",
    "avg_etof2 = -np.mean(etof2, axis=0)\n",
    "max_etof2 = max(avg_etof2)\n",
    "avg_etof3 = -np.mean(etof3, axis=0)\n",
    "max_etof3 = max(avg_etof3)\n",
    "avg_backgrd_etof = -np.mean(backgrd_etof, axis=0)\n",
    "max_backgrd_etof = max(avg_backgrd_etof)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xaxis, avg_etof1, label='1000-3000 events')\n",
    "plt.plot(xaxis, avg_etof2, label='5000-8000 events')\n",
    "plt.plot(xaxis, avg_etof3, label='10000-15000 events')\n",
    "plt.plot(xaxis, avg_backgrd_etof/max_backgrd_etof, label='Background')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.title('Electrons time of flight')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69284a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 6e-7\n",
    "max_coord = int(max_time/channel_time)\n",
    "\n",
    "avg_etof1 = avg_etof1[:max_coord]\n",
    "max_etof1 = max(avg_etof1)\n",
    "avg_etof2 = avg_etof2[:max_coord]\n",
    "max_etof2 = max(avg_etof2)\n",
    "avg_etof3 = avg_etof3[:max_coord]\n",
    "max_etof3 = max(avg_etof3)\n",
    "avg_backgrd_etof = avg_backgrd_etof[:max_coord]\n",
    "max_backgrd_etof = max(avg_backgrd_etof)\n",
    "\n",
    "xaxis = np.arange(max_coord)*channel_time\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xaxis, avg_etof1, label='1000-3000 events')\n",
    "plt.plot(xaxis, avg_etof2, label='5000-8000 events')\n",
    "plt.plot(xaxis, avg_etof3, label='10000-15000 events')\n",
    "plt.plot(xaxis, avg_backgrd_etof/max_backgrd_etof, label='Background')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Signal')\n",
    "plt.title('Electrons time of flight')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9480a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgrd_subs1 = np.abs(avg_etof1-avg_backgrd_etof)/np.abs(avg_backgrd_etof)\n",
    "bgrd_subs2 = np.abs(avg_etof2-avg_backgrd_etof)/np.abs(avg_backgrd_etof)\n",
    "bgrd_subs3 = np.abs(avg_etof3-avg_backgrd_etof)/np.abs(avg_backgrd_etof)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xaxis, bgrd_subs1, label='1000-3000 events')\n",
    "plt.plot(xaxis, bgrd_subs2, label='5000-8000 events')\n",
    "plt.plot(xaxis, bgrd_subs3, label='10000-15000 events')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Signal')\n",
    "plt.title('Background substracted electrons time of flight')\n",
    "plt.legend()\n",
    "plt.ylim(0,5e5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a04ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlimit = 1.5e-7\n",
    "\n",
    "bounded_dfevent1 = dfevent1[dfevent1.tof < xlimit]\n",
    "hist1, bin_edges = np.histogram(bounded_dfevent1.tof, bins=1000)\n",
    "\n",
    "bounded_dfevent2 = dfevent2[dfevent2.tof < xlimit]\n",
    "hist2, bin_edges = np.histogram(bounded_dfevent2.tof, bins=1000)\n",
    "\n",
    "bounded_dfevent3 = dfevent3[dfevent3.tof < xlimit]\n",
    "hist3, bin_edges = np.histogram(bounded_dfevent3.tof, bins=1000)\n",
    "\n",
    "backgrd_bounded_dfevent = backgrd_dfevent[backgrd_dfevent.tof < xlimit]\n",
    "backgrd_hist, bin_edges = np.histogram(backgrd_bounded_dfevent.tof, bins=1000)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(bin_edges[:-1], hist1, label='1000-3000 events')\n",
    "plt.plot(bin_edges[:-1], hist2, label='5000-8000 events')\n",
    "plt.plot(bin_edges[:-1], hist3, label='10000-15000 events')\n",
    "plt.plot(bin_edges[:-1], backgrd_hist, label='Background')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of hits per bin')\n",
    "plt.title('Ions time of flight')\n",
    "plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43035263",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 2e-7\n",
    "\n",
    "max_coord = int(max_time/channel_time)\n",
    "etof_sums = selected_etof.isel(data=slice(None, max_coord)).sum(dim='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(selected_dfpulse.nevents_pulse,-etof_sums)\n",
    "plt.xlabel('Number of ions')\n",
    "plt.ylabel('Number of electrons')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(selected_dfpulse.nevents_pulse,-etof_sums, s=1)\n",
    "plt.xlabel('Number of ions')\n",
    "plt.ylabel('Number of electrons')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(selected_dfpulse.nevents_pulse,-etof_sums)\n",
    "plt.xlabel('Number of ions')\n",
    "plt.ylabel('Number of electrons')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b22069",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(selected_dfpulse.nevents_pulse,-etof_sums, s=1)\n",
    "plt.xlabel('Number of ions')\n",
    "plt.ylabel('Number of electrons')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c098dc2b",
   "metadata": {},
   "source": [
    "## Pulse filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16944715",
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_dfpulse['pulseId'] = ion_dfpulse['pulseId'].astype(str)\n",
    "ion_dfpulse['last_two_digits'] = ion_dfpulse['pulseId'].str[-2:]\n",
    "result_df = ion_dfpulse.groupby('last_two_digits')['nevents_pulse'].mean().reset_index()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(result_df['last_two_digits'], result_df['nevents_pulse'], marker='o')\n",
    "plt.xlabel('Last Two Digits of pulseId')\n",
    "plt.ylabel('Average nevents_pulse')\n",
    "plt.title('Average nevents_pulse for each Last Two Digits of pulseId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounded_dfevent = ion_dfevent1[ion_dfevent1.tof < TIME_BETWEEN_PULSES]\n",
    "hist, bin_edges = np.histogram(bounded_dfevent.tof, bins=1000)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(bin_edges[:-1], hist/max(hist))\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.title('Ions time of flight')\n",
    "plt.xlim(0,1e-6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(bin_edges[:-1], hist/max(hist))\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.title('Ions time of flight')\n",
    "plt.xlim(0,2e-6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8075bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_numbers=['750-1500','3000-8000','10000-14000']\n",
    "k=0\n",
    "for i_df in [ion_dfevent1,ion_dfevent2,ion_dfevent3]:\n",
    "    print('event_numbers',event_numbers[k])\n",
    "    k+=1\n",
    "    for pulse_number in [None,'01','46','92']:\n",
    "        print('pulse number',pulse_number)\n",
    "        pulse_filtered_ion_tof(i_df,pulse_number,5e-7)\n",
    "print('background 20-40')\n",
    "pulse_filtered_ion_tof(backgrd_ion_dfevent,None,5e-7)\n",
    "print('argon')\n",
    "pulse_filtered_ion_tof(argon_dfevent,None,5e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9586c32c",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 127\n",
    "Y = 113\n",
    "WIDTH = 13\n",
    "HEIGHT = 10\n",
    "ZONE = [X,Y,WIDTH,HEIGHT,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To test where your square selection is, use:\n",
    "heatmap_with_zones(ion_dfevent500,[ZONE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_bkgrd_dfevent,spatial_bkgrd_dfpulse = spatial_ion_selection(backgrd_ion_dfevent500,backgrd_ion_dfpulse500,[ZONE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "big_ion_tof(spatial_bkgrd_dfevent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c58f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALIBRATION_LINES 500nm 7594V run390 = [1.312e-6, 9.26e-7, 7.56e-7, 6.65e-7, 5.97e-7]\n",
    "# CALIBRATION_LINES 500/300nm 5400V = [1.556e-6, 1.103e-6, 9e-7, 7.83e-7, 7e-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "calibrate(spatial_bkgrd_dfevent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25451f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_fit, b_fit = (29497236325759.32, 2.0175071848701878) #500nm 7594V run390, alcohol\n",
    "a_fit, b_fit = (19788855827929.47, 2.013491982616985) #500nm 5400V all runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbff5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_selected_dfevent, calibrated_backgrd_dfevent = apply_calibration([selected_dfevent,backgrd_dfevent],a_fit,b_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_selected_dfevent1, calibrated_backgrd_dfevent = apply_calibration([selected_dfevent1,backgrd_dfevent],a_fit,b_fit)\n",
    "calibrated_selected_dfevent2, calibrated_backgrd_dfevent = apply_calibration([selected_dfevent2,backgrd_dfevent],a_fit,b_fit)\n",
    "calibrated_selected_dfevent3, calibrated_backgrd_dfevent = apply_calibration([selected_dfevent3,backgrd_dfevent],a_fit,b_fit)\n",
    "calibrated_selected_dfevent4, calibrated_backgrd_dfevent = apply_calibration([selected_dfevent4,backgrd_dfevent],a_fit,b_fit)\n",
    "calibrated_selected_dfevent5, calibrated_backgrd_dfevent = apply_calibration([selected_dfevent5,backgrd_dfevent],a_fit,b_fit)\n",
    "calibrated_selected_dfevent6, calibrated_backgrd_dfevent = apply_calibration([selected_dfevent6,backgrd_dfevent],a_fit,b_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ea2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_dfevent500, backgrd_ion_dfevent500 = apply_calibration([ion_dfevent500,backgrd_ion_dfevent500],a_fit,b_fit)\n",
    "ion_dfevent300, backgrd_ion_dfevent300 = apply_calibration([ion_dfevent300,backgrd_ion_dfevent300],a_fit,b_fit)\n",
    "# ion_dfevent100, backgrd_ion_dfevent = apply_calibration([ion_dfevent100,backgrd_ion_dfevent],a_fit,b_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowcalibrated_ion_dfevent, lowcalibrated_backgrd_ion_dfevent = apply_calibration([lowion_dfevent300,backgrd_ion_dfevent300],a_fit,b_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400877f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_ion_dfevent1, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent1,backgrd_ion_dfevent300],a_fit,b_fit)\n",
    "calibrated_ion_dfevent2, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent2,backgrd_ion_dfevent300],a_fit,b_fit)\n",
    "calibrated_ion_dfevent3, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent3,backgrd_ion_dfevent300],a_fit,b_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ea095",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_ion_dfevent1, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent1,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent2, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent2,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent3, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent3,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent4, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent4,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent5, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent5,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent6, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent6,backgrd_ion_dfevent],a_fit,b_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_ion_dfevent10, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent10,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent05, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent05,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent025, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent025,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent012, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent012,backgrd_ion_dfevent],a_fit,b_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea64395",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_ion_dfevent10_1, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent10_1,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent10_2, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent10_2,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent10_3, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent10_3,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "\n",
    "calibrated_ion_dfevent05_1, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent05_1,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent05_2, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent05_2,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent05_3, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent05_3,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "\n",
    "calibrated_ion_dfevent025_1, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent025_1,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent025_2, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent025_2,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent025_3, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent025_3,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "\n",
    "calibrated_ion_dfevent012_1, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent012_1,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent012_2, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent012_2,backgrd_ion_dfevent],a_fit,b_fit)\n",
    "calibrated_ion_dfevent012_3, calibrated_backgrd_dfevent = apply_calibration([ion_dfevent012_3,backgrd_ion_dfevent],a_fit,b_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_selected_dfevent = ion_dfevent500\n",
    "calibrated_backgrd_dfevent = backgrd_ion_dfevent500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "MQ_LINES = [40,20,40/3,40/4,40/5]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "histselected, bin_edgesselected = np.histogram(calibrated_selected_dfevent.mq, bins=np.linspace(0,200,1000),range=(0,200))\n",
    "histgrd, bin_edgesgrd = np.histogram(calibrated_backgrd_dfevent.mq, bins=np.linspace(0,200,1000),range=(0,200))\n",
    "plt.plot(bin_edgesselected[:-1], histselected/max(histselected), linewidth = 1, c='b')\n",
    "plt.plot(bin_edgesgrd[:-1], histgrd/max(histgrd), linewidth = 1, c='g')\n",
    "plt.vlines(MQ_LINES,0,1,colors='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "big_mq_plot(calibrated_selected_dfevent,2500,(0,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a2760",
   "metadata": {},
   "source": [
    "### Heatmaps / Etofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0576943",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "LOWER_MQ = .25\n",
    "UPPER_MQ = 3\n",
    "\n",
    "mqselected_dfevent,mqselected_dfpulse,mqselected_etof = mq_selection(calibrated_selected_dfevent,selected_dfpulse,selected_etof,LOWER_MQ,UPPER_MQ)\n",
    "mqselected_backgrd_dfevent,mqselected_backgrd_dfpulse,mqselected_backgrd_etof = mq_selection(calibrated_backgrd_dfevent,backgrd_dfpulse,backgrd_etof,LOWER_MQ,UPPER_MQ)\n",
    "\n",
    "heatmap(mqselected_dfevent)\n",
    "heatmap(mqselected_backgrd_dfevent)\n",
    "\n",
    "TIME_BETWEEN_PULSES = 3.54462e-6\n",
    "CHANNELS_PER_PULSE = 14080\n",
    "channel_time = TIME_BETWEEN_PULSES/CHANNELS_PER_PULSE\n",
    "    \n",
    "xaxis = np.arange(14080)*channel_time\n",
    "avg_mqselected_etof = -np.mean(mqselected_etof, axis=0)\n",
    "avg_mqbackgrd_etof = -np.mean(mqselected_backgrd_etof, axis=0)\n",
    "max_mqselected_etof = max(avg_mqselected_etof)\n",
    "max_mqbackgrd_etof = max(avg_mqbackgrd_etof)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xaxis,avg_mqselected_etof/max_mqselected_etof,c='r')\n",
    "plt.plot(xaxis,avg_mqbackgrd_etof/max_mqbackgrd_etof,c='g')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.title('Electrons time of flight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_MQ = 20.5\n",
    "UPPER_MQ = 21.5\n",
    "\n",
    "mqselected_dfevent,mqselected_dfpulse,mqselected_etof = mq_selection(calibrated_selected_dfevent,selected_dfpulse,selected_etof,LOWER_MQ,UPPER_MQ)\n",
    "heatmap(mqselected_dfevent)\n",
    "e_tof(mqselected_etof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab51206",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqselected_dfevent1,mqselected_dfpulse1,mqselected_etof1 = mq_selection(calibrated_selected_dfevent1,selected_dfpulse1,selected_etof1,LOWER_MQ,UPPER_MQ)\n",
    "\n",
    "avg_mqselected_etof = -np.mean(mqselected_etof, axis=0)\n",
    "avg_mqselected_etof1 = -np.mean(mqselected_etof1, axis=0)\n",
    "max_mqselected_etof = max(avg_mqselected_etof)\n",
    "max_mqselected_etof1 = max(avg_mqselected_etof1)\n",
    "\n",
    "xaxis = np.arange(14080)*channel_time\n",
    "plt.figure()\n",
    "plt.plot(xaxis,avg_mqselected_etof/max_mqselected_etof,c='r')\n",
    "plt.plot(xaxis,avg_mqselected_etof1/max_mqselected_etof1,c='b')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.title('Electrons time of flight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6396f9f",
   "metadata": {},
   "source": [
    "## Fish plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1abc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of tuples where each tuple represents a tilted zone (x, y, width, height, angle)\n",
    "ZONES = [(0, 98, 256, 10, 7), (145, 0, 10, 256, 7)]  # Add the angle of rotation in degrees\n",
    "spatial_selected_dfevent,spatial_selected_dfpulse,spatial_selected_etof = tilted_spatial_ion_selection(selected_dfevent, selected_dfpulse, selected_etof, ZONES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f05914",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_calibrated_ion_dfevent1 = calibrated_ion_dfevent1[calibrated_ion_dfevent1.tof < 5e-7]\n",
    "slow_calibrated_ion_dfevent2 = calibrated_ion_dfevent2[calibrated_ion_dfevent2.tof < 5e-7]\n",
    "slow_calibrated_ion_dfevent3 = calibrated_ion_dfevent3[calibrated_ion_dfevent3.tof < 5e-7]\n",
    "slow_calibrated_ion_dfevent = calibrated_ion_dfevent[calibrated_ion_dfevent.tof < 5e-7]\n",
    "slow_lowcalibrated_ion_dfevent = lowcalibrated_ion_dfevent[lowcalibrated_ion_dfevent.tof < 5e-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ZONE = (0, 98, 256, 10, 7)\n",
    "Y_ZONE = (145, 0, 10, 256, 7)\n",
    "TOF_BINS = 1000\n",
    "MQ_BINS = 500\n",
    "\n",
    "fish_plot_x(ion_dfevent,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent,Y_ZONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a0d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ZONE = (0, 98, 256, 10, 7)\n",
    "Y_ZONE = (145, 0, 10, 256, 7)\n",
    "TOF_BINS = 1000\n",
    "MQ_BINS = 500\n",
    "\n",
    "fish_plot_x(slow_calibrated_ion_dfevent1,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(slow_calibrated_ion_dfevent1,Y_ZONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ZONE = (0, 98, 256, 10, 7)\n",
    "Y_ZONE = (145, 0, 10, 256, 7)\n",
    "TOF_BINS = 1000\n",
    "MQ_BINS = 500\n",
    "\n",
    "fish_plot_x(slow_calibrated_ion_dfevent2,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(slow_calibrated_ion_dfevent2,Y_ZONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb61c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ZONE = (0, 98, 256, 10, 7)\n",
    "Y_ZONE = (145, 0, 10, 256, 7)\n",
    "TOF_BINS = 1000\n",
    "MQ_BINS = 500\n",
    "\n",
    "fish_plot_x(calibrated_ion_dfevent2,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(calibrated_ion_dfevent2,Y_ZONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ZONE = (0, 98, 256, 10, 7)\n",
    "Y_ZONE = (145, 0, 10, 256, 7)\n",
    "TOF_BINS = 1000\n",
    "MQ_BINS = 500\n",
    "\n",
    "fish_plot_x(calibrated_ion_dfevent3,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(calibrated_ion_dfevent3,Y_ZONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ZONE = (0, 98, 256, 10, 7)\n",
    "Y_ZONE = (145, 0, 10, 256, 7)\n",
    "TOF_BINS = 1000\n",
    "MQ_BINS = 500\n",
    "\n",
    "fish_plot_x(calibrated_ion_dfevent,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(calibrated_ion_dfevent,Y_ZONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ZONE = (0, 98, 256, 10, 7)\n",
    "Y_ZONE = (145, 0, 10, 256, 7)\n",
    "TOF_BINS = 1000\n",
    "MQ_BINS = 500\n",
    "\n",
    "fish_plot_x(ion_dfevent10_1,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent10_1,Y_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_x(ion_dfevent10_2,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent10_2,Y_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_x(ion_dfevent10_3,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent10_3,Y_ZONE,TOF_BINS,MQ_BINS)\n",
    "\n",
    "fish_plot_x(ion_dfevent05_1,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent05_1,Y_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_x(ion_dfevent05_2,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent05_2,Y_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_x(ion_dfevent05_3,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent05_3,Y_ZONE,TOF_BINS,MQ_BINS)\n",
    "\n",
    "fish_plot_x(ion_dfevent025_1,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent025_1,Y_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_x(ion_dfevent025_2,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent025_2,Y_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_x(ion_dfevent025_3,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent025_3,Y_ZONE,TOF_BINS,MQ_BINS)\n",
    "\n",
    "fish_plot_x(ion_dfevent012_1,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent012_1,Y_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_x(ion_dfevent012_2,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent012_2,Y_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_x(ion_dfevent012_3,X_ZONE,TOF_BINS,MQ_BINS)\n",
    "fish_plot_y(ion_dfevent012_3,Y_ZONE,TOF_BINS,MQ_BINS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7b2ef",
   "metadata": {},
   "source": [
    "## Intensity dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f7f578",
   "metadata": {},
   "source": [
    "### tofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_EVENTS = 6\n",
    "NBINS_MQ = 1500\n",
    "\n",
    "filtered_dfs = nevents_binning_plot(calibrated_selected_dfevent,selected_dfpulse,NBINS_EVENTS,NBINS_MQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c960fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_EVENTS = 6\n",
    "NBINS_TOF = 1500\n",
    "\n",
    "filtered_dfs = nevents_binning_plot_tof(ion_dfevent500,ion_dfpulse500,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b49086",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF = 1500\n",
    "NBINS_EVENTS = 6\n",
    "\n",
    "short_filtered_dfevents, short_filtered_dfpulses, short_hists, short_bins = nions_binning_tof(ion_dfevent,ion_dfpulse,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe62d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized individually to maximum in specified region\n",
    "stacked_ion_tof_max(short_hists,NBINS_TOF,short_bins,(0,4e-7),backgrd_ion_dfevent500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ion_tof_max(short_hists,NBINS_TOF,short_bins,(.9e-6,1.3e-6),backgrd_ion_dfevent500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized individually to number of hits in specified region\n",
    "stacked_ion_tof_sum(short_hists,NBINS_TOF,short_bins,(0,4e-7),backgrd_ion_dfevent500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ion_tof_sum(short_hists,NBINS_TOF,short_bins,(.9e-6,1.3e-6),backgrd_ion_dfevent500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ac716",
   "metadata": {},
   "source": [
    "### waterfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592aeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_MQ = 1500\n",
    "NBINS_EVENTS = 20\n",
    "\n",
    "filtered_dfevents, filtered_dfpulses, filtered_etofs, hists, bins = nevents_binning(calibrated_selected_dfevent,selected_dfpulse,selected_etof,NBINS_EVENTS,NBINS_MQ)\n",
    "#filtered_dfevents, filtered_dfpulses, hists, bins = nions_binning(calibrated_ion_dfevent,ion_dfpulse,NBINS_EVENTS,NBINS_MQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfall_rel(hists,NBINS_MQ,(0.5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea86d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfall_abs(hists,NBINS_MQ,(0,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "waterfall_etof(filtered_etofs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceadfa64",
   "metadata": {},
   "source": [
    "### heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151003e",
   "metadata": {},
   "source": [
    "#### general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e707d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF = 1500\n",
    "NBINS_EVENTS = 50\n",
    "\n",
    "filtered_dfevents1, filtered_dfpulses1, hists1, bins = nions_binning_tof(ion_dfevent1,ion_dfpulse1,NBINS_EVENTS,NBINS_TOF)\n",
    "filtered_dfevents05, filtered_dfpulses05, hists05, bins = nions_binning_tof(ion_dfevent05,ion_dfpulse05,NBINS_EVENTS,NBINS_TOF)\n",
    "filtered_dfevents025, filtered_dfpulses025, hists025, bins = nions_binning_tof(ion_dfevent025,ion_dfpulse025,NBINS_EVENTS,NBINS_TOF)\n",
    "filtered_dfevents012, filtered_dfpulses012, hists012, bins = nions_binning_tof(ion_dfevent012,ion_dfpulse012,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tof = np.linspace(0,TIME_BETWEEN_PULSES,NBINS_TOF)\n",
    "np.savetxt('heatmap_tof.csv', tof, delimiter=',')\n",
    "np.savetxt('heatmap_nevents.csv', bins, delimiter=',')\n",
    "\n",
    "np.savetxt('1heatmap_matrix.csv', hists1, delimiter=',')\n",
    "np.savetxt('05heatmap_matrix.csv', hists05, delimiter=',')\n",
    "np.savetxt('025heatmap_matrix.csv', hists025, delimiter=',')\n",
    "np.savetxt('012heatmap_matrix.csv', hists012, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab600c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF = 4000\n",
    "NBINS_EVENTS = 80\n",
    "\n",
    "filtered_dfevents, filtered_dfpulses, hists, bins = nions_binning_tof(ion_dfevent,ion_dfpulse,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc3b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF = 3000\n",
    "NBINS_EVENTS = 30\n",
    "\n",
    "filtered_maxdfevents500, filtered_maxdfpulses500, maxhists500, bins = nions_binning_tof(ion_dfevent_max,ion_dfpulse_max,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5bf8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF = 3000\n",
    "NBINS_EVENTS = 30\n",
    "\n",
    "filtered_lowermaxdfevents500, filtered_lowermaxdfpulses500, lowermaxhists500, bins = nions_binning_tof(ion_dfevent_lower_max,ion_dfpulse_lower_max,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e061cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF = 3000\n",
    "NBINS_EVENTS = 40\n",
    "\n",
    "filtered_dfevents500, filtered_dfpulses500, hists500, bins = nions_binning_tof(ion_dfevent500,ion_dfpulse500,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF = 4000\n",
    "NBINS_EVENTS = 70\n",
    "\n",
    "filtered_dfevents500, filtered_dfpulses500, hists500detail, binsdetail = nions_binning_tof(ion_dfevent500,ion_dfpulse500,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781602ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF = 3000\n",
    "NBINS_EVENTS = 30\n",
    "\n",
    "filtered_dfevents100, filtered_dfpulses100, hists100, bins100 = nions_binning_tof(ion_dfevent100,ion_dfpulse100,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7569c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_MQ = 3000\n",
    "NBINS_EVENTS = 40\n",
    "\n",
    "mq_dfevents500, mq_dfpulses500, mq_hists500, mq_bins = nions_binning(ion_dfevent500,ion_dfpulse500,NBINS_EVENTS,NBINS_MQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_heatmap_rel(mq_hists500,NBINS_MQ,mq_bins,(0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f121e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_heatmap_rel_tof(hists500,NBINS_TOF,bins,(1e-7,5e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = hists500\n",
    "nbins_tof = 3000\n",
    "xlimits = (1e-7,5e-7)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', vmax=.3)\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', cmap='viridis', norm=LogNorm())\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of events slice')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "# plt.axvline(2.37e-7,c='r')\n",
    "# plt.axvline(3.34e-7,c='r')\n",
    "plt.axvline(3.7e-7,c='r')\n",
    "plt.axvline(4.1e-7,c='r')\n",
    "plt.axvline(4.5e-7,c='r')\n",
    "# plt.axhline(y=2000, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39047a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = hists500detail\n",
    "nbins_tof = NBINS_TOF\n",
    "xlimits = (1e-7,5e-7)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, binsdetail[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616167ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, cmap='plasma', shading='auto')\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', vmax=0.7)\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto', cmap='plasma', norm=LogNorm(vmin=.1, vmax=1))\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of events slice')\n",
    "# plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "# plt.axvline(2.37e-7,c='r')\n",
    "# plt.axvline(3.34e-7,c='r')\n",
    "plt.axvline(3.7e-7,c='r')\n",
    "plt.axvline(4.1e-7,c='r')\n",
    "plt.axvline(4.5e-7,c='r')\n",
    "# plt.axhline(y=2000, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto', cmap='plasma')\n",
    "colorbar = plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (ns)', fontsize=24)\n",
    "plt.ylabel('Hit intensity', fontsize=24)\n",
    "# plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(1.1e-7, 3.4e-7)\n",
    "plt.ylim(0,6800)\n",
    "\n",
    "ax1 = plt.gca()\n",
    "ax1.tick_params(axis='x', labelsize=22)\n",
    "ax1.tick_params(axis='y', labelsize=22)\n",
    "colorbar.ax.set_ylabel('Relative normalized counts', fontsize=24)\n",
    "colorbar.ax.tick_params(labelsize=22)\n",
    "\n",
    "bottom_tick_positions = ax1.get_xticks()[1:-1]\n",
    "bottom_tick_labels = [f'{tick * 1e9:.0f}' for tick in bottom_tick_positions]\n",
    "\n",
    "ax1.set_xticks(bottom_tick_positions)\n",
    "ax1.set_xticklabels(bottom_tick_labels, fontsize=22)  # Adjust fontsize\n",
    "\n",
    "\n",
    "plt.axvline(2.2e-7,c='b',linewidth=4)\n",
    "plt.axvline(1.965e-7,c='b',linewidth=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9773e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ns = X * 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca76750",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X_ns, Y, hists_norm, cmap='magma', shading='auto')\n",
    "colorbar = plt.colorbar(c, label='Normalized counts (arb. units)', extend='max')\n",
    "plt.xlabel('Time of flight (ns)', fontsize=20)\n",
    "plt.ylabel('Hit intensity (arb. unit)', fontsize=20)\n",
    "plt.xlim(120, 490)\n",
    "plt.ylim(501,4900)\n",
    "ax1 = plt.gca()\n",
    "ax1.tick_params(axis='x', labelsize=16)\n",
    "ax1.tick_params(axis='y', labelsize=16)\n",
    "colorbar.ax.set_ylabel('Normalized counts (arb. units)', fontsize=20)\n",
    "colorbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743486f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, cmap='plasma', shading='auto')\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "vmin = np.percentile(hists_norm, 20)\n",
    "vmax = np.percentile(hists_norm, 99)\n",
    "norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto', norm=norm, cmap='plasma')\n",
    "\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', vmax=0.7)\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', cmap='plasma', norm=LogNorm(vmin=.1, vmax=1))\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of events slice')\n",
    "# plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "# plt.axvline(2.37e-7,c='r')\n",
    "# plt.axvline(3.34e-7,c='r')\n",
    "plt.axvline(3.7e-7,c='r')\n",
    "plt.axvline(4.1e-7,c='r')\n",
    "plt.axvline(4.5e-7,c='r')\n",
    "# plt.axhline(y=2000, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb18f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = [500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500]\n",
    "\n",
    "# Create a new figure for the 1D projections\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot 1D projections for each intensity\n",
    "for intensity in intensities:\n",
    "    # Find the index closest to the desired intensity\n",
    "    idx = np.argmin(np.abs(bins[:-1] - intensity))\n",
    "    \n",
    "    # Extract the 1D projection for this intensity\n",
    "    projection = hists_norm[idx]\n",
    "    \n",
    "    # Plot the projection\n",
    "    plt.plot(x_edges, projection, label=f'Intensity {intensity}')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Relative normalized counts')\n",
    "plt.title('1D Projections for Different Intensities')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add vertical lines as in the original plot\n",
    "plt.axvline(3.7e-7, c='r', linestyle='--', label='Vertical lines')\n",
    "plt.axvline(4.1e-7, c='r', linestyle='--')\n",
    "plt.axvline(4.5e-7, c='r', linestyle='--')\n",
    "\n",
    "plt.xlim(x_lower, x_upper)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define parameters\n",
    "x_lower, x_upper = 3.6e-7, 4.5e-7\n",
    "intensities = [1000, 1250, 1500, 1750, 2000, 2250, 2500]\n",
    "\n",
    "# Extract and plot projections\n",
    "peak_times = []\n",
    "for intensity in intensities:\n",
    "    idx = np.argmin(np.abs(bins[:-1] - intensity))\n",
    "    projection = hists_norm[idx]\n",
    "    \n",
    "    # Get time indices for the region of interest\n",
    "    time_mask = (x_edges >= x_lower) & (x_edges <= x_upper)\n",
    "    times = x_edges[time_mask]\n",
    "    proj_masked = projection[time_mask]\n",
    "    \n",
    "    plt.plot(times, proj_masked, label=f'Intensity {intensity}')\n",
    "    \n",
    "    peak_time = times[np.argmax(proj_masked)]\n",
    "    peak_times.append(peak_time)\n",
    "\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Relative normalized counts')\n",
    "plt.title('1D Projections (3.2e-7 to 4.2e-7 s)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "peak_times = np.array(peak_times)\n",
    "# Use only the higher intensity region where exponential behavior is clear\n",
    "intensities = intensities[3:]\n",
    "peak_times = peak_times[3:]\n",
    "\n",
    "# Second plot: Exponential fitting\n",
    "def exponential(x, a, b, c):\n",
    "    return a * np.exp(b * (x - x[0])) + c  # Normalize x to improve fitting\n",
    "\n",
    "# Provide initial guesses and bounds for better fitting\n",
    "p0 = [1000, 1e7, 1000]  # Initial parameters\n",
    "bounds = ([0, 0, 0], [10000, 1e8, 5000])  # Lower and upper bounds\n",
    "\n",
    "popt, pcov = curve_fit(exponential, peak_times, intensities, p0=p0, bounds=bounds)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(peak_times, intensities, color='blue', label='Peak positions')\n",
    "\n",
    "# Plot fit line\n",
    "x_fit = np.linspace(min(peak_times), max(peak_times), 100)\n",
    "y_fit = exponential(x_fit, *popt)\n",
    "plt.plot(x_fit, y_fit, 'r-', \n",
    "         label=f'Exponential fit\\ny = {popt[0]:.2f}*exp({popt[1]:.2e}*(t-t0)) + {popt[2]:.2f}')\n",
    "\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Intensity')\n",
    "plt.title('Peak Position vs Intensity with Exponential Fit')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = hists500\n",
    "nbins_tof = NBINS_TOF\n",
    "xlimits = (3e-7,7e-7)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', vmax=.3)\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', cmap='viridis', norm=LogNorm())\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of events slice')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "# plt.axvline(2.37e-7,c='r')\n",
    "# plt.axvline(3.34e-7,c='r')\n",
    "plt.axvline(4.6e-7,c='r')\n",
    "plt.axvline(5.4e-7,c='r')\n",
    "# plt.axhline(y=2000, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d93763",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = hists500\n",
    "nbins_tof = NBINS_TOF\n",
    "xlimits = (1e-7,5e-7)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54364d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = hists300\n",
    "nbins_tof = NBINS_TOF\n",
    "xlimits = (1e-7,5e-7)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = hists100\n",
    "nbins_tof = NBINS_TOF\n",
    "xlimits = (1e-7,5e-7)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac640273",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', vmax=.3)\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', cmap='viridis', norm=LogNorm())\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of events slice')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "plt.axvline(1.9e-7,c='r')\n",
    "plt.axvline(2.7e-7,c='r')\n",
    "plt.axvline(3.7e-7,c='r')\n",
    "plt.axvline(4.4e-7,c='r')\n",
    "# plt.axhline(y=2000, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cefef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', vmax=.3)\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', cmap='viridis', norm=LogNorm())\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of events slice')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "plt.axvline(1.9e-7,c='r')\n",
    "plt.axvline(2.7e-7,c='r')\n",
    "plt.axvline(3.7e-7,c='r')\n",
    "plt.axvline(4.4e-7,c='r')\n",
    "# plt.axhline(y=2000, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ade83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', vmax=.3)\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', cmap='viridis', norm=LogNorm())\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of events slice')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "plt.axvline(1.9e-7,c='r')\n",
    "plt.axvline(2.7e-7,c='r')\n",
    "plt.axvline(3.7e-7,c='r')\n",
    "plt.axvline(4.4e-7,c='r')\n",
    "# plt.axhline(y=2000, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_heatmap_abs_tof(hists500,NBINS_TOF,bins,(1e-7,5e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde52d4e",
   "metadata": {},
   "source": [
    "#### H ions rate of change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2235dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_bins = 30\n",
    "start_time = 1.9e-7\n",
    "end_time = 2.7e-7\n",
    "\n",
    "ion_dfpulse500['bins'] = pd.cut(ion_dfpulse500['nevents_pulse'], bins=nevents_bins, include_lowest=True)\n",
    "pulseIds_by_bin = ion_dfpulse500.groupby('bins')['pulseId'].apply(set).reset_index()\n",
    "\n",
    "mask = (ion_dfevent500['tof'] >= start_time) & (ion_dfevent500['tof'] <= end_time)\n",
    "filtered_dfevent = ion_dfevent500[mask]\n",
    "\n",
    "counts = []\n",
    "for pulse_ids in pulseIds_by_bin['pulseId']:\n",
    "    count = filtered_dfevent['pulseId'].isin(pulse_ids).sum()\n",
    "    counts.append(count)\n",
    "\n",
    "rate_of_change_H = [counts[i] - counts[i-1] for i in range(1, len(counts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_bins = 30\n",
    "start_time = 2.7e-7\n",
    "end_time = 3.7e-7\n",
    "\n",
    "ion_dfpulse500['bins'] = pd.cut(ion_dfpulse500['nevents_pulse'], bins=nevents_bins, include_lowest=True)\n",
    "pulseIds_by_bin = ion_dfpulse500.groupby('bins')['pulseId'].apply(set).reset_index()\n",
    "\n",
    "mask = (ion_dfevent500['tof'] >= start_time) & (ion_dfevent500['tof'] <= end_time)\n",
    "filtered_dfevent = ion_dfevent500[mask]\n",
    "\n",
    "counts = []\n",
    "for pulse_ids in pulseIds_by_bin['pulseId']:\n",
    "    count = filtered_dfevent['pulseId'].isin(pulse_ids).sum()\n",
    "    counts.append(count)\n",
    "\n",
    "rate_of_change_H2 = [counts[i] - counts[i-1] for i in range(1, len(counts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568eb06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_bins = 30\n",
    "start_time = 3.7e-7\n",
    "end_time = 4.4e-7\n",
    "\n",
    "ion_dfpulse500['bins'] = pd.cut(ion_dfpulse500['nevents_pulse'], bins=nevents_bins, include_lowest=True)\n",
    "pulseIds_by_bin = ion_dfpulse500.groupby('bins')['pulseId'].apply(set).reset_index()\n",
    "\n",
    "mask = (ion_dfevent500['tof'] >= start_time) & (ion_dfevent500['tof'] <= end_time)\n",
    "filtered_dfevent = ion_dfevent500[mask]\n",
    "\n",
    "counts = []\n",
    "for pulse_ids in pulseIds_by_bin['pulseId']:\n",
    "    count = filtered_dfevent['pulseId'].isin(pulse_ids).sum()\n",
    "    counts.append(count)\n",
    "\n",
    "rate_of_change_H3 = [counts[i] - counts[i-1] for i in range(1, len(counts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(bins[1:-1], rate_of_change_H, label='H+')\n",
    "plt.scatter(bins[1:-1], rate_of_change_H2, label='H2+')\n",
    "plt.scatter(bins[1:-1], rate_of_change_H3, label='H3+')\n",
    "plt.xlabel('Total number of hits')\n",
    "plt.ylabel('Rate of change')\n",
    "plt.title('Hydrogen ions rate of change in terms of total intensity')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98596de3",
   "metadata": {},
   "source": [
    "#### H3+ intensity dependance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_bins = 30\n",
    "start_time = 3.7e-7\n",
    "end_time = 4.5e-7\n",
    "\n",
    "ion_dfpulse500['bins'] = pd.cut(ion_dfpulse500['nevents_pulse'], bins=nevents_bins, include_lowest=True)\n",
    "pulseIds_by_bin = ion_dfpulse500.groupby('bins')['pulseId'].apply(set).reset_index()\n",
    "\n",
    "mask = (ion_dfevent500['tof'] >= start_time) & (ion_dfevent500['tof'] <= end_time)\n",
    "filtered_dfevent = ion_dfevent500[mask]\n",
    "\n",
    "counts = []\n",
    "for pulse_ids in pulseIds_by_bin['pulseId']:\n",
    "    count = filtered_dfevent['pulseId'].isin(pulse_ids).sum()\n",
    "    counts.append(count)\n",
    "\n",
    "lower_bounds = [int(interval.left) for interval in pulseIds_by_bin['bins']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f14b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_bins = 30\n",
    "start_time = 4.6e-7\n",
    "end_time = 5.4e-7\n",
    "\n",
    "ion_dfpulse500['bins'] = pd.cut(ion_dfpulse500['nevents_pulse'], bins=nevents_bins, include_lowest=True)\n",
    "pulseIds_by_bin = ion_dfpulse500.groupby('bins')['pulseId'].apply(set).reset_index()\n",
    "\n",
    "mask = (ion_dfevent500['tof'] >= start_time) & (ion_dfevent500['tof'] <= end_time)\n",
    "filtered_dfevent = ion_dfevent500[mask]\n",
    "\n",
    "bkgrd_counts = []\n",
    "for pulse_ids in pulseIds_by_bin['pulseId']:\n",
    "    count = filtered_dfevent['pulseId'].isin(pulse_ids).sum()\n",
    "    bkgrd_counts.append(count)\n",
    "\n",
    "bkgrd_lower_bounds = [int(interval.left) for interval in pulseIds_by_bin['bins']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81463584",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lower_bounds, counts, marker='o', label='H3+')\n",
    "plt.scatter(bkgrd_lower_bounds, bkgrd_counts, marker='x', label='background')\n",
    "plt.xlabel('Total number of hits')\n",
    "plt.ylabel('Number of hits in the swoop')\n",
    "plt.title('H3+ swoop intensity in terms of total intensity')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_bins = 30\n",
    "start_time = 3.7e-7\n",
    "end_time = 4.5e-7\n",
    "\n",
    "ion_dfpulse300['bins'] = pd.cut(ion_dfpulse300['nevents_pulse'], bins=nevents_bins, include_lowest=True)\n",
    "pulseIds_by_bin = ion_dfpulse300.groupby('bins')['pulseId'].apply(set).reset_index()\n",
    "\n",
    "mask = (ion_dfevent300['tof'] >= start_time) & (ion_dfevent300['tof'] <= end_time)\n",
    "filtered_dfevent = ion_dfevent300[mask]\n",
    "\n",
    "counts = []\n",
    "for pulse_ids in pulseIds_by_bin['pulseId']:\n",
    "    count = filtered_dfevent['pulseId'].isin(pulse_ids).sum()\n",
    "    counts.append(count)\n",
    "\n",
    "lower_bounds = [int(interval.left) for interval in pulseIds_by_bin['bins']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f39810",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_bins = 30\n",
    "start_time = 4.6e-7\n",
    "end_time = 5.4e-7\n",
    "\n",
    "ion_dfpulse300['bins'] = pd.cut(ion_dfpulse300['nevents_pulse'], bins=nevents_bins, include_lowest=True)\n",
    "pulseIds_by_bin = ion_dfpulse300.groupby('bins')['pulseId'].apply(set).reset_index()\n",
    "\n",
    "mask = (ion_dfevent300['tof'] >= start_time) & (ion_dfevent300['tof'] <= end_time)\n",
    "filtered_dfevent = ion_dfevent300[mask]\n",
    "\n",
    "bkgrd_counts = []\n",
    "for pulse_ids in pulseIds_by_bin['pulseId']:\n",
    "    count = filtered_dfevent['pulseId'].isin(pulse_ids).sum()\n",
    "    bkgrd_counts.append(count)\n",
    "\n",
    "bkgrd_lower_bounds = [int(interval.left) for interval in pulseIds_by_bin['bins']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lower_bounds, counts, marker='o', label='H3+')\n",
    "plt.scatter(bkgrd_lower_bounds, bkgrd_counts, marker='x', label='background')\n",
    "plt.xlabel('Total number of hits')\n",
    "plt.ylabel('Number of hits in the swoop')\n",
    "plt.title('H3+ swoop intensity in terms of total intensity')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_bins = 30\n",
    "start_time = 3.7e-7\n",
    "end_time = 4.5e-7\n",
    "\n",
    "ion_dfpulse100['bins'] = pd.cut(ion_dfpulse100['nevents_pulse'], bins=nevents_bins, include_lowest=True)\n",
    "pulseIds_by_bin = ion_dfpulse100.groupby('bins')['pulseId'].apply(set).reset_index()\n",
    "\n",
    "mask = (ion_dfevent300['tof'] >= start_time) & (ion_dfevent300['tof'] <= end_time)\n",
    "filtered_dfevent = ion_dfevent300[mask]\n",
    "\n",
    "counts = []\n",
    "for pulse_ids in pulseIds_by_bin['pulseId']:\n",
    "    count = filtered_dfevent['pulseId'].isin(pulse_ids).sum()\n",
    "    counts.append(count)\n",
    "\n",
    "lower_bounds = [int(interval.left) for interval in pulseIds_by_bin['bins']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398d3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_bins = 30\n",
    "start_time = 4.6e-7\n",
    "end_time = 5.4e-7\n",
    "\n",
    "ion_dfpulse100['bins'] = pd.cut(ion_dfpulse100['nevents_pulse'], bins=nevents_bins, include_lowest=True)\n",
    "pulseIds_by_bin = ion_dfpulse100.groupby('bins')['pulseId'].apply(set).reset_index()\n",
    "\n",
    "mask = (ion_dfevent300['tof'] >= start_time) & (ion_dfevent300['tof'] <= end_time)\n",
    "filtered_dfevent = ion_dfevent300[mask]\n",
    "\n",
    "bkgrd_counts = []\n",
    "for pulse_ids in pulseIds_by_bin['pulseId']:\n",
    "    count = filtered_dfevent['pulseId'].isin(pulse_ids).sum()\n",
    "    bkgrd_counts.append(count)\n",
    "\n",
    "bkgrd_lower_bounds = [int(interval.left) for interval in pulseIds_by_bin['bins']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lower_bounds, counts, marker='o', label='H3+')\n",
    "plt.scatter(bkgrd_lower_bounds, bkgrd_counts, marker='x', label='background')\n",
    "plt.xlabel('Total number of hits')\n",
    "plt.ylabel('Number of hits in the swoop')\n",
    "plt.title('H3+ swoop intensity in terms of total intensity')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49522a6",
   "metadata": {},
   "source": [
    "#### Silicon species intensity dependance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efebde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = hists500\n",
    "nbins_tof = NBINS_TOF\n",
    "xlimits = (1e-7,5e-7)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "ax1 = plt.gca()\n",
    "ax1.set_facecolor('white') \n",
    "\n",
    "c = ax1.pcolormesh(X, Y, hists_norm, shading='auto', cmap='plasma')\n",
    "colorbar = plt.colorbar(c, label='Relative normalized counts (a.u.)', extend='max')\n",
    "plt.xlabel('Time of flight ($10^{-7}$ s)', fontsize=24)\n",
    "plt.ylabel('Hit intensity', fontsize=24)\n",
    "# plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# ax2 = ax1.twiny()\n",
    "\n",
    "# top_tick_positions = [1.09e-6,1.13e-6,1.17e-6,1.19e-6,1.22e-6,1.25e-6,1.3e-6]\n",
    "# top_tick_labels = ['750', '500', '300', '200', '100', '50', '0']\n",
    "# ax2.set_xticks(top_tick_positions)\n",
    "# ax2.set_xticklabels(top_tick_labels)\n",
    "\n",
    "# ax2.set_xlim(ax1.get_xlim())\n",
    "# ax2.set_xlabel('Simulated kinetic energy (eV)', fontsize=24)\n",
    "\n",
    "# ax2.spines['top'].set_color('blue')\n",
    "# ax2.xaxis.label.set_color('blue')\n",
    "# ax2.tick_params(axis='x', colors='blue', labelsize=22)\n",
    "# plt.axvline(2.37e-7,c='r',linewidth=3)\n",
    "# plt.axvline(3.34e-7,c='r',linewidth=3)\n",
    "# plt.axvline(4.1e-7,c='r',linewidth=3)\n",
    "ax1.tick_params(axis='x', labelsize=22)\n",
    "ax1.tick_params(axis='y', labelsize=22)\n",
    "colorbar.ax.set_ylabel('Relative normalized counts', fontsize=24)\n",
    "colorbar.ax.tick_params(labelsize=22)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e086124",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = hists500[:50]\n",
    "nbins_tof = NBINS_TOF\n",
    "xlimits = (1.1e-6,1.4e-6)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:50])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', vmax=.3)\n",
    "# c = plt.pcolormesh(X, Y, hists_norm, shading='auto', cmap='viridis', norm=LogNorm())\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of events slice')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "plt.axhline(y=1500, c='red')\n",
    "plt.axhline(y=2000, c='red')\n",
    "plt.axhline(y=2600, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97222718",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = hists500\n",
    "nbins_tof = NBINS_TOF\n",
    "xlimits = (1e-7,5e-7)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of events slice')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "plt.axvline(2.156e-7,c='r')\n",
    "plt.axvline(2.222e-7,c='r')\n",
    "plt.axvline(2.299e-7,c='r')\n",
    "plt.axvline(2.376e-7,c='r')\n",
    "plt.axvline(2.424e-7,c='r')\n",
    "plt.axvline(2.485e-7,c='r')\n",
    "plt.axvline(2.528e-7,c='r')\n",
    "plt.axvline(2.63e-7,c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311115ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_ion_dfpulse = ion_dfpulse500[ion_dfpulse500.nevents_pulse > 500][ion_dfpulse500.nevents_pulse < 4500]\n",
    "current_ion_dfevent = ion_dfevent500[ion_dfevent500.pulseId.isin(current_ion_dfpulse.pulseId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF = 3000\n",
    "NBINS_EVENTS = 5\n",
    "\n",
    "filtered_dfevents, filtered_dfpulses, hists, bins = nions_binning_tof(current_ion_dfevent,current_ion_dfpulse,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "xlimits = (1.6e-6,1.9e-6)\n",
    "\n",
    "nbins_tof = NBINS_TOF\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of events slice')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d8416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#250 bin size, 9 bins\n",
    "weak_Si_nevents_bins = [750,1000,1250,1500,1750,2000,2250,2500,2750] #3000\n",
    "weak_Si_lower_limit = [1.302e-6,1.293e-6,1.285e-6,1.262e-6,1.212e-6,1.188e-6,1.156e-6,1.145e-6,1.140e-6]\n",
    "weak_Si_upper_limit = [1.323e-6,1.320e-6,1.315e-6,1.312e-6,1.296e-6,1.279e-6,1.243e-6,1.191e-6,1.164e-6]\n",
    "\n",
    "#500 bin size, 10 bins\n",
    "mid_Si_nevents_bins = [3000,3500,4000,4500,5000,5500,6000,6500,7000,7500] #8000\n",
    "mid_Si_lower_limit = [1.136e-6,1.132e-6,1.129e-6,1.127e-6,1.126e-6,1.125e-6,1.124e-6,1.123e-6,1.122e-6,1.121e-6]\n",
    "mid_Si_upper_limit = [1.156e-6,1.150e-6,1.148e-6,1.144e-6,1.141e-6,1.138e-6,1.137e-6,1.136e-6,1.135e-6,1.135e-6]\n",
    "\n",
    "#1000 bin size, 7 bins\n",
    "strong_Si_nevents_bins = [8000,9000,10000,11000,12000,13000,14000] #15000\n",
    "strong_Si_lower_limit = [1.122e-6,1.121e-6,1.121e-6,1.123e-6,1.122e-6,1.122e-6,1.121e-6]\n",
    "strong_Si_upper_limit = [1.138e-6,1.148e-6,1.142e-6,1.139e-6,1.135e-6,1.134e-6,1.134e-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b38810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#250 bin size, 6 bins\n",
    "weak_SiO_nevents_bins = [500,750,1000,1250,1500,1750] #2000\n",
    "weak_SiO_lower_limit = [1.492e-6,1.488e-6,1.485e-6,1.484e-6,1.472e-6,1.472e-6]\n",
    "weak_SiO_upper_limit = [1.582e-6,1.566e-6,1.566e-6,1.562e-6,1.551e-6,1.542e-6]\n",
    "\n",
    "#no clear data from 2000 to 3000 events\n",
    "\n",
    "missing_SiO_nevents_bins = [2000]\n",
    "missing_SiO_lower_limit = []\n",
    "missing_SiO_upper_limit = []\n",
    "\n",
    "#500 bin size, 6 bins\n",
    "mid_SiO_nevents_bins = [3500,4000,4500,5000,5500] #6000\n",
    "mid_SiO_lower_limit = [1.223e-6,1.218e-6,1.217e-6,1.217e-6,1.217e-6,]\n",
    "mid_SiO_upper_limit = [1.263e-6,1.260e-6,1.252e-6,1.246e-6,1.244e-6]\n",
    "\n",
    "#1000 bin size, 9 bins\n",
    "strong_SiO_nevents_bins = [6000,7000,8000,9000,10000,11000,12000,13000,14000] #15000\n",
    "strong_SiO_lower_limit = [1.219e-6,1.217e-6,1.215e-6,1.219e-6,1.220e-6,1.219e-6,1.219e-6,1.216e-6,1.215e-6]\n",
    "strong_SiO_upper_limit = [1.243e-6,1.244e-6,1.243e-6,1.243e-6,1.243e-6,1.242e-6,1.245e-6,1.244e-6,1.246e-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#250 bin size, 4 bins\n",
    "weak_Si2_nevents_bins = [500,750,1000,1250] #1500\n",
    "weak_Si2_lower_limit = [1.695e-6,1.684e-6,1.677e-6,1.669e-6]\n",
    "weak_Si2_upper_limit = [1.763e-6,1.747e-6,1.726e-6,1.723e-6]\n",
    "\n",
    "#100 bin size, 10 bins\n",
    "mid_Si2_nevents_bins = [1500,1600,1700,1800,1900,2000,2100,2200,2300,2400] #2500\n",
    "mid_Si2_lower_limit = [1.663e-6,1.651e-6,1.654e-6,1.647e-6,1.629e-6,1.603e-6,1.583e-6,1.534e-6,1.520e-6,1.514e-6]\n",
    "mid_Si2_upper_limit = [1.698e-6,1.695e-6,1.691e-6,1.686e-6,1.676e-6,1.665e-6,1.647e-6,1.622e-6,1.587e-6,1.564e-6]\n",
    "\n",
    "#500 bin size, 4 bins\n",
    "strong_Si2_nevents_bins = [2500,3000,3500,4000] #4500\n",
    "strong_Si2_lower_limit = [1.508e-6,1.503e-6,1.505e-6,1.502e-6]\n",
    "strong_Si2_upper_limit = [1.552e-6,1.543e-6,1.538e-6,1.538e-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0071f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Si_nevents_bins = sum([weak_Si_nevents_bins, mid_Si_nevents_bins, strong_Si_nevents_bins,[15000]], [])\n",
    "Si_lower_limit = sum([weak_Si_lower_limit, mid_Si_lower_limit, strong_Si_lower_limit], [])\n",
    "Si_upper_limit = sum([weak_Si_upper_limit, mid_Si_upper_limit, strong_Si_upper_limit], [])\n",
    "\n",
    "SiO_nevents_bins = sum([weak_SiO_nevents_bins, missing_SiO_nevents_bins, mid_SiO_nevents_bins, strong_SiO_nevents_bins,[15000]], [])\n",
    "SiO_lower_limit = sum([weak_SiO_lower_limit, missing_SiO_lower_limit, mid_SiO_lower_limit, strong_SiO_lower_limit], [])\n",
    "SiO_upper_limit = sum([weak_SiO_upper_limit, missing_SiO_upper_limit, mid_SiO_upper_limit, strong_SiO_upper_limit], [])\n",
    "\n",
    "Si2_nevents_bins = sum([weak_Si2_nevents_bins, mid_Si2_nevents_bins, strong_Si2_nevents_bins,[4500]], [])\n",
    "Si2_lower_limit = sum([weak_Si2_lower_limit, mid_Si2_lower_limit, strong_Si2_lower_limit], [])\n",
    "Si2_upper_limit = sum([weak_Si2_upper_limit, mid_Si2_upper_limit, strong_Si2_upper_limit], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_limit = Si2_lower_limit\n",
    "upper_limit = Si2_upper_limit\n",
    "nevents_bins = Si2_nevents_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_bin_edges = nevents_bins\n",
    "dfpulse = current_ion_dfpulse\n",
    "dfevent = current_ion_dfevent\n",
    "nbins_tof = 3000\n",
    "\n",
    "\n",
    "filtered_dfevents = []\n",
    "filtered_dfpulses = []\n",
    "hists = []\n",
    "\n",
    "for i in range(len(nevents_bin_edges) - 1):\n",
    "\n",
    "    start_edge = nevents_bin_edges[i]\n",
    "    end_edge = nevents_bin_edges[i + 1]\n",
    "\n",
    "    # Filtering the pulses within the current bin range\n",
    "    filtered_dfpulse = dfpulse[(dfpulse.nevents_pulse >= start_edge) & (dfpulse.nevents_pulse < end_edge)]\n",
    "    filtered_dfevent = dfevent[dfevent.pulseId.isin(filtered_dfpulse.pulseId)]\n",
    "\n",
    "    filtered_dfevents.append(filtered_dfevent)\n",
    "    filtered_dfpulses.append(filtered_dfpulse)\n",
    "    \n",
    "    # Histogram of tof with predefined bins\n",
    "    hist, bin_edges = np.histogram(\n",
    "        filtered_dfevent.tof, \n",
    "        bins=np.linspace(0, TIME_BETWEEN_PULSES, nbins_tof + 1),\n",
    "        range=(0, TIME_BETWEEN_PULSES)\n",
    "    )\n",
    "    \n",
    "    # Normalize the histogram by the number of pulses in the filtered_dfpulse\n",
    "    if len(filtered_dfpulse) > 0:\n",
    "        hists.append(hist / len(filtered_dfpulse))\n",
    "    else:\n",
    "        hists.append(np.zeros_like(hist))  # Handle case with no pulses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d36162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "xlimits = (1.4e-6, 1.8e-6)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES / nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower / precision), int(x_upper / precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "    \n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "\n",
    "# Generate meshgrid\n",
    "X, Y = np.meshgrid(x_edges, nevents_bin_edges[:-1])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Number of events slice')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "\n",
    "# Adjust y-ticks to be centered on the y-bins\n",
    "y_tick_positions = nevents_bin_edges[:-1] - 0.5 * np.diff(nevents_bin_edges)\n",
    "plt.yticks(ticks=y_tick_positions, labels=nevents_bin_edges[:-1])\n",
    "\n",
    "y_tick_edges = np.append(y_tick_positions,nevents_bin_edges[-1]- 0.5 * (nevents_bin_edges[-1]-nevents_bin_edges[-2]))\n",
    "\n",
    "# Plotting the vertical lines at the specified x-coordinates using the centered y-tick positions\n",
    "for i, x_coord in enumerate(lower_limit):\n",
    "    y_lower = y_tick_edges[i] # Start of the y-bin\n",
    "    y_upper = y_tick_edges[i+1] # End of the y-bin\n",
    "    plt.plot([x_coord, x_coord], [y_lower, y_upper], color='red', linestyle='-', linewidth=2)\n",
    "\n",
    "for i, x_coord in enumerate(upper_limit):\n",
    "    y_lower = y_tick_edges[i] # Start of the y-bin\n",
    "    y_upper = y_tick_edges[i+1] # End of the y-bin\n",
    "    plt.plot([x_coord, x_coord], [y_lower, y_upper], color='red', linestyle='-', linewidth=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e23a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_limit = Si2_lower_limit\n",
    "upper_limit = Si2_upper_limit\n",
    "nevents_bins = Si2_nevents_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b804bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpulse['bins'] = pd.cut(dfpulse['nevents_pulse'], bins=nevents_bins, include_lowest=True)\n",
    "\n",
    "pulseIds_by_bin = dfpulse.groupby('bins')['pulseId'].apply(list).reset_index()\n",
    "\n",
    "filtered_dfevent = dfevent[dfevent.pulseId.isin(pulseIds_by_bin)]\n",
    "\n",
    "counts = []\n",
    "\n",
    "# Iterate over each bin range and corresponding pulseIds\n",
    "for i, row in pulseIds_by_bin.iterrows():\n",
    "    # Filter dfevent based on pulseIds for the current bin\n",
    "    filtered_events = dfevent[dfevent['pulseId'].isin(row['pulseId'])]\n",
    "    \n",
    "    # Apply the counting logic\n",
    "    count = ((filtered_events.tof >= lower_limit[i]) & (filtered_events.tof <= upper_limit[i])).sum()\n",
    "    counts.append(count)\n",
    "    \n",
    "lower_bounds = [int(interval.left) for interval in pulseIds_by_bin['bins']]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(lower_bounds, counts, marker='o')\n",
    "plt.xlabel('Swoop intensity')\n",
    "plt.ylabel('Number of hits')\n",
    "plt.title('SiO swoop intensity in terms of total intensity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_swoop_intensity(dfevent,dfpulse,nevents_bins,lower_limit,upper_limit):\n",
    "    \n",
    "    dfpulse['bins'] = pd.cut(dfpulse['nevents_pulse'], bins=nevents_bins, include_lowest=True)\n",
    "\n",
    "    pulseIds_by_bin = dfpulse.groupby('bins')['pulseId'].apply(list).reset_index()\n",
    "\n",
    "    filtered_dfevent = dfevent[dfevent.pulseId.isin(pulseIds_by_bin)]\n",
    "\n",
    "    counts = []\n",
    "\n",
    "    # Iterate over each bin range and corresponding pulseIds\n",
    "    for i, row in pulseIds_by_bin.iterrows():\n",
    "        # Filter dfevent based on pulseIds for the current bin\n",
    "        filtered_events = dfevent[dfevent['pulseId'].isin(row['pulseId'])]\n",
    "\n",
    "        # Apply the counting logic\n",
    "        count = ((filtered_events.tof >= lower_limit[i]) & (filtered_events.tof <= upper_limit[i])).sum()\n",
    "        counts.append(count)\n",
    "\n",
    "    lower_bounds = [int(interval.left) for interval in pulseIds_by_bin['bins']]\n",
    "    \n",
    "    return lower_bounds, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "Si_lower_bounds, Si_counts = compute_swoop_intensity(dfevent,dfpulse,Si_nevents_bins,Si_lower_limit,Si_upper_limit)\n",
    "SiO_lower_bounds, SiO_counts = compute_swoop_intensity(dfevent,dfpulse,SiO_nevents_bins,SiO_lower_limit,SiO_upper_limit)\n",
    "Si2_lower_bounds, Si2_counts = compute_swoop_intensity(dfevent,dfpulse,Si2_nevents_bins,Si2_lower_limit,Si2_upper_limit)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(Si_lower_bounds, Si_counts, label='Si')\n",
    "plt.scatter(SiO_lower_bounds, SiO_counts, label='SiO')\n",
    "plt.scatter(Si2_lower_bounds, Si2_counts, label='2Si')\n",
    "plt.xlabel('Swoop intensity')\n",
    "plt.ylabel('Number of hits')\n",
    "plt.title('Swoop intensities in terms of total intensity')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hists = hists500\n",
    "nbins_tof = NBINS_TOF\n",
    "xlimits = (1.08e-6,1.2e-6)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "hists_short = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist #/ max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "    hists_short.append(shortened_hist)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "hists_norm = np.array(hists_norm)\n",
    "hists_short = np.array(hists_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3857822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Hit intensity')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(X.min(), X.max())\n",
    "plt.ylim(Y.min(), Y.max())\n",
    "\n",
    "# Define a threshold for high-intensity regions\n",
    "threshold = 20  # Adjust this value as necessary\n",
    "high_intensity_mask = hists_norm > threshold\n",
    "\n",
    "# Identify connected components in the high-intensity mask\n",
    "labeled_array, num_features = label(high_intensity_mask)\n",
    "\n",
    "# Find the largest connected component\n",
    "sizes = np.bincount(labeled_array.ravel())\n",
    "sizes[0] = 0  # Ignore the background\n",
    "largest_label = sizes.argmax()\n",
    "\n",
    "# Create a mask for the largest component (the swoop)\n",
    "swoop_mask = labeled_array == largest_label\n",
    "\n",
    "# Integrate the area under the high-intensity swoop\n",
    "area_high_intensity_swoop = np.sum(swoop_mask * hists_short)\n",
    "\n",
    "# Highlight the high-intensity swoop region\n",
    "plt.contour(X, Y, swoop_mask, levels=[0.5], colors='red', linewidths=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Integrated area under the high-intensity swoop region: {area_high_intensity_swoop:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hists = hists500\n",
    "nbins_tof = NBINS_TOF\n",
    "xlimits = (1.21e-6,1.4e-6)#(1.4e-6,1.8e-6)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events = len(hists)\n",
    "hists_norm = []\n",
    "hists_short = []\n",
    "\n",
    "for i in range(nbins_events):\n",
    "\n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist #/ max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "    hists_short.append(shortened_hist)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins[:-1])\n",
    "hists_norm = np.array(hists_norm)\n",
    "hists_short = np.array(hists_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ee79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Hit intensity')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(X.min(), X.max())\n",
    "plt.ylim(Y.min(), Y.max())\n",
    "\n",
    "# Define a threshold for high-intensity regions\n",
    "threshold = 3  # Adjust this value as necessary\n",
    "high_intensity_mask = hists_norm > threshold\n",
    "\n",
    "# Identify connected components in the high-intensity mask\n",
    "labeled_array, num_features = label(high_intensity_mask)\n",
    "\n",
    "# Find the largest connected component\n",
    "sizes = np.bincount(labeled_array.ravel())\n",
    "sizes[0] = 0  # Ignore the background\n",
    "largest_label = sizes.argmax()\n",
    "\n",
    "# Create a mask for the largest component (the swoop)\n",
    "swoop_mask = labeled_array == largest_label\n",
    "\n",
    "# Integrate the area under the high-intensity swoop\n",
    "area_high_intensity_swoop = np.sum(swoop_mask * hists_short)\n",
    "\n",
    "# Highlight the high-intensity swoop region\n",
    "plt.contour(X, Y, swoop_mask, levels=[0.5], colors='red', linewidths=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Integrated area under the high-intensity swoop region: {area_high_intensity_swoop:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b542004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hists = hists500\n",
    "nbins_tof = NBINS_TOF1\n",
    "xlimits = (1.08e-6,1.4e-6)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events1 = len(hists2)\n",
    "hists_norm = []\n",
    "hists_short = []\n",
    "\n",
    "for i in range(nbins_events1):\n",
    "\n",
    "    shortened_hist = hists2[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist #/ max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "    hists_short.append(shortened_hist)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins2[:-1])\n",
    "hists_norm = np.array(hists_norm)\n",
    "hists_short = np.array(hists_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Hit intensity')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(X.min(), X.max())\n",
    "plt.ylim(Y.min(), Y.max())\n",
    "\n",
    "# Define a threshold for high-intensity regions\n",
    "threshold = 30  # Adjust this value as necessary\n",
    "high_intensity_mask = hists_norm > threshold\n",
    "\n",
    "# Identify connected components in the high-intensity mask\n",
    "labeled_array, num_features = label(high_intensity_mask)\n",
    "\n",
    "# Find the largest connected component\n",
    "sizes = np.bincount(labeled_array.ravel())\n",
    "sizes[0] = 0  # Ignore the background\n",
    "largest_label = sizes.argmax()\n",
    "\n",
    "# Create a mask for the largest component (the swoop)\n",
    "swoop_mask = labeled_array == largest_label\n",
    "\n",
    "# Integrate the area under the high-intensity swoop\n",
    "area_high_intensity_swoop = np.sum(swoop_mask * hists_short)\n",
    "\n",
    "# Highlight the high-intensity swoop region\n",
    "plt.contour(X, Y, swoop_mask, levels=[0.5], colors='red', linewidths=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Integrated area under the high-intensity swoop region: {area_high_intensity_swoop:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hists = hists500\n",
    "nbins_tof = NBINS_TOF1\n",
    "xlimits = (1.21e-6,1.4e-6)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events1 = len(hists2)\n",
    "hists_norm = []\n",
    "hists_short = []\n",
    "\n",
    "for i in range(nbins_events1):\n",
    "\n",
    "    shortened_hist = hists2[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist #/ max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "    hists_short.append(shortened_hist)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins2[:-1])\n",
    "hists_norm = np.array(hists_norm)\n",
    "hists_short = np.array(hists_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ccc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Hit intensity')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(X.min(), X.max())\n",
    "plt.ylim(Y.min(), Y.max())\n",
    "\n",
    "# Define a threshold for high-intensity regions\n",
    "threshold = 20  # Adjust this value as necessary\n",
    "high_intensity_mask = hists_norm > threshold\n",
    "\n",
    "# Identify connected components in the high-intensity mask\n",
    "labeled_array, num_features = label(high_intensity_mask)\n",
    "\n",
    "# Find the largest connected component\n",
    "sizes = np.bincount(labeled_array.ravel())\n",
    "sizes[0] = 0  # Ignore the background\n",
    "largest_label = sizes.argmax()\n",
    "\n",
    "# Create a mask for the largest component (the swoop)\n",
    "swoop_mask = labeled_array == largest_label\n",
    "\n",
    "# Integrate the area under the high-intensity swoop\n",
    "area_high_intensity_swoop = np.sum(swoop_mask * hists_short)\n",
    "\n",
    "# Highlight the high-intensity swoop region\n",
    "plt.contour(X, Y, swoop_mask, levels=[0.5], colors='red', linewidths=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Integrated area under the high-intensity swoop region: {area_high_intensity_swoop:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1078f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF1 = 4000\n",
    "NBINS_EVENTS1 = 80\n",
    "\n",
    "filtered_dfevents1, filtered_dfpulses1, hists1, bins1 = nions_binning_tof(ion_dfevent500,ion_dfpulse500,NBINS_EVENTS1,NBINS_TOF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6768dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists2 = hists1[15:65,:]\n",
    "bins2 = bins1[15:66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hists = hists500\n",
    "nbins_tof = NBINS_TOF1\n",
    "xlimits = (1.08e-6,1.4e-6)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events1 = len(hists2)\n",
    "hists_norm = []\n",
    "hists_short = []\n",
    "\n",
    "for i in range(nbins_events1):\n",
    "\n",
    "    shortened_hist = hists2[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist #/ max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "    hists_short.append(shortened_hist)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins2[:-1])\n",
    "hists_norm = np.array(hists_norm)\n",
    "hists_short = np.array(hists_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Hit intensity')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(X.min(), X.max())\n",
    "plt.ylim(Y.min(), Y.max())\n",
    "\n",
    "# Define a threshold for high-intensity regions\n",
    "threshold = 30  # Adjust this value as necessary\n",
    "high_intensity_mask = hists_norm > threshold\n",
    "\n",
    "# Identify connected components in the high-intensity mask\n",
    "labeled_array, num_features = label(high_intensity_mask)\n",
    "\n",
    "# Find the largest connected component\n",
    "sizes = np.bincount(labeled_array.ravel())\n",
    "sizes[0] = 0  # Ignore the background\n",
    "largest_label = sizes.argmax()\n",
    "\n",
    "# Create a mask for the largest component (the swoop)\n",
    "swoop_mask = labeled_array == largest_label\n",
    "\n",
    "# Integrate the area under the high-intensity swoop\n",
    "area_high_intensity_swoop = np.sum(swoop_mask * hists_short)\n",
    "\n",
    "# Highlight the high-intensity swoop region\n",
    "plt.contour(X, Y, swoop_mask, levels=[0.5], colors='red', linewidths=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Integrated area under the high-intensity swoop region: {area_high_intensity_swoop:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hists = hists500\n",
    "nbins_tof = NBINS_TOF1\n",
    "xlimits = (1.21e-6,1.4e-6)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events1 = len(hists2)\n",
    "hists_norm = []\n",
    "hists_short = []\n",
    "\n",
    "for i in range(nbins_events1):\n",
    "\n",
    "    shortened_hist = hists2[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist #/ max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "    hists_short.append(shortened_hist)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins2[:-1])\n",
    "hists_norm = np.array(hists_norm)\n",
    "hists_short = np.array(hists_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Hit intensity')\n",
    "plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(X.min(), X.max())\n",
    "plt.ylim(Y.min(), Y.max())\n",
    "\n",
    "# Define a threshold for high-intensity regions\n",
    "threshold = 20  # Adjust this value as necessary\n",
    "high_intensity_mask = hists_norm > threshold\n",
    "\n",
    "# Identify connected components in the high-intensity mask\n",
    "labeled_array, num_features = label(high_intensity_mask)\n",
    "\n",
    "# Find the largest connected component\n",
    "sizes = np.bincount(labeled_array.ravel())\n",
    "sizes[0] = 0  # Ignore the background\n",
    "largest_label = sizes.argmax()\n",
    "\n",
    "# Create a mask for the largest component (the swoop)\n",
    "swoop_mask = labeled_array == largest_label\n",
    "\n",
    "# Integrate the area under the high-intensity swoop\n",
    "area_high_intensity_swoop = np.sum(swoop_mask * hists_short)\n",
    "\n",
    "# Highlight the high-intensity swoop region\n",
    "plt.contour(X, Y, swoop_mask, levels=[0.5], colors='red', linewidths=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Integrated area under the high-intensity swoop region: {area_high_intensity_swoop:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0064982",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF1 = 4000\n",
    "NBINS_EVENTS1 = 60\n",
    "\n",
    "filtered_dfevents1, filtered_dfpulses1, hists1, bins1 = nions_binning_tof(ion_dfevent500,ion_dfpulse500,NBINS_EVENTS1,NBINS_TOF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF1 = 4000\n",
    "NBINS_EVENTS1 = 60\n",
    "\n",
    "filtered_dfevents1, filtered_dfpulses1, hists1, bins1 = nions_binning_tof(dfevent_single_shot,dfpulse_single_shot,NBINS_EVENTS1,NBINS_TOF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists2 = hists1[15:65,:]\n",
    "bins2 = bins1[15:66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists2 = hists1[15:65,:]\n",
    "bins2 = bins1[15:66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hists = hists500\n",
    "nbins_tof = NBINS_TOF1\n",
    "xlimits = (1.08e-6,1.4e-6)\n",
    "\n",
    "precision = TIME_BETWEEN_PULSES/nbins_tof\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins_events1 = len(hists1)\n",
    "hists_norm = []\n",
    "hists_short = []\n",
    "\n",
    "for i in range(nbins_events1):\n",
    "\n",
    "    shortened_hist = hists1[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "    hists_short.append(shortened_hist)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "X, Y = np.meshgrid(x_edges, bins1[:-1])\n",
    "hists_norm = np.array(hists_norm)\n",
    "hists_short = np.array(hists_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4090e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto', cmap='plasma')\n",
    "colorbar = plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('Time of flight (s)', fontsize=24)\n",
    "plt.ylabel('Hit intensity', fontsize=24)\n",
    "# plt.title('Relative heatmap for number of events slices with respect to tof')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "\n",
    "\n",
    "ax1 = plt.gca()\n",
    "# ax2 = ax1.twiny()\n",
    "\n",
    "# top_tick_positions = [1.09e-6,1.13e-6,1.17e-6,1.19e-6,1.22e-6,1.25e-6,1.3e-6]\n",
    "# top_tick_labels = ['750', '500', '300', '200', '100', '50', '0']\n",
    "# ax2.set_xticks(top_tick_positions)\n",
    "# ax2.set_xticklabels(top_tick_labels)\n",
    "\n",
    "# ax2.set_xlim(ax1.get_xlim())\n",
    "# ax2.set_xlabel('Simulated kinetic energy (eV)', fontsize=24)\n",
    "\n",
    "# ax2.spines['top'].set_color('blue')\n",
    "# ax2.xaxis.label.set_color('blue')\n",
    "# ax2.tick_params(axis='x', colors='blue', labelsize=22)\n",
    "ax1.tick_params(axis='x', labelsize=22)\n",
    "ax1.tick_params(axis='y', labelsize=22)\n",
    "colorbar.ax.set_ylabel('Relative normalized counts', fontsize=24)\n",
    "colorbar.ax.tick_params(labelsize=22)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_TOF = 1500\n",
    "NBINS_EVENTS = 20\n",
    "\n",
    "filtered_dfeventslow, filtered_dfpulseslow, histslow, binslow = nions_binning_tof(ion_dfevent_lower_max,ion_dfpulse_lower_max,NBINS_EVENTS,NBINS_TOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_heatmap_rel_tof(histslow,NBINS_TOF,binslow,(1e-7,5e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_heatmap_rel_tof(histslow,NBINS_TOF,binslow,(1.1e-6,1.4e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75ef5b",
   "metadata": {},
   "source": [
    "## Presentation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "# Compute main plot data\n",
    "histselected, bin_edgesselected = np.histogram(ion_dfevent500.mq, bins=np.linspace(0,120,1000),range=(0,120))\n",
    "\n",
    "# Compute heatmap subplot data\n",
    "Y_ZONE = (145, 0, 10, 256, 7)\n",
    "fish_dfevent = fish_plot_ion_selection(calibrated_ion_dfevent,Y_ZONE)\n",
    "fish_dfevent_yfiltered = fish_dfevent[fish_dfevent.y > 32][fish_dfevent.y < 224]\n",
    "\n",
    "mq_bins = 241\n",
    "mq_bin_edges = np.linspace(0,120,mq_bins)\n",
    "fish_dfevent_yfiltered['mq_bin'] = pd.cut(fish_dfevent_yfiltered['mq'], bins=mq_bin_edges, labels=False)\n",
    "\n",
    "heatmap_data = fish_dfevent_yfiltered.pivot_table(index='y', columns='mq_bin', aggfunc='size', fill_value=0)\n",
    "\n",
    "max_value = heatmap_data.values.max()\n",
    "normalized_data = heatmap_data.values / max_value\n",
    "\n",
    "# Create a figure with gridspec\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 1], hspace=0.1)\n",
    "\n",
    "# Create main plot\n",
    "ax_main = plt.subplot(gs[0])\n",
    "x = bin_edgesselected[:-1]\n",
    "ax_main.plot(x, histselected/max(histselected), linewidth = 1, c='b')\n",
    "ax_main.set_ylabel('Normalized number of events')\n",
    "\n",
    "# Create heatmap subplot\n",
    "ax_heatmap = plt.subplot(gs[1], sharex=ax_main)\n",
    "cax = ax_heatmap.imshow(normalized_data, cmap='viridis', aspect='auto', norm=LogNorm(), extent=[x.min(), x.max(), 0, 1])\n",
    "ax_heatmap.set_xlabel('m/q')\n",
    "ax_heatmap.set_yticklabels(np.linspace(224,34,6,dtype=int))\n",
    "ax_heatmap.set_ylabel('y')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(cax, ax=ax_heatmap, label='Normalized number of events', norm=LogNorm(), orientation='horizontal', pad=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbb14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "calibrated_ion_dfevent = ion_dfevent500\n",
    "\n",
    "# Compute main plot data\n",
    "histselected, bin_edgesselected = np.histogram(calibrated_ion_dfevent.mq, bins=np.linspace(0,120,1000),range=(0,120))\n",
    "\n",
    "# Compute heatmap subplot data\n",
    "Y_ZONE = (145, 0, 10, 256, 7)\n",
    "fish_dfevent = fish_plot_ion_selection(calibrated_ion_dfevent,Y_ZONE)\n",
    "fish_dfevent_yfiltered = fish_dfevent[fish_dfevent.y > 32][fish_dfevent.y < 224]\n",
    "\n",
    "mq_bins = 241\n",
    "mq_bin_edges = np.linspace(0,120,mq_bins)\n",
    "fish_dfevent_yfiltered['mq_bin'] = pd.cut(fish_dfevent_yfiltered['mq'], bins=mq_bin_edges, labels=False)\n",
    "\n",
    "heatmap_data = fish_dfevent_yfiltered.pivot_table(index='y', columns='mq_bin', aggfunc='size', fill_value=0)\n",
    "\n",
    "max_value = heatmap_data.values.max()\n",
    "normalized_data = heatmap_data.values / max_value\n",
    "\n",
    "# Create a figure with gridspec\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 1], hspace=0.1)\n",
    "\n",
    "# Create main plot\n",
    "ax_main = plt.subplot(gs[0])\n",
    "x = bin_edgesselected[:-1]\n",
    "ax_main.plot(x, histselected/max(histselected), linewidth = 1, c='b')\n",
    "ax_main.set_ylabel('Normalized number of events')\n",
    "\n",
    "# Create heatmap subplot\n",
    "ax_heatmap = plt.subplot(gs[1], sharex=ax_main)\n",
    "cax = ax_heatmap.imshow(normalized_data, cmap='magma', aspect='auto', norm=LogNorm(), extent=[x.min(), x.max(), 0, 1])\n",
    "ax_heatmap.set_xlabel('m/q')\n",
    "ax_heatmap.set_yticklabels(np.linspace(224,34,6,dtype=int))\n",
    "ax_heatmap.set_ylabel('y')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(cax, ax=ax_heatmap, label='Normalized number of events', norm=LogNorm(), orientation='horizontal', pad=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bd3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(calibrated_selected_dfevent)\n",
    "\n",
    "LOWER_MQ = .5\n",
    "UPPER_MQ = 1.5\n",
    "mqselected_dfevent,mqselected_dfpulse,mqselected_etof = mq_selection(calibrated_selected_dfevent,selected_dfpulse,selected_etof,LOWER_MQ,UPPER_MQ)\n",
    "heatmap(mqselected_dfevent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_dfevent = ion_dfevent[ion_dfevent.tof < 40e-9][ion_dfevent.tof > 10e-9]\n",
    "heatmap(short_dfevent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d3d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_MQ = 5000\n",
    "NBINS_EVENTS = 300\n",
    "xlimits = (0,5)\n",
    "\n",
    "filtered_dfevents, filtered_dfpulses, filtered_etofs, hists, bins = nevents_binning(calibrated_selected_dfevent,selected_dfpulse,selected_etof,NBINS_EVENTS,NBINS_MQ)\n",
    "\n",
    "precision = 200/NBINS_MQ\n",
    "x_lower, x_upper = xlimits\n",
    "hist_lower, hist_upper = int(x_lower/precision), int(x_upper/precision)\n",
    "nbins = len(hists)-150\n",
    "hists_norm = []\n",
    "    \n",
    "for i in range(nbins):\n",
    "    \n",
    "    shortened_hist = hists[i][hist_lower:hist_upper]\n",
    "    hist_norm = shortened_hist / max(shortened_hist)\n",
    "    hists_norm.append(hist_norm)\n",
    "\n",
    "x_edges = np.linspace(x_lower, x_upper, hist_upper-hist_lower)\n",
    "y_edges = bins[:-1-150]\n",
    "X, Y = np.meshgrid(x_edges, y_edges)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "c = plt.pcolormesh(X, Y, hists_norm, shading='auto')\n",
    "plt.colorbar(c, label='Relative normalized counts', extend='max')\n",
    "plt.xlabel('m/q')\n",
    "plt.ylabel('Number of events slice')\n",
    "plt.title('Relative heatmap for number of events slices with respect to m/q')\n",
    "plt.xlim(x_lower, x_upper)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounded_calibrated_selected_dfevent = calibrated_selected_dfevent[calibrated_selected_dfevent.tof < TIME_BETWEEN_PULSES]\n",
    "hist, bin_edges = np.histogram(bounded_calibrated_selected_dfevent.tof, bins=1000)\n",
    "\n",
    "channel_time = TIME_BETWEEN_PULSES/CHANNELS_PER_PULSE\n",
    "    \n",
    "xaxis = np.arange(CHANNELS_PER_PULSE)*channel_time\n",
    "avg_selected_etof = -np.mean(selected_etof, axis=0)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(bin_edges[:-1], hist/max(hist))\n",
    "plt.plot(xaxis,avg_selected_etof/max(avg_selected_etof))\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.xlim(0,20e-7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54248e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounded_selected_dfevent = selected_dfevent[selected_dfevent.tof < TIME_BETWEEN_PULSES]\n",
    "hist, bin_edges = np.histogram(bounded_selected_dfevent.tof, bins=1000)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(bin_edges[:-1], hist/max(hist))\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.yscale('log')\n",
    "plt.xlim(0,1e-7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_LIMITS = (5e-9,16e-8)\n",
    "\n",
    "channel_time = TIME_BETWEEN_PULSES/CHANNELS_PER_PULSE\n",
    "\n",
    "lower_xlimit, upper_xlimit = X_LIMITS\n",
    "lower_int_xlimit, upper_int_xlimit = int(lower_xlimit/channel_time), int(upper_xlimit/channel_time)\n",
    "shortened_etof = selected_etof[:,lower_int_xlimit:upper_int_xlimit]\n",
    "shortened_backgrd_etof = backgrd_etof[:,lower_int_xlimit:upper_int_xlimit]\n",
    "    \n",
    "xaxis = np.arange(lower_int_xlimit,upper_int_xlimit)*channel_time\n",
    "avg_selected_etof = -np.mean(shortened_etof, axis=0)\n",
    "avg_backgrd_etof = -np.mean(shortened_backgrd_etof, axis=0)\n",
    "max_selected_etof = max(avg_selected_etof)\n",
    "max_backgrd_etof = max(avg_backgrd_etof)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xaxis,avg_selected_etof/max_selected_etof,c='r')\n",
    "plt.plot(xaxis,avg_backgrd_etof/max_backgrd_etof,c='g')\n",
    "plt.xlabel('Time of flight (s)')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.title('Electrons time of flight')\n",
    "plt.xlim(lower_xlimit, upper_xlimit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed9859",
   "metadata": {},
   "source": [
    "## Covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfe0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MQ_BINS = 400\n",
    "\n",
    "mq_np_covariance(calibrated_ion_dfevent,MQ_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = False\n",
    "VMIN = -1\n",
    "VMAX = 10\n",
    "    \n",
    "mq_np_covariance(calibrated_selected_dfevent,MQ_BINS,LOG,VMIN,VMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1be399",
   "metadata": {},
   "outputs": [],
   "source": [
    "MQ_BINS = 400\n",
    "MQ_BIN_RANGE = (0,3)\n",
    "\n",
    "mq_covariance_1d(calibrated_selected_dfevent,MQ_BIN_RANGE,MQ_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a54591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mq_covariance_1d(dfevent,mq_bin_range,mq_bins=200):\n",
    "    'Produces 1d covariance of an m/q bin range vs m/q employing the numpy cov function'\n",
    "    'Uses dfevent as input, can select number of mq bins'\n",
    "    \n",
    "    mq_bin_edges = np.linspace(0,200,mq_bins+1)\n",
    "\n",
    "    dfevent['mq_bin'] = pd.cut(dfevent['mq'], bins=mq_bin_edges)\n",
    "\n",
    "    result_matrix = pd.crosstab(dfevent['pulseId'], dfevent['mq_bin'])\n",
    "    result_numpy_matrix = result_matrix.values\n",
    "\n",
    "    array_hyd = result_matrix.values[:,mq_bin_range[0]:mq_bin_range[1]].sum(axis=1)\n",
    "    shape = result_numpy_matrix.shape[1]\n",
    "\n",
    "    # Calculate the covariance between each row of result_numpy_matrix and array_hyd\n",
    "    covariances = np.array([np.cov(np.column_stack((result_numpy_matrix[:, j], array_hyd)), rowvar=False)[0, 1] for j in range(shape)])\n",
    "\n",
    "    xaxis = np.linspace(0,200,shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xaxis,covariances)\n",
    "    plt.xlabel('m/q')\n",
    "    plt.ylabel('Covariance with m/q')\n",
    "    plt.title(f'Covariance Plot between m/q bin range {mq_bin_range} and m/q')\n",
    "    ax.set_yscale('symlog', linthresh=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0679a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tof_covariance_1d(dfevent, tof_range, nbins_ion_tof=200, max_ion_limit=TIME_BETWEEN_PULSES):\n",
    "    '''Produces 1d covariance of TOF range vs TOF using numpy cov function'''\n",
    "    \n",
    "    # Create TOF bins\n",
    "    tof_bin_edges = np.linspace(0, max_ion_limit, nbins_ion_tof+1)\n",
    "    dfevent['tof_bin'] = pd.cut(dfevent['tof'], bins=tof_bin_edges)\n",
    "\n",
    "    # Convert time values to bin indices\n",
    "    start_bin = np.digitize(tof_range[0], tof_bin_edges) - 1\n",
    "    end_bin = np.digitize(tof_range[1], tof_bin_edges) - 1\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    start_bin = max(0, start_bin)\n",
    "    end_bin = min(nbins_ion_tof, end_bin)\n",
    "\n",
    "    # Create pulseId vs TOF bin matrix\n",
    "    result_matrix = pd.crosstab(dfevent['pulseId'], dfevent['tof_bin'])\n",
    "    result_numpy_matrix = result_matrix.values\n",
    "\n",
    "    # Sum over specified TOF bin range\n",
    "    array_hyd = result_matrix.values[:, start_bin:end_bin].sum(axis=1)\n",
    "    shape = result_numpy_matrix.shape[1]\n",
    "\n",
    "    # Calculate covariance with each TOF bin\n",
    "    covariances = np.array([np.cov(np.column_stack((result_numpy_matrix[:, j], array_hyd)), rowvar=False)[0, 1] for j in range(shape)])\n",
    "\n",
    "    # Plot configuration\n",
    "    xaxis = np.linspace(0, max_ion_limit, shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xaxis, covariances)\n",
    "    plt.xlabel('TOF')\n",
    "    plt.ylabel('Covariance with TOF')\n",
    "    plt.title(f'Covariance Plot between TOF range {tof_range} and TOF')\n",
    "    ax.set_yscale('symlog', linthresh=10)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOF_BINS = 400\n",
    "TOF_RANGE = (3.8e-7, 4.2e-7)\n",
    "\n",
    "tof_covariance_1d(ion_dfevent500,TOF_RANGE,TOF_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961342f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_tof(ion_dfevent500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0df869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unclear when this is needed (for runs [389,390] it is; for runs [375 to 404] it is not)\n",
    "new_calibrated_selected_dfevent = fix_missing_row(calibrated_selected_dfevent,selected_dfpulse,MQ_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mq_covariance(new_calibrated_selected_dfevent,selected_dfpulse,MQ_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a596d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = False\n",
    "VMIN = -1\n",
    "VMAX = 1\n",
    "    \n",
    "mq_covariance(new_calibrated_selected_dfevent,selected_dfpulse,MQ_BINS,LOG,VMIN,VMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe7232",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ALPHA in np.arange(1,1.51,.1):\n",
    "    print('alpha = ', ALPHA)\n",
    "    mq_partial_covariance(new_calibrated_selected_dfevent,selected_dfpulse,MQ_BINS,ALPHA,LOG,VMIN,VMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_ION_TOF = 300\n",
    "MAX_ION_LIMIT = 1e-6\n",
    "ALPHA = 1\n",
    "LOG = False\n",
    "VMIN = -1e5\n",
    "VMAX = 1e5\n",
    "\n",
    "ion_ion_covariance(ion_dfevent500,ion_dfpulse500,NBINS_ION_TOF,MAX_ION_LIMIT,ALPHA,LOG,VMIN,VMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_ION_TOF = 500\n",
    "MAX_ION_LIMIT = 5e-7\n",
    "ALPHA = 1\n",
    "LOG = True\n",
    "VMIN = 0\n",
    "VMAX = 5\n",
    "\n",
    "ion_ion_covariance(ion_dfevent500,ion_dfpulse500,NBINS_ION_TOF,MAX_ION_LIMIT,ALPHA,LOG,VMIN,VMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2220107",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_ION_TOF = 500\n",
    "MAX_ION_LIMIT = 3e-6\n",
    "ALPHA = 1\n",
    "LOG = False\n",
    "VMIN = -10\n",
    "VMAX = 10\n",
    "\n",
    "ion_ion_covariance(ion_dfevent500,ion_dfpulse500,NBINS_ION_TOF,MAX_ION_LIMIT,ALPHA,LOG,VMIN,VMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234352f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_ION_TOF = 500\n",
    "MAX_ION_LIMIT = 2e-6\n",
    "ALPHA = 1\n",
    "LOG = False\n",
    "VMIN = -10\n",
    "VMAX = 10\n",
    "\n",
    "ion_ion_covariance(ion_dfevent,ion_dfpulse,NBINS_ION_TOF,MAX_ION_LIMIT,ALPHA,LOG,VMIN,VMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc360b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_ION_TOF = 150\n",
    "NBINS_E_TOF = 100\n",
    "MAX_ION_LIMIT = 3e-6\n",
    "MAX_E_LIMIT = 3e-7\n",
    "ALPHA = 1\n",
    "LOG = True\n",
    "VMIN = -1e5\n",
    "VMAX = 1e5\n",
    "\n",
    "etof_ion_covariance(dfevent1,dfpulse1,etof1,NBINS_ION_TOF,NBINS_E_TOF,MAX_ION_LIMIT,MAX_E_LIMIT,ALPHA,LOG)#,VMIN,VMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977beb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_ION_TOF = 150\n",
    "NBINS_E_TOF = 100\n",
    "MAX_ION_LIMIT = 3e-6\n",
    "MAX_E_LIMIT = 3e-7\n",
    "ALPHA = 1\n",
    "LOG = True\n",
    "VMIN = -1e5\n",
    "VMAX = 1e5\n",
    "\n",
    "etof_ion_covariance(dfevent2,dfpulse2,etof2,NBINS_ION_TOF,NBINS_E_TOF,MAX_ION_LIMIT,MAX_E_LIMIT,ALPHA,LOG)#,VMIN,VMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a64b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_ION_TOF = 150\n",
    "NBINS_E_TOF = 100\n",
    "MAX_ION_LIMIT = 3e-6\n",
    "MAX_E_LIMIT = 3e-7\n",
    "ALPHA = 1\n",
    "LOG = True\n",
    "VMIN = -1e5\n",
    "VMAX = 1e5\n",
    "\n",
    "etof_ion_covariance(dfevent3,dfpulse3,etof3,NBINS_ION_TOF,NBINS_E_TOF,MAX_ION_LIMIT,MAX_E_LIMIT,ALPHA,LOG)#,VMIN,VMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_ION_TOF = 150\n",
    "NBINS_E_TOF = 100\n",
    "MAX_ION_LIMIT = 3e-6\n",
    "MAX_E_LIMIT = 3e-7\n",
    "ALPHA = 1\n",
    "LOG = True\n",
    "VMIN = -1e2\n",
    "VMAX = 1e2\n",
    "\n",
    "etof_ion_covariance(selected_dfevent,selected_dfpulse,selected_etof,NBINS_ION_TOF,NBINS_E_TOF,MAX_ION_LIMIT,MAX_E_LIMIT,ALPHA,LOG)#,VMIN,VMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eba6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etof_ion_covariance_1d(dfevent, etof, etof_range, nbins_ion_tof=200, max_ion_limit=TIME_BETWEEN_PULSES):\n",
    "    'Produces 1D covariance between selected etof range and ion TOF'\n",
    "    \n",
    "    # Create ion TOF bins\n",
    "    ion_tof_bin_edges = np.linspace(0, max_ion_limit, nbins_ion_tof+1)\n",
    "    dfevent['tof_bin'] = pd.cut(dfevent['tof'], bins=ion_tof_bin_edges)\n",
    "    ion_matrix = pd.crosstab(dfevent['pulseId'], dfevent['tof_bin'])\n",
    "    \n",
    "    # Process etof data\n",
    "    coords_etof = etof.assign_coords(data=np.arange(0, TIME_BETWEEN_PULSES, channel_time))\n",
    "    e_tof_bin_edges = np.linspace(0, max_ion_limit, nbins_ion_tof+1)\n",
    "    binned_etof = coords_etof.groupby_bins(\"data\", e_tof_bin_edges).sum()\n",
    "    e_matrix = binned_etof.to_pandas()\n",
    "    \n",
    "    # Align matrices\n",
    "    ion_list = ion_matrix.index.to_list()\n",
    "    # ion_list.append(0)\n",
    "    e_list = e_matrix.index.to_list()\n",
    "    truefalse = np.equal(ion_list, e_list)\n",
    "    first_instance = np.argmax(~truefalse)\n",
    "    # e_matrix = e_matrix.drop(e_list[first_instance])\n",
    "    \n",
    "    # Extract etof range of interest\n",
    "    etof_range_idx = (e_matrix.columns >= etof_range[0]) & (e_matrix.columns <= etof_range[1])\n",
    "    etof_sum = e_matrix.iloc[:, etof_range_idx].sum(axis=1)\n",
    "    \n",
    "    # Calculate covariances\n",
    "    covariances = np.array([\n",
    "        np.cov(np.column_stack((ion_matrix.values[:, j], etof_sum)), rowvar=False)[0, 1] \n",
    "        for j in range(ion_matrix.shape[1])\n",
    "    ])\n",
    "    \n",
    "    # Plot results\n",
    "    xaxis = np.linspace(0, max_ion_limit, len(covariances))\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(xaxis, covariances)\n",
    "    plt.xlabel('Ion Time of Flight (s)')\n",
    "    plt.ylabel('Covariance with eTOF')\n",
    "    plt.title(f'Covariance between eTOF range {etof_range[0]:.2e}-{etof_range[1]:.2e} s and Ion TOF')\n",
    "    \n",
    "    # Add vertical lines at key positions\n",
    "    plt.axvline(3.7e-7, c='r', linestyle='--')\n",
    "    plt.axvline(4.1e-7, c='r', linestyle='--')\n",
    "    plt.axvline(4.5e-7, c='r', linestyle='--')\n",
    "    \n",
    "    ax.set_yscale('symlog', linthresh=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d997a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etof_ion_covariance_1d(dfevent, etof, etof_range, nbins_ion_tof=200, max_ion_limit=TIME_BETWEEN_PULSES):\n",
    "    'Produces 1D covariance between selected etof range and ion TOF'\n",
    "    \n",
    "    # Create ion TOF bins\n",
    "    ion_tof_bin_edges = np.linspace(0, max_ion_limit, nbins_ion_tof+1)\n",
    "    dfevent['tof_bin'] = pd.cut(dfevent['tof'], bins=ion_tof_bin_edges)\n",
    "    ion_matrix = pd.crosstab(dfevent['pulseId'], dfevent['tof_bin'])\n",
    "    \n",
    "    # Process etof data\n",
    "    coords_etof = etof.assign_coords(data=np.arange(0, TIME_BETWEEN_PULSES, channel_time))\n",
    "    e_tof_bin_edges = np.linspace(0, max_ion_limit, nbins_ion_tof+1)\n",
    "    binned_etof = coords_etof.groupby_bins(\"data\", e_tof_bin_edges).sum()\n",
    "    e_matrix = binned_etof.to_pandas()\n",
    "    \n",
    "    # Align matrices\n",
    "    ion_list = ion_matrix.index.to_list()\n",
    "    # ion_list.append(0)\n",
    "    e_list = e_matrix.index.to_list()\n",
    "    truefalse = np.equal(ion_list, e_list)\n",
    "    first_instance = np.argmax(~truefalse)\n",
    "    # e_matrix = e_matrix.drop(e_list[first_instance])\n",
    "    \n",
    "    # Extract etof range of interest using the bin midpoints\n",
    "    bin_midpoints = np.array([interval.mid for interval in e_matrix.columns])\n",
    "    etof_range_idx = (bin_midpoints >= etof_range[0]) & (bin_midpoints <= etof_range[1])\n",
    "    etof_sum = e_matrix.iloc[:, etof_range_idx].sum(axis=1)\n",
    "    \n",
    "    # Calculate covariances\n",
    "    covariances = np.array([\n",
    "        np.cov(np.column_stack((ion_matrix.values[:, j], etof_sum)), rowvar=False)[0, 1] \n",
    "        for j in range(ion_matrix.shape[1])\n",
    "    ])\n",
    "    \n",
    "    # Plot results\n",
    "    xaxis = np.linspace(0, max_ion_limit, len(covariances))\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(xaxis, covariances)\n",
    "    plt.xlabel('Ion Time of Flight (s)')\n",
    "    plt.ylabel('Covariance with eTOF')\n",
    "    plt.title(f'Covariance between eTOF range {etof_range[0]:.2e}-{etof_range[1]:.2e} s and Ion TOF')\n",
    "    \n",
    "    plt.axvline(3.7e-7, c='r', linestyle='--')\n",
    "    plt.axvline(4.1e-7, c='r', linestyle='--')\n",
    "    plt.axvline(4.5e-7, c='r', linestyle='--')\n",
    "    \n",
    "    ax.set_yscale('symlog', linthresh=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22012479",
   "metadata": {},
   "outputs": [],
   "source": [
    "etof_range = (1.2e-7, 2e-7)\n",
    "etof_ion_covariance_1d(selected_dfevent, selected_etof, etof_range, nbins_ion_tof=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0210a57c",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7aeeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_curve, auc\n",
    "import cv2\n",
    "\n",
    "# Function to load and preprocess data from xarray\n",
    "def load_and_preprocess_data(pnccd_array, label_array, test_size=0.2, random_state=42):\n",
    "    \"\"\"Load and preprocess the xarray data arrays.\"\"\"\n",
    "    # Convert xarray to numpy\n",
    "    images = pnccd_array.values\n",
    "    \n",
    "    # Split data into training and testing sets with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        images, labels, test_size=test_size, random_state=random_state, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Normalize the images to [0, 1] range\n",
    "    X_train = X_train.astype('float32') / np.max(X_train)\n",
    "    X_test = X_test.astype('float32') / np.max(X_test)\n",
    "    \n",
    "    # Reshape to add channel dimension\n",
    "    if X_train.ndim == 3:\n",
    "        X_train = X_train[..., np.newaxis]\n",
    "        X_test = X_test[..., np.newaxis]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Convert images to polar coordinates for better feature extraction\n",
    "def to_polar(image, center=None):\n",
    "    \"\"\"Transform image to polar coordinates.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    if center is None:\n",
    "        center = (h // 2, w // 2)\n",
    "        \n",
    "    # Calculate maximum radius\n",
    "    max_radius = np.sqrt((h - center[0])**2 + (w - center[1])**2)\n",
    "    \n",
    "    # Define output polar image dimensions (radius, angle)\n",
    "    polar_h = int(max_radius)\n",
    "    polar_w = 360\n",
    "    \n",
    "    # Create mapping matrix\n",
    "    mapping = np.zeros((polar_h, polar_w, 2), dtype=np.float32)\n",
    "    \n",
    "    # Fill mapping matrix\n",
    "    for r in range(polar_h):\n",
    "        for theta in range(polar_w):\n",
    "            # Convert to radians\n",
    "            theta_rad = np.deg2rad(theta)\n",
    "            \n",
    "            # Calculate corresponding cartesian coordinates\n",
    "            x = center[0] + r * np.cos(theta_rad)\n",
    "            y = center[1] + r * np.sin(theta_rad)\n",
    "            \n",
    "            mapping[r, theta] = [y, x]\n",
    "    \n",
    "    # Perform the remapping\n",
    "    polar = cv2.remap(image, mapping, None, cv2.INTER_LINEAR)\n",
    "    \n",
    "    return polar\n",
    "\n",
    "# Data generator with balanced sampling and augmentation\n",
    "class BalancedDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Data generator that provides balanced batches and applies augmentation.\"\"\"\n",
    "    def __init__(self, X, y, batch_size=32, augment_minority=True, use_polar=False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.augment_minority = augment_minority\n",
    "        self.use_polar = use_polar\n",
    "        \n",
    "        # Find indices for each class\n",
    "        self.majority_indices = np.where(y == 0)[0]\n",
    "        self.minority_indices = np.where(y == 1)[0]\n",
    "        \n",
    "        # Calculate samples per batch\n",
    "        self.n_majority = self.batch_size // 2\n",
    "        self.n_minority = self.batch_size - self.n_majority\n",
    "        \n",
    "        # Data augmentation\n",
    "        self.datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rotation_range=360,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches per epoch.\"\"\"\n",
    "        return int(np.ceil(len(self.minority_indices) * 10 / self.n_minority))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data.\"\"\"\n",
    "        # Generate batch indices with balanced classes\n",
    "        majority_batch_indices = np.random.choice(\n",
    "            self.majority_indices, self.n_majority, replace=False\n",
    "        )\n",
    "        minority_batch_indices = np.random.choice(\n",
    "            self.minority_indices, self.n_minority, replace=True\n",
    "        )\n",
    "        batch_indices = np.concatenate([majority_batch_indices, minority_batch_indices])\n",
    "        np.random.shuffle(batch_indices)\n",
    "        \n",
    "        # Generate data\n",
    "        X_batch = np.array([self.X[i].copy() for i in batch_indices])\n",
    "        y_batch = np.array([self.y[i] for i in batch_indices])\n",
    "        \n",
    "        # Apply transformations\n",
    "        for i, (x, y) in enumerate(zip(X_batch, y_batch)):\n",
    "            # Apply polar transformation if requested\n",
    "            if self.use_polar and np.random.random() > 0.5:\n",
    "                if x.ndim == 3:\n",
    "                    polar_image = to_polar(x[:,:,0])\n",
    "                    X_batch[i] = polar_image[..., np.newaxis]\n",
    "                else:\n",
    "                    X_batch[i] = to_polar(x)\n",
    "            \n",
    "            # Apply augmentation to minority class\n",
    "            if self.augment_minority and y == 1:\n",
    "                X_batch[i] = self.datagen.random_transform(x)\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffle indices after each epoch.\"\"\"\n",
    "        np.random.shuffle(self.majority_indices)\n",
    "        np.random.shuffle(self.minority_indices)\n",
    "\n",
    "# Build a CNN model specifically for diffraction pattern classification\n",
    "def build_model(input_shape, filters_start=32):\n",
    "    \"\"\"Build a CNN model designed for diffraction pattern classification.\"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First convolutional block\n",
    "    model.add(layers.Conv2D(filters_start, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(filters_start, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    # Second convolutional block\n",
    "    model.add(layers.Conv2D(filters_start*2, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(filters_start*2, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    # Third convolutional block\n",
    "    model.add(layers.Conv2D(filters_start*4, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    # Global pooling and dense layers\n",
    "    model.add(layers.GlobalAveragePooling2D())  # Good for rotational invariance\n",
    "    model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    \n",
    "    # Compile model with weighted binary crossentropy\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to train model with class balancing\n",
    "def train_model(X_train, y_train, X_val, y_val, model, batch_size=32, epochs=50, use_generator=True, use_polar=True):\n",
    "    \"\"\"Train the model with balanced sampling and appropriate callbacks.\"\"\"\n",
    "    # Calculate class weights to handle imbalance\n",
    "    n_samples = len(y_train)\n",
    "    n_monomers = np.sum(y_train == 0)\n",
    "    n_dimers = np.sum(y_train == 1)\n",
    "    class_weight = {\n",
    "        0: n_samples / (2 * n_monomers),\n",
    "        1: n_samples / (2 * n_dimers)\n",
    "    }\n",
    "    \n",
    "    # Callbacks for training\n",
    "    callbacks_list = [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_precision',\n",
    "            patience=10,\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath='best_diffraction_model.h5',\n",
    "            monitor='val_precision',\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train with generator for balanced batches\n",
    "    if use_generator:\n",
    "        train_gen = BalancedDataGenerator(\n",
    "            X_train, y_train, batch_size=batch_size, \n",
    "            augment_minority=True, use_polar=use_polar\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks_list,\n",
    "            class_weight=class_weight\n",
    "        )\n",
    "    else:\n",
    "        # Standard training with class weights\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks_list,\n",
    "            class_weight=class_weight\n",
    "        )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Function to evaluate model and find optimal threshold\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model and determine optimal classification threshold.\"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_prob = model.predict(X_test).flatten()\n",
    "    \n",
    "    # Find optimal threshold using precision-recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "    f1_scores = 2 * recall * precision / (recall + precision + 1e-10)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    # Apply threshold\n",
    "    y_pred = (y_pred_prob >= optimal_threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Collect metrics\n",
    "    metrics = {\n",
    "        'report': report,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'roc_auc': roc_auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred\n",
    "\n",
    "# Main function to run the entire pipeline\n",
    "def main(pnccd_array, label_array):\n",
    "    \"\"\"Complete pipeline for training and evaluating the model.\"\"\"\n",
    "    # Load and preprocess data\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data(\n",
    "        pnccd_array, label_array\n",
    "    )\n",
    "    \n",
    "    # Further split training data for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    # Print dataset information\n",
    "    print(f\"Dataset shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    print(f\"Class distribution:\")\n",
    "    print(f\"Training: Monomers = {np.sum(y_train == 0)}, Dimers = {np.sum(y_train == 1)}\")\n",
    "    \n",
    "    # Build the model\n",
    "    model = build_model(X_train.shape[1:])\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    model, history = train_model(\n",
    "        X_train, y_train, X_val, y_val, model,\n",
    "        batch_size=32, epochs=50, use_generator=True, use_polar=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics, y_pred = evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "    return model, history, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aee5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example testing code\n",
    "def test_model(model, pnccd_array, label_array):\n",
    "    # Select test samples by train ID\n",
    "    test_ids = np.random.choice(pnccd_array.trainId.values, 100, replace=False)\n",
    "    \n",
    "    # Extract test data\n",
    "    test_data = pnccd_array.sel(trainId=test_ids).values\n",
    "    test_labels = label_array.sel(trainId=test_ids).values  # Convert to 0-based\n",
    "    \n",
    "    # Preprocess\n",
    "    test_data = test_data.astype('float32') / np.max(test_data)\n",
    "    test_data = test_data[..., np.newaxis]  # Add channel dimension\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(test_data)\n",
    "    \n",
    "    # Get classification using optimal threshold (from previous evaluation)\n",
    "    y_pred = (predictions >= 0.5).astype(int).flatten()\n",
    "\n",
    "    cm = confusion_matrix(test_labels, y_pred)\n",
    "    sns.heatmap(cm, annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate metrics\n",
    "    print(classification_report(test_labels, y_pred))\n",
    "    \n",
    "    return test_data, test_labels, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ef1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_array = actual_result.dropna(dim='trainId')\n",
    "\n",
    "trainIds = np.array(cleaned_array.trainId)\n",
    "label_array = np.array(cleaned_array, dtype=int)\n",
    "\n",
    "pnccd_array = high_fluence_pnccd[high_fluence_pnccd.trainId.isin(trainIds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "import cv2\n",
    "\n",
    "def physically_meaningful_augmentation(image, n_augmentations=4):\n",
    "    \"\"\"\n",
    "    Apply physically meaningful augmentations to diffraction patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : numpy array\n",
    "        Original diffraction pattern image\n",
    "    n_augmentations : int\n",
    "        Number of augmented versions to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list of numpy arrays\n",
    "        Augmented images\n",
    "    \"\"\"\n",
    "    # Ensure image is 2D\n",
    "    if image.ndim == 3:\n",
    "        image = image.squeeze()\n",
    "    \n",
    "    # Extract image properties\n",
    "    height, width = image.shape\n",
    "    center_y, center_x = height // 2, width // 2\n",
    "    \n",
    "    augmented_images = []\n",
    "    \n",
    "    for i in range(n_augmentations):\n",
    "        # Start with a copy of the original\n",
    "        aug_img = image.copy()\n",
    "        \n",
    "        # 1. Rotation - physically meaningful for diffraction patterns\n",
    "        # Rotate around the center of the diffraction pattern\n",
    "        rotation_angle = np.random.uniform(0, 360)\n",
    "        aug_img = transform.rotate(\n",
    "            aug_img, \n",
    "            rotation_angle, \n",
    "            center=(center_x, center_y),\n",
    "            mode='reflect', \n",
    "            preserve_range=True\n",
    "        )\n",
    "        \n",
    "        # 2. Minor zoom - simulates slight changes in detector distance\n",
    "        # Use very conservative zoom to maintain pattern integrity\n",
    "        if np.random.random() > 0.5:\n",
    "            zoom_factor = np.random.uniform(0.95, 1.05)\n",
    "            aug_img = transform.rescale(\n",
    "                aug_img, \n",
    "                zoom_factor, \n",
    "                anti_aliasing=True,\n",
    "                preserve_range=True\n",
    "            )\n",
    "            \n",
    "            # Crop or pad to maintain original size\n",
    "            if zoom_factor > 1:\n",
    "                # Crop\n",
    "                new_h, new_w = aug_img.shape\n",
    "                start_h = (new_h - height) // 2\n",
    "                start_w = (new_w - width) // 2\n",
    "                aug_img = aug_img[start_h:start_h+height, start_w:start_w+width]\n",
    "            else:\n",
    "                # Pad\n",
    "                new_h, new_w = aug_img.shape\n",
    "                pad_h = (height - new_h) // 2\n",
    "                pad_w = (width - new_w) // 2\n",
    "                aug_img = np.pad(\n",
    "                    aug_img, \n",
    "                    ((pad_h, height - new_h - pad_h), \n",
    "                     (pad_w, width - new_w - pad_w)), \n",
    "                    mode='constant'\n",
    "                )\n",
    "        \n",
    "        # 3. Minor intensity variations - simulates exposure differences\n",
    "        if np.random.random() > 0.5:\n",
    "            intensity_factor = np.random.uniform(0.9, 1.1)\n",
    "            aug_img = aug_img * intensity_factor\n",
    "            \n",
    "            # Ensure we don't exceed original range\n",
    "            aug_img = np.clip(aug_img, 0, np.max(image))\n",
    "        \n",
    "        # 4. Minor noise addition - simulates detector noise\n",
    "        if np.random.random() > 0.5:\n",
    "            noise_level = np.random.uniform(0, 0.03 * np.max(image))\n",
    "            noise = np.random.normal(0, noise_level, aug_img.shape)\n",
    "            aug_img = aug_img + noise\n",
    "            \n",
    "            # Ensure we stay in valid range\n",
    "            aug_img = np.clip(aug_img, 0, np.max(image))\n",
    "            \n",
    "        augmented_images.append(aug_img)\n",
    "    \n",
    "    return augmented_images\n",
    "\n",
    "def to_polar(image):\n",
    "    \"\"\"\n",
    "    Transform image to polar coordinates centered at the diffraction pattern center.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : numpy array\n",
    "        Original diffraction pattern image\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy array\n",
    "        Polar coordinate representation\n",
    "    \"\"\"\n",
    "    # Ensure image is 2D\n",
    "    if image.ndim == 3:\n",
    "        image = image.squeeze()\n",
    "        \n",
    "    height, width = image.shape\n",
    "    \n",
    "    # For diffraction patterns, center is typically in the middle\n",
    "    center = (height // 2, width // 2)\n",
    "    \n",
    "    # Calculate maximum radius\n",
    "    max_radius = min(center[0], center[1], height - center[0], width - center[1])\n",
    "    \n",
    "    # Define output polar image dimensions (radius, angle)\n",
    "    polar_height = max_radius\n",
    "    polar_width = 360  # Full circle in degrees\n",
    "    \n",
    "    # Create coordinate mapping\n",
    "    y, x = np.indices((height, width))\n",
    "    \n",
    "    # Calculate r and theta for each pixel\n",
    "    r = np.sqrt((y - center[0])**2 + (x - center[1])**2)\n",
    "    theta = np.arctan2(y - center[0], x - center[1]) * 180 / np.pi\n",
    "    \n",
    "    # Normalize radius to range [0, 1]\n",
    "    r_normalized = r / max_radius\n",
    "    \n",
    "    # Shift theta to range [0, 360]\n",
    "    theta = (theta + 360) % 360\n",
    "    \n",
    "    # Map to output coordinates\n",
    "    r_idx = np.clip(r, 0, max_radius-1).astype(int)\n",
    "    theta_idx = (theta * polar_width / 360).astype(int) % polar_width\n",
    "    \n",
    "    # Create output polar image\n",
    "    polar = np.zeros((polar_height, polar_width))\n",
    "    \n",
    "    # For each pixel in polar space, find the corresponding pixel in the image\n",
    "    for i in range(polar_height):\n",
    "        for j in range(polar_width):\n",
    "            # Convert polar to Cartesian\n",
    "            theta_rad = j * 2 * np.pi / polar_width\n",
    "            r_val = i\n",
    "            \n",
    "            y_val = int(center[0] + r_val * np.sin(theta_rad))\n",
    "            x_val = int(center[1] + r_val * np.cos(theta_rad))\n",
    "            \n",
    "            # Check if within image bounds\n",
    "            if 0 <= y_val < height and 0 <= x_val < width:\n",
    "                polar[i, j] = image[y_val, x_val]\n",
    "    \n",
    "    return polar\n",
    "\n",
    "# Function to visualize original and augmented images\n",
    "def visualize_augmentations(original_image, n_augmentations=4):\n",
    "    \"\"\"\n",
    "    Generate and visualize physically meaningful augmentations for diffraction patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_image : numpy array\n",
    "        Original diffraction pattern image\n",
    "    n_augmentations : int\n",
    "        Number of augmented versions to generate\n",
    "    \"\"\"\n",
    "    # Generate augmentations\n",
    "    augmented_images = physically_meaningful_augmentation(original_image, n_augmentations)\n",
    "    \n",
    "    # Create figure for visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Display original image\n",
    "    plt.subplot(1, n_augmentations + 1, 1)\n",
    "    plt.imshow(original_image, cmap='viridis')\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display augmented images\n",
    "    for i, aug_img in enumerate(augmented_images):\n",
    "        plt.subplot(1, n_augmentations + 1, i + 2)\n",
    "        plt.imshow(aug_img, cmap='viridis')\n",
    "        plt.title(f\"Aug #{i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize polar transforms\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Display original polar transform\n",
    "    original_polar = to_polar(original_image)\n",
    "    plt.subplot(1, n_augmentations + 1, 1)\n",
    "    plt.imshow(original_polar, cmap='viridis')\n",
    "    plt.title(\"Original (Polar)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display augmented polar transforms\n",
    "    for i, aug_img in enumerate(augmented_images):\n",
    "        aug_polar = to_polar(aug_img)\n",
    "        plt.subplot(1, n_augmentations + 1, i + 2)\n",
    "        plt.imshow(aug_polar, cmap='viridis')\n",
    "        plt.title(f\"Aug #{i+1} (Polar)\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnccd_np_array = np.array(pnccd_array)\n",
    "visualize_augmentations(pnccd_np_array[0], n_augmentations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history, metrics = main(pnccd_array, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call this function with your trained model\n",
    "test_data, test_labels, predictions = test_model(model, pnccd_array, cleaned_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c07fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_loading(pnccd_array, label_array):\n",
    "    \"\"\"Test that data loads correctly and matches expected shapes and label values.\"\"\"\n",
    "    assert pnccd_array.shape[0] == label_array.shape[0], \"Mismatch in number of images and labels\"\n",
    "    assert set(np.unique(label_array.values)).issubset({0, 1}), \"Labels should be 1 (monomer) or 2 (dimer)\"\n",
    "    print(\"Data loading test passed.\")\n",
    "\n",
    "def test_class_distribution(label_array):\n",
    "    \"\"\"Print and check class distribution.\"\"\"\n",
    "    unique, counts = np.unique(label_array.values, return_counts=True)\n",
    "    print(\"Class distribution:\")\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"Label {u}: {c} samples\")\n",
    "    assert min(counts) > 0, \"One of the classes has zero samples.\"\n",
    "    print(\"Class distribution test passed.\")\n",
    "\n",
    "def test_model_architecture(model, input_shape):\n",
    "    \"\"\"Check if the model can process a batch and output correct shape.\"\"\"\n",
    "    dummy_input = np.random.rand(2, *input_shape).astype(np.float32)\n",
    "    output = model.predict(dummy_input)\n",
    "    assert output.shape == (2, 1), \"Model output shape mismatch.\"\n",
    "    print(\"Model architecture test passed.\")\n",
    "\n",
    "def test_hyperparameter_grid(X_train, y_train, X_val, y_val, build_model_fn, param_grid):\n",
    "    \"\"\"Test different hyperparameters and print validation accuracy.\"\"\"\n",
    "    results = []\n",
    "    for filters in param_grid['filters_start']:\n",
    "        model = build_model_fn(X_train.shape[1:], filters_start=filters)\n",
    "        model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
    "        val_acc = model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "        results.append((filters, val_acc))\n",
    "        print(f\"Filters: {filters}, Val Accuracy: {val_acc:.3f}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c42e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance_metrics(model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"Print classification report and confusion matrix.\"\"\"\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    y_pred_prob = model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(test_data)\n",
    "plt.figure(figsize=(20,int(3*n/5)))\n",
    "for i in range(n):\n",
    "    plt.subplot(int(n/5),5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(test_data[i])\n",
    "    plt.xlabel(f'actual = {test_labels[i]}')\n",
    "    plt.xlabel(f'predicted = {predictions[i]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55925c",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.fftpack import fft2, fftshift\n",
    "import xarray as xr\n",
    "from skimage.transform import warp_polar\n",
    "\n",
    "da = high_fluence_pnccd\n",
    "\n",
    "n = 20  # Number of random images to plot\n",
    "random_indices = np.random.choice(da.shape[0], n, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(n, 5, figsize=(20, 3 * n))\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    image = da[idx].values\n",
    "    \n",
    "    # Original image\n",
    "    axes[i, 0].imshow(image, cmap='viridis')\n",
    "    axes[i, 0].set_title(f\"Original Image {idx}\")\n",
    "\n",
    "    # Greyscale version (already greyscale)\n",
    "    axes[i, 1].imshow(image, cmap='gray')\n",
    "    axes[i, 1].set_title(\"Greyscale Image\")\n",
    "\n",
    "    # Convolution version\n",
    "    kernel = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])\n",
    "    convolved_image = convolve2d(image, kernel, mode='same', boundary='wrap')\n",
    "    axes[i, 2].imshow(convolved_image, cmap='gray')\n",
    "    axes[i, 2].set_title(\"Convolution Image\")\n",
    "\n",
    "    # Fourier transform\n",
    "    f_transform = np.log(np.abs(fftshift(fft2(image))) + 1)\n",
    "    axes[i, 3].imshow(f_transform, cmap='gray')\n",
    "    axes[i, 3].set_title(\"Fourier Transform\")\n",
    "\n",
    "    # Cartesian to polar coordinate conversion\n",
    "    polar_image = polar_to_rectilinear(image)\n",
    "    \n",
    "    axes[i, 4].imshow(polar_image, cmap='viridis', aspect='auto')\n",
    "    axes[i, 4].set_title(\"Polar Coordinates\")\n",
    "    axes[i, 4].set_xlabel(\"Angle ()\")\n",
    "    axes[i, 4].set_ylabel(\"Radius (r)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import center_of_mass\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def extract_azimuthal_intensity(image, center=None, radius_range=None, \n",
    "                               bins=360, mask_beamstop=True):\n",
    "    \"\"\"\n",
    "    Extract azimuthal intensity variation from a diffraction pattern.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : 2D numpy array\n",
    "        The diffraction pattern image\n",
    "    center : tuple, optional\n",
    "        (y, x) coordinates of diffraction center. If None, uses center of mass\n",
    "    radius_range : tuple, optional\n",
    "        (min_radius, max_radius) to consider for integration. If None, uses full image\n",
    "    bins : int, optional\n",
    "        Number of azimuthal bins (default: 360 for 1-degree resolution)\n",
    "    mask_beamstop : bool, optional\n",
    "        Whether to exclude the beamstop region from the analysis\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (angles, intensities)\n",
    "        angles: array of azimuthal angles in degrees\n",
    "        intensities: normalized intensity at each angle\n",
    "    \"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) > 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Find the center if not provided\n",
    "    if center is None:\n",
    "        # Use a thresholded image for better center detection\n",
    "        threshold = np.percentile(image, 95)  # Adjust percentile as needed\n",
    "        binary_image = image > threshold\n",
    "        center = center_of_mass(binary_image)\n",
    "\n",
    "    print(center)\n",
    "    \n",
    "    # Create coordinate grid\n",
    "    rows, cols = image.shape\n",
    "    y, x = np.indices((rows, cols))\n",
    "    \n",
    "    # Calculate radial distance from center\n",
    "    r = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
    "    \n",
    "    # Calculate azimuthal angle (in degrees)\n",
    "    print(y,x)\n",
    "    phi = np.arctan2(y - center[1], x - center[0]) * 180 / np.pi\n",
    "    # Normalize to [0, 360)\n",
    "    phi = (phi + 360) % 360\n",
    "    \n",
    "    # Create mask for beamstop if needed\n",
    "    if mask_beamstop:\n",
    "        # Adjust threshold and size as needed for your images\n",
    "        threshold_value = np.percentile(image, 10)\n",
    "        beamstop_mask = image <= threshold_value\n",
    "        image = np.ma.array(image, mask=beamstop_mask)\n",
    "    \n",
    "    # Set radius range if not provided\n",
    "    if radius_range is None:\n",
    "        min_radius = 5  # Avoid center pixels\n",
    "        max_radius = min(center[0], center[1], \n",
    "                         rows - center[0], cols - center[1])\n",
    "    else:\n",
    "        min_radius, max_radius = radius_range\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.plot(center[0],center[1],'or')\n",
    "    \n",
    "    # Create mask for specified radius range\n",
    "    radius_mask = (r >= min_radius) & (r <= max_radius)\n",
    "    \n",
    "    # Bin the azimuthal angles\n",
    "    bin_edges = np.linspace(0, 360, bins + 1)\n",
    "    azimuthal_intensity = np.zeros(bins)\n",
    "    \n",
    "    for i in range(bins):\n",
    "        angle_mask = (phi >= bin_edges[i]) & (phi < bin_edges[i + 1])\n",
    "        valid_mask = angle_mask & radius_mask\n",
    "        if np.any(valid_mask):\n",
    "            azimuthal_intensity[i] = np.mean(image[valid_mask])\n",
    "    \n",
    "    # Normalize the intensity\n",
    "    if np.max(azimuthal_intensity) > 0:\n",
    "        azimuthal_intensity = azimuthal_intensity / np.max(azimuthal_intensity)\n",
    "    \n",
    "    return bin_edges[:-1], azimuthal_intensity\n",
    "\n",
    "def plot_azimuthal_profile(image, angles, intensities, title=\"Azimuthal Intensity Profile\"):\n",
    "    \"\"\"Plot the original image alongside its azimuthal intensity profile\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot the original image\n",
    "    ax1.imshow(image, cmap='viridis')\n",
    "    ax1.set_title(\"Original Diffraction Pattern\")\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Plot the azimuthal profile\n",
    "    ax2.plot(angles, intensities, 'b-')\n",
    "    ax2.set_xlabel(\"Azimuthal Angle (degrees)\")\n",
    "    ax2.set_ylabel(\"Normalized Intensity\")\n",
    "    ax2.set_title(title)\n",
    "    ax2.set_xlim(0, 360)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def process_multiple_images(image_folder, pattern=\"*.jpg\", save_folder=None):\n",
    "    \"\"\"Process multiple diffraction pattern images\"\"\"\n",
    "    if save_folder and not Path(save_folder).exists():\n",
    "        Path(save_folder).mkdir(parents=True)\n",
    "    \n",
    "    image_paths = glob.glob(str(Path(image_folder) / pattern))\n",
    "    results = []\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        img_name = Path(img_path).stem\n",
    "        print(f\"Processing {img_name}...\")\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"Failed to load {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Extract azimuthal profile\n",
    "        angles, intensities = extract_azimuthal_intensity(image)\n",
    "        \n",
    "        # Plot and save if needed\n",
    "        fig = plot_azimuthal_profile(image, angles, intensities, \n",
    "                                    title=f\"Azimuthal Profile: {img_name}\")\n",
    "        \n",
    "        if save_folder:\n",
    "            fig.savefig(str(Path(save_folder) / f\"{img_name}_azimuthal.png\"))\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Save the data as CSV\n",
    "            data = np.column_stack((angles, intensities))\n",
    "            np.savetxt(str(Path(save_folder) / f\"{img_name}_azimuthal.csv\"), \n",
    "                      data, delimiter=',', header='angle,intensity')\n",
    "        \n",
    "        results.append({\n",
    "            'name': img_name,\n",
    "            'angles': angles,\n",
    "            'intensities': intensities\n",
    "        })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32660fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "angles, intensities = extract_azimuthal_intensity(monomer, (104,100))\n",
    "plot_azimuthal_profile(monomer, angles, intensities)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b7932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def polar_to_rectilinear(data_array):\n",
    "    \"\"\"\n",
    "    Converts a polar diffraction image (as an xarray DataArray) to a rectilinear image.\n",
    "\n",
    "    Args:\n",
    "        data_array (xr.DataArray): The input polar diffraction image as an xarray DataArray.\n",
    "\n",
    "    Returns:\n",
    "        rectilinear_image (numpy.ndarray): The rectilinear transformed image.\n",
    "    \"\"\"\n",
    "    # Get the image data as a NumPy array\n",
    "    # image = data_array.values\n",
    "\n",
    "    # Get image dimensions\n",
    "    height, width = image.shape\n",
    "\n",
    "    # Define the center of the diffraction pattern (assumed bottom-center)\n",
    "    center_x, center_y = width // 2 + 3, height\n",
    "\n",
    "    # Define the maximum radius to transform\n",
    "    max_radius = min(center_x, height)\n",
    "\n",
    "    # Define the output rectilinear image size\n",
    "    rect_height = max_radius  # Vertical axis corresponds to radius\n",
    "    rect_width = 180  # Only half of the original 360 degrees\n",
    "\n",
    "    # Create a mesh grid for the polar transformation (half-circle)\n",
    "    theta = np.linspace(0, np.pi, rect_width)  # 0 to 180 degrees\n",
    "    r = np.linspace(0, max_radius, rect_height)  # Radial distances\n",
    "\n",
    "    # Create coordinate matrices\n",
    "    Theta, R = np.meshgrid(theta, r)\n",
    "\n",
    "    # Convert polar coordinates to Cartesian\n",
    "    X = center_x + R * np.cos(Theta)\n",
    "    Y = center_y - R * np.sin(Theta)\n",
    "\n",
    "    # Map coordinates to integer values\n",
    "    X = X.astype(np.float32)\n",
    "    Y = Y.astype(np.float32)\n",
    "\n",
    "    # Perform the remapping\n",
    "    rectilinear_image = cv2.remap(image, X, Y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
    "\n",
    "    # Flip vertically to keep intensity strongest at the bottom\n",
    "    rectilinear_image = cv2.flip(rectilinear_image, 0)\n",
    "\n",
    "    return rectilinear_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bee276",
   "metadata": {},
   "outputs": [],
   "source": [
    "monomer_rect = polar_to_rectilinear(monomer)\n",
    "dimer_rect = polar_to_rectilinear(dimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ec096",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(monomer_rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dimer_rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import map_coordinates\n",
    "import xarray as xr\n",
    "\n",
    "def polar_to_cartesian(polar_data):\n",
    "    \"\"\"\n",
    "    Convert a polar image (r, ) to Cartesian coordinates (x, y)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    polar_data : xarray.DataArray\n",
    "        The input data in polar coordinates where:\n",
    "        - First dimension (dim_0) is radius\n",
    "        - Second dimension (dim_1) is angle in degrees\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cartesian_image : np.ndarray\n",
    "        The transformed image in Cartesian coordinates\n",
    "    \"\"\"\n",
    "    # Extract values from the DataArray\n",
    "    polar_image = polar_data.values\n",
    "    \n",
    "    # Get dimensions\n",
    "    r_dim, theta_dim = polar_image.shape\n",
    "    \n",
    "    # Create coordinate arrays for radius and angle\n",
    "    # Assuming angles go from 0 to 360 degrees\n",
    "    angles = np.linspace(0, 2*np.pi, theta_dim, endpoint=False)\n",
    "    \n",
    "    # Determine the output image size (should be large enough to contain the full circle)\n",
    "    max_radius = r_dim - 1\n",
    "    output_shape = (2*max_radius, 2*max_radius)\n",
    "    \n",
    "    # Initialize the output Cartesian image\n",
    "    cartesian_image = np.zeros(output_shape)\n",
    "    \n",
    "    # Create coordinate meshgrid for the output image\n",
    "    y_coords, x_coords = np.indices(output_shape)\n",
    "    \n",
    "    # Calculate the center of the output image\n",
    "    y_center, x_center = max_radius, max_radius\n",
    "    \n",
    "    # Convert Cartesian coordinates to polar coordinates\n",
    "    x_from_center = x_coords - x_center\n",
    "    y_from_center = y_coords - y_center\n",
    "    r = np.sqrt(x_from_center**2 + y_from_center**2)\n",
    "    theta = np.arctan2(y_from_center, x_from_center)\n",
    "    \n",
    "    # Convert negative angles to positive\n",
    "    theta = np.mod(theta, 2*np.pi)\n",
    "    \n",
    "    # Scale theta to indices in the polar image\n",
    "    theta_idx = theta * theta_dim / (2*np.pi)\n",
    "    \n",
    "    # Create coordinate arrays for mapping\n",
    "    # Only map points within the valid radius\n",
    "    mask = r <= max_radius\n",
    "    coords = np.vstack((r[mask], theta_idx[mask]))\n",
    "    \n",
    "    # Sample from the polar image using interpolation\n",
    "    cartesian_image[mask] = map_coordinates(polar_image, coords, order=1)\n",
    "    \n",
    "    return cartesian_image\n",
    "\n",
    "def plot_polar_and_cartesian(polar_data, cartesian_image, title='Diffraction Pattern'):\n",
    "    \"\"\"\n",
    "    Plot both the polar and Cartesian versions of the image\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot polar image\n",
    "    im1 = ax1.imshow(polar_data.values, aspect='auto', cmap='viridis')\n",
    "    ax1.set_title('Polar Coordinates')\n",
    "    ax1.set_xlabel('Angle ()')\n",
    "    ax1.set_ylabel('Radius (r)')\n",
    "    \n",
    "    # Plot Cartesian image\n",
    "    im2 = ax2.imshow(cartesian_image, cmap='viridis')\n",
    "    ax2.set_title('Cartesian Coordinates')\n",
    "    ax2.set_xlabel('X-axis')\n",
    "    ax2.set_ylabel('Y-axis')\n",
    "    \n",
    "    # Add colorbars\n",
    "    plt.colorbar(im1, ax=ax1, label='Intensity')\n",
    "    plt.colorbar(im2, ax=ax2, label='Intensity')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Main function to process the DataArray\n",
    "def process_diffraction_pattern(data_array, n_samples=1):\n",
    "    \"\"\"\n",
    "    Process n_samples random images from the DataArray\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_array : xarray.DataArray\n",
    "        The input data array with dimensions (trainId, dim_0, dim_1)\n",
    "    n_samples : int\n",
    "        Number of random samples to process\n",
    "    \"\"\"\n",
    "    # Select random samples if needed\n",
    "    if n_samples == 1:\n",
    "        # Just take the first image\n",
    "        sample_idx = 0\n",
    "        polar_data = data_array.isel(trainId=sample_idx)\n",
    "    else:\n",
    "        # Select random samples\n",
    "        sample_indices = np.random.choice(data_array.dims['trainId'], n_samples, replace=False)\n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            polar_data = data_array.isel(trainId=idx)\n",
    "            cartesian_image = polar_to_cartesian(polar_data)\n",
    "            plot_polar_and_cartesian(polar_data, cartesian_image, \n",
    "                                     title=f'Diffraction Pattern (Sample {i+1}, trainId={idx})')\n",
    "            plt.show()\n",
    "        return\n",
    "    \n",
    "    # For single sample case\n",
    "    cartesian_image = polar_to_cartesian(polar_data)\n",
    "    fig = plot_polar_and_cartesian(polar_data, cartesian_image)\n",
    "    \n",
    "    return cartesian_image\n",
    "\n",
    "cartesian_image = process_diffraction_pattern(da)\n",
    "\n",
    "# Example usage (assuming da is your xarray DataArray):\n",
    "\"\"\"\n",
    "cartesian_image = process_diffraction_pattern(da)\n",
    "\n",
    "# Or process multiple patterns\n",
    "process_diffraction_pattern(da, n_samples=3)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm  # For showing progress in a notebook\n",
    "\n",
    "# Function to preprocess a diffraction image\n",
    "def preprocess_diffraction_image(image):\n",
    "    \"\"\"Preprocess a diffraction image.\"\"\"\n",
    "    # Convert image to float and normalize\n",
    "    image = image.astype(float)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Function to extract features from a diffraction image\n",
    "def extract_diffraction_features(image):\n",
    "    \"\"\"Extract features that distinguish between monomers and dimers.\"\"\"\n",
    "    # Get image dimensions and center\n",
    "    h, w = image.shape\n",
    "    center_y, center_x = h // 2, w // 2\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    \n",
    "    # Calculate radius and angle for each pixel\n",
    "    r = np.sqrt((x - center_x)**2 + (y - center_y)**2).astype(int)\n",
    "    theta = np.arctan2(y - center_y, x - center_x)\n",
    "    theta = (theta + 2*np.pi) % (2*np.pi)\n",
    "    \n",
    "    # 1. Extract radial profile (intensity vs. radius)\n",
    "    max_r = int(np.max(r))\n",
    "    radial_profile = np.zeros(max_r + 1)\n",
    "    for i in range(max_r + 1):\n",
    "        mask = r == i\n",
    "        if mask.any():\n",
    "            radial_profile[i] = np.mean(image[mask])\n",
    "    \n",
    "    # Downsample radial profile\n",
    "    n_radial_samples = 40\n",
    "    radial_profile_downsampled = np.interp(\n",
    "        np.linspace(0, max_r, n_radial_samples), \n",
    "        np.arange(max_r + 1), \n",
    "        radial_profile\n",
    "    )\n",
    "    \n",
    "    # 2. Extract azimuthal profiles at different radii\n",
    "    n_angular_bins = 36  # 10 degrees per bin\n",
    "    azimuthal_features = []\n",
    "    \n",
    "    # Sample at 3 different radii\n",
    "    sample_radii = [max_r // 3, max_r // 2, 2 * max_r // 3]\n",
    "    \n",
    "    for r_sample in sample_radii:\n",
    "        azimuthal_profile = np.zeros(n_angular_bins)\n",
    "        \n",
    "        for i in range(n_angular_bins):\n",
    "            theta_min = i * (2*np.pi/n_angular_bins)\n",
    "            theta_max = (i+1) * (2*np.pi/n_angular_bins)\n",
    "            \n",
    "            # Select pixels in this angular bin and radius range\n",
    "            angular_mask = (theta >= theta_min) & (theta < theta_max)\n",
    "            radius_mask = (r >= r_sample-2) & (r <= r_sample+2)\n",
    "            mask = angular_mask & radius_mask\n",
    "            \n",
    "            if mask.any():\n",
    "                azimuthal_profile[i] = np.mean(image[mask])\n",
    "        \n",
    "        # Calculate variance (higher for interference patterns)\n",
    "        azimuthal_var = np.var(azimuthal_profile)\n",
    "        # Calculate max/min ratio (higher for interference patterns)\n",
    "        azimuthal_max_min_ratio = np.max(azimuthal_profile) / (np.min(azimuthal_profile) + 1e-6)\n",
    "        \n",
    "        azimuthal_features.extend([azimuthal_var, azimuthal_max_min_ratio])\n",
    "    \n",
    "    # Combine features\n",
    "    features = np.concatenate([radial_profile_downsampled, azimuthal_features])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Main function to classify diffraction patterns from xarray\n",
    "def classify_diffraction_patterns(data_array, sample_size=None):\n",
    "    \"\"\"\n",
    "    Classify diffraction images from an xarray DataArray into monomers and dimers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_array : xarray.DataArray\n",
    "        The xarray containing the diffraction patterns\n",
    "    sample_size : int, optional\n",
    "        Number of images to sample for clustering (useful for large datasets)\n",
    "        If None, all images are used\n",
    "    \"\"\"\n",
    "    # Get number of images\n",
    "    n_images = data_array.shape[0]\n",
    "    \n",
    "    # Sample images if sample_size is provided\n",
    "    if sample_size is not None and sample_size < n_images:\n",
    "        indices = np.random.choice(n_images, sample_size, replace=False)\n",
    "    else:\n",
    "        indices = np.arange(n_images)\n",
    "    \n",
    "    # Extract features from each image\n",
    "    features_list = []\n",
    "    \n",
    "    print(f\"Extracting features from {len(indices)} images...\")\n",
    "    for i in tqdm(indices):\n",
    "        # Get image from the xarray\n",
    "        img = data_array[i].values\n",
    "        \n",
    "        # Preprocess image\n",
    "        processed_img = preprocess_diffraction_image(img)\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_diffraction_features(processed_img)\n",
    "        features_list.append(features)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features_array = np.array(features_list)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_array)\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    print(\"Applying K-means clustering...\")\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "    sample_labels = kmeans.fit_predict(features_scaled)\n",
    "    \n",
    "    # If we sampled, predict for all images\n",
    "    if sample_size is not None and sample_size < n_images:\n",
    "        print(\"Predicting labels for all images...\")\n",
    "        labels = np.zeros(n_images, dtype=int)\n",
    "        labels[indices] = sample_labels\n",
    "        \n",
    "        # Process remaining images\n",
    "        remaining_indices = np.setdiff1d(np.arange(n_images), indices)\n",
    "        for i in tqdm(remaining_indices):\n",
    "            img = data_array[i].values\n",
    "            processed_img = preprocess_diffraction_image(img)\n",
    "            features = extract_diffraction_features(processed_img)\n",
    "            features_scaled = scaler.transform(features.reshape(1, -1))\n",
    "            labels[i] = kmeans.predict(features_scaled)[0]\n",
    "    else:\n",
    "        labels = sample_labels\n",
    "    \n",
    "    return labels, kmeans, scaler, indices\n",
    "\n",
    "# Function to determine which cluster corresponds to monomers and which to dimers\n",
    "def determine_cluster_identity(data_array, indices, labels):\n",
    "    \"\"\"Determine which cluster is monomer and which is dimer.\"\"\"\n",
    "    # Get indices for each cluster\n",
    "    cluster_0_indices = indices[labels[indices] == 0]\n",
    "    cluster_1_indices = indices[labels[indices] == 1]\n",
    "    \n",
    "    # If either cluster is empty, return a default mapping\n",
    "    if len(cluster_0_indices) == 0 or len(cluster_1_indices) == 0:\n",
    "        return {0: \"Cluster 0\", 1: \"Cluster 1\"}\n",
    "    \n",
    "    # Calculate average azimuthal variance for each cluster\n",
    "    azimuthal_vars_0 = []\n",
    "    azimuthal_vars_1 = []\n",
    "    \n",
    "    print(\"Analyzing cluster characteristics...\")\n",
    "    # Sample a few images from each cluster for efficiency\n",
    "    sample_size = min(20, len(cluster_0_indices), len(cluster_1_indices))\n",
    "    \n",
    "    for i in np.random.choice(cluster_0_indices, sample_size, replace=False):\n",
    "        img = data_array[i].values\n",
    "        processed_img = preprocess_diffraction_image(img)\n",
    "        features = extract_diffraction_features(processed_img)\n",
    "        # The azimuthal variance features start after the radial profile\n",
    "        azimuthal_vars = features[40:][::2]  # Every other feature is a variance\n",
    "        azimuthal_vars_0.append(np.mean(azimuthal_vars))\n",
    "    \n",
    "    for i in np.random.choice(cluster_1_indices, sample_size, replace=False):\n",
    "        img = data_array[i].values\n",
    "        processed_img = preprocess_diffraction_image(img)\n",
    "        features = extract_diffraction_features(processed_img)\n",
    "        azimuthal_vars = features[40:][::2]\n",
    "        azimuthal_vars_1.append(np.mean(azimuthal_vars))\n",
    "    \n",
    "    avg_var_0 = np.mean(azimuthal_vars_0)\n",
    "    avg_var_1 = np.mean(azimuthal_vars_1)\n",
    "    \n",
    "    # The cluster with lower azimuthal variance corresponds to monomers (full rings)\n",
    "    # The cluster with higher azimuthal variance corresponds to dimers (interference patterns)\n",
    "    if avg_var_0 < avg_var_1:\n",
    "        return {0: \"Monomer (Full Rings)\", 1: \"Dimer (Interference Pattern)\"}\n",
    "    else:\n",
    "        return {1: \"Monomer (Full Rings)\", 0: \"Dimer (Interference Pattern)\"}\n",
    "\n",
    "# Function to visualize classification results\n",
    "def visualize_classification(data_array, labels, label_map, n_per_class=3):\n",
    "    \"\"\"Visualize classification results showing examples from each class.\"\"\"\n",
    "    fig, axes = plt.subplots(2, n_per_class, figsize=(15, 8))\n",
    "    fig.suptitle(\"Diffraction Pattern Classification Results\", fontsize=16)\n",
    "    \n",
    "    # For each cluster\n",
    "    for cluster_idx in range(2):\n",
    "        # Get indices of images in this cluster\n",
    "        cluster_indices = np.where(labels == cluster_idx)[0]\n",
    "        \n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Sample n_per_class images\n",
    "        sample_indices = np.random.choice(cluster_indices, \n",
    "                                         min(n_per_class, len(cluster_indices)), \n",
    "                                         replace=False)\n",
    "        \n",
    "        # Display images\n",
    "        for i, img_idx in enumerate(sample_indices):\n",
    "            if i < n_per_class:\n",
    "                ax = axes[cluster_idx, i]\n",
    "                img = data_array[img_idx].values\n",
    "                ax.imshow(img, cmap='viridis')\n",
    "                ax.set_title(f\"{label_map[cluster_idx]}\\nImage {img_idx}\")\n",
    "                ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Function to analyze the distribution of classes\n",
    "def analyze_distribution(labels, label_map):\n",
    "    \"\"\"Analyze and visualize the distribution of classes.\"\"\"\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.bar([label_map[label] for label in unique_labels], counts)\n",
    "    \n",
    "    # Add count labels on top of bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                 bar.get_height() + 5, \n",
    "                 str(count), \n",
    "                 ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('Distribution of Diffraction Pattern Classes')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print percentages\n",
    "    total = len(labels)\n",
    "    print(\"Class Distribution:\")\n",
    "    for label in unique_labels:\n",
    "        percent = 100 * np.sum(labels == label) / total\n",
    "        print(f\"{label_map[label]}: {np.sum(labels == label)} images ({percent:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee995aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnccd = high_fluence_pnccd\n",
    "\n",
    "# First, inspect the loaded data\n",
    "print(f\"Data shape: {pnccd.shape}\")\n",
    "print(f\"Number of images: {pnccd.dims[0]}\")\n",
    "print(f\"Image dimensions: {pnccd.shape[1]} x {pnccd.shape[2]}\")\n",
    "\n",
    "# Display a few example images\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    idx = np.random.randint(0, pnccd.shape[0])\n",
    "    ax.imshow(pnccd[idx].values, cmap='viridis')\n",
    "    ax.set_title(f\"Image {idx}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Set sample size for efficient processing\n",
    "# Use a smaller sample for initial model training, then classify all images\n",
    "sample_size = 200  # Adjust based on computational resources\n",
    "\n",
    "# Run the classification\n",
    "labels, kmeans, scaler, sampled_indices = classify_diffraction_patterns(\n",
    "    pnccd, sample_size=sample_size\n",
    ")\n",
    "\n",
    "# Determine which cluster is monomer/dimer\n",
    "label_map = determine_cluster_identity(pnccd, sampled_indices, labels)\n",
    "print(f\"Cluster mapping: {label_map}\")\n",
    "\n",
    "# Visualize classification results\n",
    "visualize_classification(pnccd, labels, label_map, n_per_class=3)\n",
    "\n",
    "# Analyze distribution of classes\n",
    "analyze_distribution(labels, label_map)\n",
    "\n",
    "# Create a new xarray DataArray with the labels\n",
    "label_da = xr.DataArray(\n",
    "    labels,\n",
    "    dims=['trainId'],\n",
    "    coords={'trainId': pnccd.coords['trainId']}\n",
    ")\n",
    "\n",
    "# Find indices of each class\n",
    "monomer_indices = np.where(labels == [k for k, v in label_map.items() if 'Monomer' in v][0])[0]\n",
    "dimer_indices = np.where(labels == [k for k, v in label_map.items() if 'Dimer' in v][0])[0]\n",
    "\n",
    "print(f\"Found {len(monomer_indices)} monomers and {len(dimer_indices)} dimers\")\n",
    "\n",
    "# Optional: Save the classification results\n",
    "# label_da.to_netcdf('diffraction_labels.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these imports to your existing code\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def visualize_clusters_2d(data_array, features_scaled, labels, kmeans, label_map, method='pca'):\n",
    "    \"\"\"\n",
    "    Visualize the clustering results in 2D space using dimensionality reduction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_array : xarray.DataArray\n",
    "        The original image data array\n",
    "    features_scaled : array\n",
    "        The scaled feature array used for clustering\n",
    "    labels : array\n",
    "        Cluster labels assigned to each data point\n",
    "    kmeans : KMeans\n",
    "        The fitted KMeans model\n",
    "    label_map : dict\n",
    "        Mapping from cluster indices to meaningful labels\n",
    "    method : str\n",
    "        Dimensionality reduction method ('pca' or 'tsne')\n",
    "    \"\"\"\n",
    "    # Apply dimensionality reduction\n",
    "    if method.lower() == 'pca':\n",
    "        print(\"Applying PCA for dimensionality reduction...\")\n",
    "        reducer = PCA(n_components=2)\n",
    "        reduced_features = reducer.fit_transform(features_scaled)\n",
    "        title_prefix = \"PCA\"\n",
    "        \n",
    "        # Calculate explained variance\n",
    "        explained_variance = reducer.explained_variance_ratio_.sum() * 100\n",
    "        title_suffix = f\"(Explained variance: {explained_variance:.2f}%)\"\n",
    "    else:\n",
    "        print(\"Applying t-SNE for dimensionality reduction...\")\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "        reduced_features = reducer.fit_transform(features_scaled)\n",
    "        title_prefix = \"t-SNE\"\n",
    "        title_suffix = \"\"\n",
    "    \n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create custom colormap\n",
    "    colors = ['#1f77b4', '#ff7f0e']  # Blue and orange\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                 c=labels, cmap=cmap, alpha=0.7, s=50)\n",
    "    \n",
    "    # If using PCA, also show cluster centers\n",
    "    if method.lower() == 'pca':\n",
    "        # Transform cluster centers\n",
    "        centers_reduced = reducer.transform(kmeans.cluster_centers_)\n",
    "        \n",
    "        # Plot cluster centers\n",
    "        plt.scatter(centers_reduced[:, 0], centers_reduced[:, 1], \n",
    "                   marker='X', s=200, c='red', label='Cluster Centers')\n",
    "        \n",
    "        # Draw decision boundary (for visual clarity, only draw a limited sample)\n",
    "        # This is a simplified approximation of the decision boundary\n",
    "        x_min, x_max = reduced_features[:, 0].min() - 1, reduced_features[:, 0].max() + 1\n",
    "        y_min, y_max = reduced_features[:, 1].min() - 1, reduced_features[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "        \n",
    "        # For visualization purposes, we use the 2D PCA space to approximate decision boundary\n",
    "        # This isn't the true boundary in the original feature space\n",
    "        xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "        Z = kmeans.predict(reducer.inverse_transform(xy))\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        # Plot decision boundary with transparency\n",
    "        plt.contour(xx, yy, Z, colors='k', linestyles='-', alpha=0.3)\n",
    "    \n",
    "    # Create legend with cluster labels\n",
    "    legend_elements = []\n",
    "    for i in range(len(label_map)):\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                               markerfacecolor=colors[i], markersize=10, \n",
    "                               label=label_map[i]))\n",
    "    \n",
    "    # Add cluster centers to legend\n",
    "    legend_elements.append(plt.Line2D([0], [0], marker='X', color='w', \n",
    "                          markerfacecolor='red', markersize=10, \n",
    "                          label='Cluster Centers'))\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='best', fontsize=12)\n",
    "    \n",
    "    # Set plot title and labels\n",
    "    plt.title(f\"{title_prefix} Visualization of Diffraction Pattern Clusters {title_suffix}\", fontsize=16)\n",
    "    plt.xlabel(f\"{title_prefix} Component 1\", fontsize=14)\n",
    "    plt.ylabel(f\"{title_prefix} Component 2\", fontsize=14)\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Improve layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display example images from each cluster\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle(f\"Example images from each cluster\", fontsize=16)\n",
    "    \n",
    "    for cluster_idx in range(2):\n",
    "        cluster_indices = np.where(labels == cluster_idx)[0]\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Sample 3 images (or fewer if not enough)\n",
    "        sample_indices = np.random.choice(cluster_indices, min(3, len(cluster_indices)), replace=False)\n",
    "        \n",
    "        for j, img_idx in enumerate(sample_indices):\n",
    "            if j < 3:\n",
    "                ax = axes[cluster_idx, j]\n",
    "                img = data_array[img_idx].values\n",
    "                im = ax.imshow(img, cmap='viridis')\n",
    "                ax.set_title(f\"{label_map[cluster_idx]}\\nImage {img_idx}\")\n",
    "                ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "    return reduced_features\n",
    "\n",
    "def visualize_feature_importance(kmeans, scaler, label_map, n_features=10):\n",
    "    \"\"\"\n",
    "    Visualize the most important features that distinguish the clusters.\n",
    "    \"\"\"\n",
    "    # Calculate the absolute difference between cluster centers\n",
    "    center_diff = np.abs(kmeans.cluster_centers_[0] - kmeans.cluster_centers_[1])\n",
    "    \n",
    "    # Get the indices of the top differentiating features\n",
    "    top_indices = np.argsort(center_diff)[-n_features:]\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = []\n",
    "    for i in range(len(center_diff)):\n",
    "        if i < 40:  # First 40 are radial features\n",
    "            feature_names.append(f\"Radial_{i}\")\n",
    "        else:  # Remaining are azimuthal features\n",
    "            idx = (i - 40) // 2\n",
    "            if (i - 40) % 2 == 0:\n",
    "                feature_names.append(f\"Azimuthal_var_{idx}\")\n",
    "            else:\n",
    "                feature_names.append(f\"Azimuthal_ratio_{idx}\")\n",
    "    \n",
    "    # Calculate the original values (not scaled)\n",
    "    centers_original = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot a bar for each top feature\n",
    "    y_pos = np.arange(n_features)\n",
    "    \n",
    "    # Plot values for cluster 0\n",
    "    plt.barh(y_pos - 0.2, centers_original[0, top_indices], height=0.4, \n",
    "            color='#1f77b4', alpha=0.7, label=label_map[0])\n",
    "    \n",
    "    # Plot values for cluster 1\n",
    "    plt.barh(y_pos + 0.2, centers_original[1, top_indices], height=0.4, \n",
    "            color='#ff7f0e', alpha=0.7, label=label_map[1])\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.yticks(y_pos, [feature_names[i] for i in top_indices])\n",
    "    plt.xlabel('Feature Value', fontsize=12)\n",
    "    plt.title('Top Differentiating Features Between Clusters', fontsize=16)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Improve layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this line first to enable interactive mode in your Jupyter notebook \n",
    "%matplotlib notebook\n",
    "\n",
    "def interactive_tsne_plot(pnccd, reduced_features, labels, label_map):\n",
    "    \"\"\"\n",
    "    Create an interactive t-SNE plot where clicking on a point displays the corresponding image.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pnccd : xarray.DataArray\n",
    "        The original diffraction pattern images\n",
    "    reduced_features : numpy.ndarray\n",
    "        The t-SNE embedding coordinates from visualize_clusters_2d\n",
    "    labels : numpy.ndarray\n",
    "        Cluster labels for each image\n",
    "    label_map : dict\n",
    "        Mapping from cluster indices to meaningful labels\n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create a nearest neighbors model for finding closest point to click\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(reduced_features)\n",
    "    \n",
    "    # Set up the figure with two subplots side by side\n",
    "    fig, (ax_tsne, ax_img) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # Create scatter plot on the first axis\n",
    "    colors = ['#1f77b4', '#ff7f0e']  # Blue for monomers, orange for dimers\n",
    "    \n",
    "    for i, (cluster_idx, cluster_name) in enumerate(label_map.items()):\n",
    "        mask = labels == cluster_idx\n",
    "        ax_tsne.scatter(\n",
    "            reduced_features[mask, 0], \n",
    "            reduced_features[mask, 1],\n",
    "            c=colors[i], \n",
    "            alpha=0.7, \n",
    "            s=50, \n",
    "            label=cluster_name\n",
    "        )\n",
    "    \n",
    "    # Add cluster centers if available\n",
    "    ax_tsne.scatter(\n",
    "        [reduced_features[np.where(labels == k)[0][0], 0] for k in label_map.keys()],\n",
    "        [reduced_features[np.where(labels == k)[0][0], 1] for k in label_map.keys()],\n",
    "        marker='X',\n",
    "        s=200,\n",
    "        c='red',\n",
    "        label='Cluster Centers'\n",
    "    )\n",
    "    \n",
    "    # Add labels and legend to t-SNE plot\n",
    "    ax_tsne.set_title(\"t-SNE Visualization (Click on a point to view image)\", fontsize=16)\n",
    "    ax_tsne.set_xlabel(\"t-SNE Component 1\", fontsize=14)\n",
    "    ax_tsne.set_ylabel(\"t-SNE Component 2\", fontsize=14)\n",
    "    ax_tsne.legend(fontsize=12)\n",
    "    ax_tsne.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Initial display in image subplot\n",
    "    img_placeholder = np.zeros((pnccd.shape[1], pnccd.shape[2]))\n",
    "    img_display = ax_img.imshow(img_placeholder, cmap='viridis')\n",
    "    ax_img.set_title(\"Click on a point to display its image\", fontsize=14)\n",
    "    ax_img.axis('off')\n",
    "    \n",
    "    # For highlighting the selected point\n",
    "    highlight, = ax_tsne.plot([], [], 'o', markersize=12, color='red', fillstyle='none', mew=2)\n",
    "    \n",
    "    # Function to handle click events\n",
    "    def onclick(event):\n",
    "        # Check if the click is within the scatter plot axes\n",
    "        if event.inaxes != ax_tsne:\n",
    "            return\n",
    "        \n",
    "        # Get the coordinates of the click\n",
    "        click_x, click_y = event.xdata, event.ydata\n",
    "        click_point = np.array([[click_x, click_y]])\n",
    "        \n",
    "        # Find the nearest point in the t-SNE space\n",
    "        distances, indices = nbrs.kneighbors(click_point)\n",
    "        nearest_idx = indices[0][0]\n",
    "        \n",
    "        # Update highlight position\n",
    "        highlight.set_data([reduced_features[nearest_idx, 0]], [reduced_features[nearest_idx, 1]])\n",
    "        \n",
    "        # Get the image and its label\n",
    "        img = pnccd[nearest_idx].values\n",
    "        cluster_label = labels[nearest_idx]\n",
    "        label_text = label_map[cluster_label]\n",
    "        \n",
    "        # Update the image display\n",
    "        img_display.set_data(img)\n",
    "        img_display.set_clim(vmin=np.min(img), vmax=np.max(img))\n",
    "        ax_img.set_title(f\"{label_text} - Image #{nearest_idx}\", fontsize=14)\n",
    "        \n",
    "        # Redraw the figure to update the plot\n",
    "        fig.canvas.draw_idle()\n",
    "    \n",
    "    # Connect the click event to the handler function\n",
    "    fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "    \n",
    "    # Make sure layout looks good\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_tsne_plot(pnccd=high_fluence_pnccd, \n",
    "                     reduced_features=reduced_features, \n",
    "                     labels=labels, \n",
    "                     label_map=label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa882d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the following variables:\n",
    "# - pnccd: xarray.DataArray containing diffraction pattern images\n",
    "# - features_scaled: the scaled features used for t-SNE\n",
    "# - labels: cluster assignments for each image\n",
    "# - label_map: dict mapping from cluster index to label name\n",
    "# - reduced_features: the t-SNE embedding coordinates\n",
    "\n",
    "# Create the interactive plot\n",
    "interactive_tsne_plot(pnccd, all_features_scaled, labels, label_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a2502",
   "metadata": {},
   "source": [
    "## the working CNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
